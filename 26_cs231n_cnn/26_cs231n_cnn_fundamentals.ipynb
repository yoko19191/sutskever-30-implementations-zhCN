{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®ºæ–‡26ï¼šCS231n - è§†è§‰è¯†åˆ«çš„å·ç§¯ç¥žç»ç½‘ç»œ\n",
    "\n",
    "**è¯¾ç¨‹**ï¼šæ–¯å¦ç¦CS231nï¼ˆ2017æ˜¥å­£ï¼‰- Fei-Fei Liã€Justin Johnsonã€Serena Yeung\n",
    "\n",
    "**è§†è§‰åœ£ç»**ï¼šCS231næ˜¯æ·±åº¦å­¦ä¹ ç”¨äºŽè®¡ç®—æœºè§†è§‰çš„æƒå¨è¯¾ç¨‹ã€‚æœ¬ç¬”è®°æœ¬å°†å…¶æ ¸å¿ƒæ¦‚å¿µæç‚¼ä¸ºå•ä¸ªå¯æ‰§è¡Œå®žçŽ°ï¼Œä»…ä½¿ç”¨NumPyã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## CS231næ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "CS231næ•™æŽˆè§†è§‰è¯†åˆ«çš„åŸºç¡€ï¼š\n",
    "- å›¾åƒåˆ†ç±»æµç¨‹ï¼ˆä»Žåƒç´ åˆ°é¢„æµ‹ï¼‰\n",
    "- åå‘ä¼ æ’­å’Œä¼˜åŒ–\n",
    "- å·ç§¯ç¥žç»ç½‘ç»œ\n",
    "- çŽ°ä»£æž¶æž„ï¼ˆAlexNetã€VGGã€ResNetï¼‰\n",
    "- è®­ç»ƒæŠ€æœ¯å’Œ\"çœ‹æŠ¤\"ç¥žç»ç½‘ç»œ\n",
    "\n",
    "## æœ¬å®žçŽ°\n",
    "\n",
    "æˆ‘ä»¬å°†ä»Žé›¶å¼€å§‹æž„å»ºå®Œæ•´çš„è§†è§‰æµç¨‹ï¼š\n",
    "\n",
    "1. **kè¿‘é‚»**ï¼šåŸºçº¿åˆ†ç±»å™¨\n",
    "2. **çº¿æ€§åˆ†ç±»å™¨**ï¼šSVMå’ŒSoftmax\n",
    "3. **ä¼˜åŒ–**ï¼šSGDã€åŠ¨é‡ã€å­¦ä¹ çŽ‡è°ƒåº¦\n",
    "4. **ç¥žç»ç½‘ç»œ**ï¼š2å±‚å…¨è¿žæŽ¥ç½‘ç»œ\n",
    "5. **åå‘ä¼ æ’­**ï¼šæ‰‹åŠ¨æ¢¯åº¦è®¡ç®—\n",
    "6. **å·ç§¯ç½‘ç»œ**ï¼šConvã€poolã€ReLUå±‚\n",
    "7. **æž¶æž„**ï¼šAlexNeté£Žæ ¼çš„CNNã€VGGã€ResNetæ¦‚å¿µ\n",
    "8. **å¯è§†åŒ–**ï¼šæ˜¾è‘—æ€§å›¾ã€æ»¤æ³¢å™¨å¯è§†åŒ–\n",
    "\n",
    "## ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦\n",
    "\n",
    "**CS231nåŽŸåˆ™é€‚ç”¨äºŽæ‰€æœ‰åœ°æ–¹**ï¼š\n",
    "- AlexNet (2012) â†’ ImageNetçªç ´\n",
    "- VGG/ResNet â†’ æ ‡å‡†è§†è§‰éª¨å¹²\n",
    "- è¿™é‡Œçš„æŠ€æœ¯ â†’ çŽ°ä»£transformerã€æ‰©æ•£æ¨¡åž‹\n",
    "\n",
    "**ä¸Žè®ºæ–‡#7çš„è”ç³»**ï¼šè¿™ä¸ºAlexNetæä¾›äº†æ•™å­¦åŸºç¡€ï¼\n",
    "\n",
    "è®©æˆ‘ä»¬ä»Žç¬¬ä¸€åŽŸåˆ™æž„å»ºè§†è§‰ç³»ç»Ÿï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import convolve\n",
    "from typing import Tuple, List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"CS231n: From Pixels to Predictions\")\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"\\nReady to learn computer vision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬1èŠ‚ï¼šæ•°æ®é›† - åˆæˆCIFAR-10\n",
    "\n",
    "CS231nä½¿ç”¨CIFAR-10ï¼ˆ10ä¸ªç±»åˆ«ï¼Œ32Ã—32 RGBå›¾åƒï¼‰ã€‚æˆ‘ä»¬å°†ç”Ÿæˆå…·æœ‰ç±»ä¼¼ç»“æž„çš„åˆæˆæ•°æ®ã€‚\n",
    "\n",
    "## æ•°æ®ç”Ÿæˆç­–ç•¥\n",
    "\n",
    "åˆ›å»ºå…·æœ‰ç±»åˆ«ç‰¹å®šæ¨¡å¼çš„ç¨‹åºåŒ–32Ã—32å›¾åƒï¼š\n",
    "- **ç±»åˆ«0-2**ï¼šèžºæ—‹çº¿ï¼ˆä¸åŒæ—‹è½¬ï¼‰\n",
    "- **ç±»åˆ«3-5**ï¼šæ£‹ç›˜ï¼ˆä¸åŒé¢‘çŽ‡ï¼‰\n",
    "- **ç±»åˆ«6-7**ï¼šæ¸å˜ï¼ˆä¸åŒæ–¹å‘ï¼‰\n",
    "- **ç±»åˆ«8-9**ï¼šåœ†å½¢ï¼ˆä¸åŒå¤§å°ï¼‰\n",
    "\n",
    "è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ï¼š\n",
    "- å¯å­¦ä¹ çš„æ¨¡å¼ï¼ˆéžçº¯å™ªå£°ï¼‰\n",
    "- è§†è§‰å¤šæ ·æ€§ï¼ˆæµ‹è¯•ä¸åŒç‰¹å¾ï¼‰\n",
    "- å³æ—¶ç”Ÿæˆï¼ˆæ— éœ€ä¸‹è½½ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_cifar(num_samples: int = 1000, \n",
    "                             img_size: int = 32, \n",
    "                             num_classes: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate synthetic CIFAR-10 style dataset.\n",
    "    \n",
    "    Returns:\n",
    "        X: (N, 32, 32, 3) RGB images\n",
    "        y: (N,) class labels\n",
    "    \"\"\"\n",
    "    X = np.zeros((num_samples, img_size, img_size, 3))\n",
    "    y = np.random.randint(0, num_classes, num_samples)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        label = y[i]\n",
    "        img = np.random.randn(img_size, img_size, 3) * 0.1  # Base noise\n",
    "        \n",
    "        # Class-specific patterns\n",
    "        if label < 3:  # Spirals\n",
    "            theta = np.linspace(0, 4*np.pi, 200)\n",
    "            r = np.linspace(0, img_size/2, 200)\n",
    "            rotation = label * np.pi / 3\n",
    "            x_coords = (r * np.cos(theta + rotation) + img_size/2).astype(int)\n",
    "            y_coords = (r * np.sin(theta + rotation) + img_size/2).astype(int)\n",
    "            valid = (x_coords >= 0) & (x_coords < img_size) & (y_coords >= 0) & (y_coords < img_size)\n",
    "            img[y_coords[valid], x_coords[valid], :] = [1.0, 0.5, 0.0]\n",
    "            \n",
    "        elif label < 6:  # Checkerboards\n",
    "            freq = (label - 2) * 2\n",
    "            xx, yy = np.meshgrid(np.arange(img_size), np.arange(img_size))\n",
    "            pattern = ((xx // freq) + (yy // freq)) % 2\n",
    "            img[:, :, 0] = pattern\n",
    "            img[:, :, 1] = 1 - pattern\n",
    "            \n",
    "        elif label < 8:  # Gradients\n",
    "            if label == 6:\n",
    "                img[:, :, 0] = np.linspace(0, 1, img_size)[None, :]\n",
    "            else:\n",
    "                img[:, :, 1] = np.linspace(0, 1, img_size)[:, None]\n",
    "                \n",
    "        else:  # Circles\n",
    "            radius = (label - 7) * 8 + 5\n",
    "            yy, xx = np.ogrid[:img_size, :img_size]\n",
    "            circle = ((xx - img_size/2)**2 + (yy - img_size/2)**2 <= radius**2)\n",
    "            img[circle, 2] = 1.0\n",
    "        \n",
    "        X[i] = np.clip(img, 0, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Generate train/val/test splits\n",
    "print(\"Generating synthetic CIFAR-10...\\n\")\n",
    "\n",
    "X_train, y_train = generate_synthetic_cifar(num_samples=2000)\n",
    "X_val, y_val = generate_synthetic_cifar(num_samples=400)\n",
    "X_test, y_test = generate_synthetic_cifar(num_samples=400)\n",
    "\n",
    "print(f\"Training set:   X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation set: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test set:       X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    ax.imshow(X_train[idx])\n",
    "    ax.set_title(f'Class {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Synthetic CIFAR-10: Sample Images per Class', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Flatten for traditional classifiers\n",
    "X_train_flat = X_train.reshape(len(X_train), -1)  # (N, 3072)\n",
    "X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "X_test_flat = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "print(f\"\\nFlattened shape: {X_train_flat.shape} (32Ã—32Ã—3 = 3072 pixels)\")\n",
    "print(\"\\nâœ“ Dataset ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬2èŠ‚ï¼škè¿‘é‚»ï¼ˆkNNï¼‰\n",
    "\n",
    "**æœ€ç®€å•çš„åˆ†ç±»å™¨**ï¼šç»™å®šæµ‹è¯•å›¾åƒï¼Œæ‰¾åˆ°kä¸ªæœ€è¿‘çš„è®­ç»ƒå›¾åƒå¹¶å¯¹æ ‡ç­¾è¿›è¡ŒæŠ•ç¥¨ã€‚\n",
    "\n",
    "## ç®—æ³•\n",
    "\n",
    "1. è®¡ç®—åˆ°æ‰€æœ‰è®­ç»ƒå›¾åƒçš„è·ç¦»ï¼š$d(x_{\\text{test}}, x_{\\text{train}})$\n",
    "2. æ‰¾åˆ°kä¸ªæœ€è¿‘çš„é‚»å±…\n",
    "3. å¯¹å®ƒä»¬çš„æ ‡ç­¾è¿›è¡Œå¤šæ•°æŠ•ç¥¨\n",
    "\n",
    "## è·ç¦»åº¦é‡\n",
    "\n",
    "**L1ï¼ˆæ›¼å“ˆé¡¿ï¼‰**ï¼š\n",
    "$$d_1(x, y) = \\sum_i |x_i - y_i|$$\n",
    "\n",
    "**L2ï¼ˆæ¬§å‡ é‡Œå¾—ï¼‰**ï¼š\n",
    "$$d_2(x, y) = \\sqrt{\\sum_i (x_i - y_i)^2}$$\n",
    "\n",
    "## ä¸ºä»€ä¹ˆkNNå¾ˆé‡è¦\n",
    "\n",
    "- **æ— è®­ç»ƒ**ï¼šåªè®°å¿†æ•°æ®\n",
    "- **æµ‹è¯•æ—¶æ…¢**ï¼šæ¯æ¬¡é¢„æµ‹O(N)\n",
    "- **åŸºçº¿**ï¼šå»ºç«‹ä¸‹ç•Œ\n",
    "- **å®žè·µä¸­ä»Žä¸ä½¿ç”¨**ï¼šä½†åœ¨æ•™å­¦ä¸Šå¾ˆé‡è¦ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor:\n",
    "    \"\"\"k-Nearest Neighbor classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, k: int = 5):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"'Train' by memorizing data (no actual training!).\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        print(f\"kNN 'trained' on {len(X)} samples\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray, distance_metric: str = 'l2') -> np.ndarray:\n",
    "        \"\"\"Predict labels for test data.\n",
    "        \n",
    "        Args:\n",
    "            X: (N_test, D) test data\n",
    "            distance_metric: 'l1' or 'l2'\n",
    "        \n",
    "        Returns:\n",
    "            y_pred: (N_test,) predicted labels\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        y_pred = np.zeros(num_test, dtype=int)\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            # Compute distances to all training samples\n",
    "            if distance_metric == 'l1':\n",
    "                distances = np.sum(np.abs(self.X_train - X[i]), axis=1)\n",
    "            else:  # l2\n",
    "                distances = np.sqrt(np.sum((self.X_train - X[i])**2, axis=1))\n",
    "            \n",
    "            # Find k nearest neighbors\n",
    "            k_nearest = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_nearest]\n",
    "            \n",
    "            # Majority vote\n",
    "            y_pred[i] = np.argmax(np.bincount(k_nearest_labels))\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def compute_accuracy(self, X: np.ndarray, y: np.ndarray, **kwargs) -> float:\n",
    "        \"\"\"Compute classification accuracy.\"\"\"\n",
    "        y_pred = self.predict(X, **kwargs)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "\n",
    "# Train kNN (just memorize)\n",
    "print(\"Testing k-Nearest Neighbors...\\n\")\n",
    "\n",
    "knn = KNearestNeighbor(k=5)\n",
    "knn.train(X_train_flat, y_train)\n",
    "\n",
    "# Test different k values\n",
    "k_values = [1, 3, 5, 10, 20]\n",
    "accuracies_l1 = []\n",
    "accuracies_l2 = []\n",
    "\n",
    "print(\"\\nTesting different k values...\")\n",
    "for k in k_values:\n",
    "    knn.k = k\n",
    "    acc_l1 = knn.compute_accuracy(X_val_flat[:100], y_val[:100], distance_metric='l1')\n",
    "    acc_l2 = knn.compute_accuracy(X_val_flat[:100], y_val[:100], distance_metric='l2')\n",
    "    accuracies_l1.append(acc_l1)\n",
    "    accuracies_l2.append(acc_l2)\n",
    "    print(f\"  k={k:2d}: L1={acc_l1:.1%}, L2={acc_l2:.1%}\")\n",
    "\n",
    "# Plot accuracy vs k\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy vs k\n",
    "axes[0].plot(k_values, accuracies_l1, 'o-', linewidth=2, markersize=8, label='L1 distance')\n",
    "axes[0].plot(k_values, accuracies_l2, 's-', linewidth=2, markersize=8, label='L2 distance')\n",
    "axes[0].set_xlabel('k (number of neighbors)', fontsize=11)\n",
    "axes[0].set_ylabel('Validation Accuracy', fontsize=11)\n",
    "axes[0].set_title('kNN: Hyperparameter Tuning', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Confusion matrix for k=5\n",
    "knn.k = 5\n",
    "y_pred = knn.predict(X_val_flat[:200], distance_metric='l2')\n",
    "y_true = y_val[:200]\n",
    "\n",
    "confusion = np.zeros((10, 10))\n",
    "for true, pred in zip(y_true, y_pred):\n",
    "    confusion[true, pred] += 1\n",
    "\n",
    "im = axes[1].imshow(confusion, cmap='Blues')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=11)\n",
    "axes[1].set_ylabel('True Label', fontsize=11)\n",
    "axes[1].set_title('Confusion Matrix (k=5, L2)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ kNN: No training, slow at test time\")\n",
    "print(\"   â€¢ k=1: Overfits (memorizes noise)\")\n",
    "print(\"   â€¢ k too large: Underfits (averages too much)\")\n",
    "print(\"   â€¢ Best k: Found via validation set\")\n",
    "print(f\"   â€¢ Best accuracy: {max(max(accuracies_l1), max(accuracies_l2)):.1%} (baseline!)\")\n",
    "print(\"\\nâœ“ kNN complete! Let's do better with parametric models...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬3èŠ‚ï¼šçº¿æ€§åˆ†ç±»å™¨ - SVMå’ŒSoftmax\n",
    "\n",
    "**å‚æ•°åŒ–æ¨¡åž‹**ï¼šå­¦ä¹ æƒé‡çŸ©é˜µ$W$æ¥é¢„æµ‹åˆ†æ•°ã€‚\n",
    "\n",
    "## è¯„åˆ†å‡½æ•°\n",
    "\n",
    "$$f(x; W, b) = Wx + b$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $x \\in \\mathbb{R}^D$ï¼šè¾“å…¥å›¾åƒï¼ˆ3072åƒç´ ï¼‰\n",
    "- $W \\in \\mathbb{R}^{C \\times D}$ï¼šæƒé‡çŸ©é˜µï¼ˆ10 Ã— 3072ï¼‰\n",
    "- $b \\in \\mathbb{R}^C$ï¼šåç½®å‘é‡ï¼ˆ10,ï¼‰\n",
    "- è¾“å‡ºï¼š$f \\in \\mathbb{R}^C$ï¼šç±»åˆ«åˆ†æ•°ï¼ˆ10,ï¼‰\n",
    "\n",
    "## æŸå¤±å‡½æ•°\n",
    "\n",
    "### 1. å¤šç±»SVMæŸå¤±ï¼ˆHingeæŸå¤±ï¼‰\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^N \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$$\n",
    "\n",
    "å…¶ä¸­$\\Delta = 1$æ˜¯è¾¹ç•Œã€‚\n",
    "\n",
    "**ç›´è§‰**ï¼šæ­£ç¡®ç±»åˆ†æ•°åº”è‡³å°‘æ¯”é”™è¯¯ç±»é«˜$\\Delta$ã€‚\n",
    "\n",
    "### 2. SoftmaxæŸå¤±ï¼ˆäº¤å‰ç†µï¼‰\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N \\log\\left(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}}\\right)$$\n",
    "\n",
    "**ç›´è§‰**ï¼šæœ€å¤§åŒ–æ­£ç¡®ç±»çš„å¯¹æ•°æ¦‚çŽ‡ã€‚\n",
    "\n",
    "## æ­£åˆ™åŒ–\n",
    "\n",
    "æ·»åŠ æƒ©ç½šä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼š\n",
    "\n",
    "$$L_{\\text{total}} = L_{\\text{data}} + \\lambda R(W)$$\n",
    "\n",
    "å¸¸è§é€‰æ‹©ï¼š\n",
    "- **L2**ï¼š$R(W) = \\sum_{i,j} W_{ij}^2$ï¼ˆæƒé‡è¡°å‡ï¼‰\n",
    "- **L1**ï¼š$R(W) = \\sum_{i,j} |W_{ij}|$ï¼ˆç¨€ç–æ€§ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier:\n",
    "    \"\"\"Linear classifier with SVM or Softmax loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 3072, num_classes: int = 10):\n",
    "        self.W = np.random.randn(input_dim, num_classes) * 0.0001\n",
    "        self.b = np.zeros(num_classes)\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute class scores.\n",
    "        \n",
    "        Args:\n",
    "            X: (N, D) input data\n",
    "        \n",
    "        Returns:\n",
    "            scores: (N, C) class scores\n",
    "        \"\"\"\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def svm_loss(self, X: np.ndarray, y: np.ndarray, reg: float = 1e-5) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute SVM loss and gradients.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss\n",
    "            dW: Gradient of loss w.r.t. W\n",
    "            db: Gradient of loss w.r.t. b\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        scores = self.forward(X)  # (N, C)\n",
    "        \n",
    "        # Compute margins\n",
    "        correct_scores = scores[range(N), y].reshape(-1, 1)  # (N, 1)\n",
    "        margins = np.maximum(0, scores - correct_scores + 1)  # (N, C)\n",
    "        margins[range(N), y] = 0  # Don't count correct class\n",
    "        \n",
    "        # Loss\n",
    "        loss = np.sum(margins) / N\n",
    "        loss += reg * np.sum(self.W ** 2)  # L2 regularization\n",
    "        \n",
    "        # Gradients\n",
    "        binary = (margins > 0).astype(float)  # (N, C)\n",
    "        binary[range(N), y] = -np.sum(binary, axis=1)  # Correct class gets negative\n",
    "        \n",
    "        dW = (X.T @ binary) / N + 2 * reg * self.W\n",
    "        db = np.sum(binary, axis=0) / N\n",
    "        \n",
    "        return loss, dW, db\n",
    "    \n",
    "    def softmax_loss(self, X: np.ndarray, y: np.ndarray, reg: float = 1e-5) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute Softmax loss and gradients.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss\n",
    "            dW: Gradient of loss w.r.t. W\n",
    "            db: Gradient of loss w.r.t. b\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        scores = self.forward(X)  # (N, C)\n",
    "        \n",
    "        # Numerical stability: shift scores\n",
    "        scores -= np.max(scores, axis=1, keepdims=True)\n",
    "        \n",
    "        # Softmax probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # (N, C)\n",
    "        \n",
    "        # Loss\n",
    "        correct_log_probs = -np.log(probs[range(N), y] + 1e-10)\n",
    "        loss = np.sum(correct_log_probs) / N\n",
    "        loss += reg * np.sum(self.W ** 2)\n",
    "        \n",
    "        # Gradients\n",
    "        dscores = probs.copy()\n",
    "        dscores[range(N), y] -= 1  # Subtract 1 from correct class\n",
    "        dscores /= N\n",
    "        \n",
    "        dW = X.T @ dscores + 2 * reg * self.W\n",
    "        db = np.sum(dscores, axis=0)\n",
    "        \n",
    "        return loss, dW, db\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute classification accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "\n",
    "def train_linear_classifier(classifier: LinearClassifier,\n",
    "                           X_train: np.ndarray,\n",
    "                           y_train: np.ndarray,\n",
    "                           X_val: np.ndarray,\n",
    "                           y_val: np.ndarray,\n",
    "                           loss_function: str = 'softmax',\n",
    "                           learning_rate: float = 1e-3,\n",
    "                           reg: float = 1e-5,\n",
    "                           num_iters: int = 1000,\n",
    "                           batch_size: int = 200,\n",
    "                           verbose: bool = True) -> Dict:\n",
    "    \"\"\"Train linear classifier using SGD.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    N = X_train.shape[0]\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        # Sample mini-batch\n",
    "        batch_indices = np.random.choice(N, batch_size, replace=False)\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        \n",
    "        # Compute loss and gradients\n",
    "        if loss_function == 'svm':\n",
    "            loss, dW, db = classifier.svm_loss(X_batch, y_batch, reg)\n",
    "        else:  # softmax\n",
    "            loss, dW, db = classifier.softmax_loss(X_batch, y_batch, reg)\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Update parameters\n",
    "        classifier.W -= learning_rate * dW\n",
    "        classifier.b -= learning_rate * db\n",
    "        \n",
    "        # Check accuracy periodically\n",
    "        if it % 100 == 0:\n",
    "            train_acc = classifier.accuracy(X_train[:1000], y_train[:1000])\n",
    "            val_acc = classifier.accuracy(X_val, y_val)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iter {it:4d}/{num_iters}: Loss={loss:.4f}, Train Acc={train_acc:.2%}, Val Acc={val_acc:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'loss_history': loss_history,\n",
    "        'train_acc_history': train_acc_history,\n",
    "        'val_acc_history': val_acc_history\n",
    "    }\n",
    "\n",
    "\n",
    "# Train Softmax classifier\n",
    "print(\"Training Softmax Classifier...\\n\")\n",
    "\n",
    "softmax_clf = LinearClassifier()\n",
    "softmax_history = train_linear_classifier(\n",
    "    softmax_clf, X_train_flat, y_train, X_val_flat, y_val,\n",
    "    loss_function='softmax',\n",
    "    learning_rate=1e-3,\n",
    "    reg=1e-5,\n",
    "    num_iters=1000\n",
    ")\n",
    "\n",
    "# Train SVM classifier for comparison\n",
    "print(\"\\nTraining SVM Classifier...\\n\")\n",
    "\n",
    "svm_clf = LinearClassifier()\n",
    "svm_history = train_linear_classifier(\n",
    "    svm_clf, X_train_flat, y_train, X_val_flat, y_val,\n",
    "    loss_function='svm',\n",
    "    learning_rate=1e-3,\n",
    "    reg=1e-5,\n",
    "    num_iters=1000\n",
    ")\n",
    "\n",
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "axes[0].plot(softmax_history['loss_history'], label='Softmax', alpha=0.7)\n",
    "axes[0].plot(svm_history['loss_history'], label='SVM', alpha=0.7)\n",
    "axes[0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Training Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy curves\n",
    "iters_check = np.arange(0, 1000, 100)\n",
    "axes[1].plot(iters_check, softmax_history['val_acc_history'], 'o-', label='Softmax', linewidth=2)\n",
    "axes[1].plot(iters_check, svm_history['val_acc_history'], 's-', label='SVM', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1].set_ylabel('Validation Accuracy', fontsize=11)\n",
    "axes[1].set_title('Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Visualize learned weights (as images)\n",
    "W_img = softmax_clf.W.T.reshape(10, 32, 32, 3)  # (10, 32, 32, 3)\n",
    "W_grid = np.zeros((32*2, 32*5, 3))\n",
    "for i in range(10):\n",
    "    row, col = i // 5, i % 5\n",
    "    W_normalized = (W_img[i] - W_img[i].min()) / (W_img[i].max() - W_img[i].min() + 1e-10)\n",
    "    W_grid[row*32:(row+1)*32, col*32:(col+1)*32] = W_normalized\n",
    "\n",
    "axes[2].imshow(W_grid)\n",
    "axes[2].set_title('Learned Weight Templates', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final test accuracy\n",
    "test_acc_softmax = softmax_clf.accuracy(X_test_flat, y_test)\n",
    "test_acc_svm = svm_clf.accuracy(X_test_flat, y_test)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Final Test Accuracy:\")\n",
    "print(f\"  Softmax: {test_acc_softmax:.2%}\")\n",
    "print(f\"  SVM:     {test_acc_svm:.2%}\")\n",
    "print(f\"  kNN:     {max(max(accuracies_l1), max(accuracies_l2)):.2%} (baseline)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ Linear classifier: f(x) = Wx + b (one template per class)\")\n",
    "print(\"   â€¢ SVM: Margin-based (hinge loss)\")\n",
    "print(\"   â€¢ Softmax: Probability-based (cross-entropy)\")\n",
    "print(\"   â€¢ Both outperform kNN and train fast!\")\n",
    "print(\"   â€¢ Weights look like averaged class templates\")\n",
    "print(\"\\nâœ“ Linear classifiers complete! Let's add nonlinearity...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬4èŠ‚ï¼šä¼˜åŒ– - SGDã€åŠ¨é‡å’Œå­¦ä¹ çŽ‡è°ƒåº¦\n",
    "\n",
    "## éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰\n",
    "\n",
    "æ›´æ–°è§„åˆ™ï¼š\n",
    "$$w_{t+1} = w_t - \\eta \\nabla L(w_t)$$\n",
    "\n",
    "å…¶ä¸­$\\eta$æ˜¯å­¦ä¹ çŽ‡ã€‚\n",
    "\n",
    "## å¸¦åŠ¨é‡çš„SGD\n",
    "\n",
    "æ·»åŠ é€Ÿåº¦é¡¹ï¼š\n",
    "$$v_{t+1} = \\rho v_t - \\eta \\nabla L(w_t)$$\n",
    "$$w_{t+1} = w_t + v_{t+1}$$\n",
    "\n",
    "å…¶ä¸­$\\rho \\in [0, 1]$æ˜¯åŠ¨é‡ç³»æ•°ï¼ˆé€šå¸¸ä¸º0.9ï¼‰ã€‚\n",
    "\n",
    "**å¥½å¤„**ï¼šå¹³æ»‘æ›´æ–°ï¼ŒåŠ é€Ÿé€šè¿‡å³¡è°·ã€‚\n",
    "\n",
    "## å­¦ä¹ çŽ‡è°ƒåº¦\n",
    "\n",
    "**æ­¥è¡°å‡**ï¼š\n",
    "$$\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t / T \\rfloor}$$\n",
    "\n",
    "**æŒ‡æ•°è¡°å‡**ï¼š\n",
    "$$\\eta_t = \\eta_0 e^{-kt}$$\n",
    "\n",
    "**1/tè¡°å‡**ï¼š\n",
    "$$\\eta_t = \\frac{\\eta_0}{1 + kt}$$\n",
    "\n",
    "## çœ‹æŠ¤å­¦ä¹ è¿‡ç¨‹\n",
    "\n",
    "**CS231næ™ºæ…§**ï¼š\n",
    "1. ä»Žå°lrå¼€å§‹ï¼ˆ1e-3åˆ°1e-4ï¼‰\n",
    "2. ç›‘æŽ§æŸå¤±ï¼šåº”è¯¥å¹³ç¨³ä¸‹é™\n",
    "3. æ£€æŸ¥æ¢¯åº¦ï¼šä¸å¤ªå°ä¹Ÿä¸å¤ªå¤§\n",
    "4. å¯è§†åŒ–æƒé‡ï¼šåº”æ˜¾ç¤ºç»“æž„\n",
    "5. é¦–å…ˆè¿‡æ‹Ÿåˆå°æ•°æ®é›†ï¼ˆå¥å…¨æ€§æ£€æŸ¥ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base optimizer class.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-3):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Update parameter using gradient.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Vanilla SGD optimizer.\"\"\"\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray) -> np.ndarray:\n",
    "        return param - self.learning_rate * grad\n",
    "\n",
    "\n",
    "class SGDMomentum(Optimizer):\n",
    "    \"\"\"SGD with momentum.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-3, momentum: float = 0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.velocity = {}\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray, param_id: str = 'default') -> np.ndarray:\n",
    "        if param_id not in self.velocity:\n",
    "            self.velocity[param_id] = np.zeros_like(param)\n",
    "        \n",
    "        self.velocity[param_id] = self.momentum * self.velocity[param_id] - self.learning_rate * grad\n",
    "        return param + self.velocity[param_id]\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Adam optimizer (adaptive learning rates).\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-3, beta1: float = 0.9, beta2: float = 0.999):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = 1e-8\n",
    "        self.m = {}  # First moment\n",
    "        self.v = {}  # Second moment\n",
    "        self.t = {}  # Timestep\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray, param_id: str = 'default') -> np.ndarray:\n",
    "        if param_id not in self.m:\n",
    "            self.m[param_id] = np.zeros_like(param)\n",
    "            self.v[param_id] = np.zeros_like(param)\n",
    "            self.t[param_id] = 0\n",
    "        \n",
    "        self.t[param_id] += 1\n",
    "        t = self.t[param_id]\n",
    "        \n",
    "        # Update biased moments\n",
    "        self.m[param_id] = self.beta1 * self.m[param_id] + (1 - self.beta1) * grad\n",
    "        self.v[param_id] = self.beta2 * self.v[param_id] + (1 - self.beta2) * (grad ** 2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = self.m[param_id] / (1 - self.beta1 ** t)\n",
    "        v_hat = self.v[param_id] / (1 - self.beta2 ** t)\n",
    "        \n",
    "        # Update\n",
    "        return param - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "\n",
    "def learning_rate_schedule(initial_lr: float, iteration: int, schedule_type: str = 'step') -> float:\n",
    "    \"\"\"Compute learning rate with schedule.\n",
    "    \n",
    "    Args:\n",
    "        initial_lr: Initial learning rate\n",
    "        iteration: Current iteration\n",
    "        schedule_type: 'step', 'exp', or 'inverse'\n",
    "    \"\"\"\n",
    "    if schedule_type == 'step':\n",
    "        # Decay by 0.5 every 250 iterations\n",
    "        return initial_lr * (0.5 ** (iteration // 250))\n",
    "    elif schedule_type == 'exp':\n",
    "        # Exponential decay\n",
    "        return initial_lr * np.exp(-0.001 * iteration)\n",
    "    else:  # inverse\n",
    "        # 1/t decay\n",
    "        return initial_lr / (1 + 0.001 * iteration)\n",
    "\n",
    "\n",
    "# Compare optimizers\n",
    "print(\"Comparing optimizers...\\n\")\n",
    "\n",
    "optimizers = {\n",
    "    'SGD': SGD(learning_rate=1e-3),\n",
    "    'SGD+Momentum': SGDMomentum(learning_rate=1e-3, momentum=0.9),\n",
    "    'Adam': Adam(learning_rate=1e-3)\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "\n",
    "for name, optimizer in optimizers.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    clf = LinearClassifier()\n",
    "    \n",
    "    loss_history = []\n",
    "    for it in range(500):\n",
    "        batch_indices = np.random.choice(len(X_train_flat), 200)\n",
    "        X_batch = X_train_flat[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        \n",
    "        loss, dW, db = clf.softmax_loss(X_batch, y_batch, reg=1e-5)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        if isinstance(optimizer, (SGDMomentum, Adam)):\n",
    "            clf.W = optimizer.update(clf.W, dW, 'W')\n",
    "            clf.b = optimizer.update(clf.b, db, 'b')\n",
    "        else:\n",
    "            clf.W = optimizer.update(clf.W, dW)\n",
    "            clf.b = optimizer.update(clf.b, db)\n",
    "    \n",
    "    histories[name] = loss_history\n",
    "    final_acc = clf.accuracy(X_val_flat, y_val)\n",
    "    print(f\"  Final val acc: {final_acc:.2%}\\n\")\n",
    "\n",
    "# Visualize optimizer comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "for name, history in histories.items():\n",
    "    axes[0].plot(history, label=name, linewidth=2, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Optimizer Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Plot 2: Learning rate schedules\n",
    "iters = np.arange(1000)\n",
    "for schedule in ['step', 'exp', 'inverse']:\n",
    "    lrs = [learning_rate_schedule(1e-3, it, schedule) for it in iters]\n",
    "    axes[1].plot(iters, lrs, label=schedule.capitalize(), linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1].set_ylabel('Learning Rate', fontsize=11)\n",
    "axes[1].set_title('Learning Rate Schedules', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ SGD: Simple but can be slow\")\n",
    "print(\"   â€¢ Momentum: Smooths updates, accelerates convergence\")\n",
    "print(\"   â€¢ Adam: Adaptive rates, often works out-of-the-box\")\n",
    "print(\"   â€¢ Learning rate schedule: Helps fine-tuning\")\n",
    "print(\"   â€¢ Babysitting: Monitor loss, check gradients, visualize weights\")\n",
    "print(\"\\nâœ“ Optimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬5èŠ‚ï¼šç¥žç»ç½‘ç»œ - æ·»åŠ éžçº¿æ€§\n",
    "\n",
    "çº¿æ€§åˆ†ç±»å™¨æœ‰æ ¹æœ¬é™åˆ¶ã€‚ç¥žç»ç½‘ç»œé€šè¿‡éšè—å±‚æ·»åŠ **éžçº¿æ€§**ã€‚\n",
    "\n",
    "## 2å±‚ç¥žç»ç½‘ç»œ\n",
    "\n",
    "$$h = \\text{ReLU}(W_1 x + b_1)$$\n",
    "$$y = W_2 h + b_2$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $x \\in \\mathbb{R}^D$ï¼šè¾“å…¥ï¼ˆ3072ï¼‰\n",
    "- $W_1 \\in \\mathbb{R}^{D \\times H}$ï¼šç¬¬ä¸€å±‚æƒé‡\n",
    "- $h \\in \\mathbb{R}^H$ï¼šéšè—å±‚ï¼ˆä¾‹å¦‚H=100ï¼‰\n",
    "- $W_2 \\in \\mathbb{R}^{H \\times C}$ï¼šç¬¬äºŒå±‚æƒé‡\n",
    "- $y \\in \\mathbb{R}^C$ï¼šè¾“å‡ºåˆ†æ•°ï¼ˆ10ï¼‰\n",
    "\n",
    "## æ¿€æ´»å‡½æ•°\n",
    "\n",
    "**ReLU**ï¼ˆä¿®æ­£çº¿æ€§å•å…ƒï¼‰ï¼š\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "**Sigmoid**ï¼š\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "**Tanh**ï¼š\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "**ReLUæ›´å—é’ç**ï¼šå¿«é€Ÿã€æ— é¥±å’Œã€åœ¨å®žè·µä¸­æ•ˆæžœå¾ˆå¥½ã€‚\n",
    "\n",
    "## åå‘ä¼ æ’­\n",
    "\n",
    "é€šè¿‡è®¡ç®—å›¾çš„é“¾å¼æ³•åˆ™ï¼š\n",
    "\n",
    "1. å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ¿€æ´»\n",
    "2. åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦\n",
    "\n",
    "å¯¹äºŽReLUï¼š\n",
    "$$\\frac{\\partial \\text{ReLU}}{\\partial x} = \\begin{cases} 1 & \\text{å¦‚æžœ } x > 0 \\\\ 0 & \\text{å¦åˆ™} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    \"\"\"Two-layer fully-connected neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 3072, hidden_dim: int = 100, num_classes: int = 10):\n",
    "        \"\"\"Initialize network with Xavier/He initialization.\"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.params['b1'] = np.zeros(hidden_dim)\n",
    "        self.params['W2'] = np.random.randn(hidden_dim, num_classes) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.params['b2'] = np.zeros(num_classes)\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Forward pass with caching for backprop.\n",
    "        \n",
    "        Returns:\n",
    "            scores: (N, C) class scores\n",
    "            cache: Dictionary with intermediate values\n",
    "        \"\"\"\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "        # Layer 1: Linear + ReLU\n",
    "        z1 = X @ W1 + b1  # (N, H)\n",
    "        h1 = np.maximum(0, z1)  # ReLU\n",
    "        \n",
    "        # Layer 2: Linear\n",
    "        scores = h1 @ W2 + b2  # (N, C)\n",
    "        \n",
    "        cache = {'X': X, 'z1': z1, 'h1': h1}\n",
    "        return scores, cache\n",
    "    \n",
    "    def loss(self, X: np.ndarray, y: np.ndarray, reg: float = 0.0) -> Tuple[float, Dict]:\n",
    "        \"\"\"Compute loss and gradients.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss\n",
    "            grads: Dictionary with gradients for each parameter\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        scores, cache = self.forward(X)\n",
    "        \n",
    "        # Compute softmax loss\n",
    "        scores -= np.max(scores, axis=1, keepdims=True)  # Numerical stability\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        loss = -np.sum(np.log(probs[range(N), y] + 1e-10)) / N\n",
    "        loss += reg * (np.sum(self.params['W1']**2) + np.sum(self.params['W2']**2))\n",
    "        \n",
    "        # Backward pass\n",
    "        grads = {}\n",
    "        \n",
    "        # Gradient on scores\n",
    "        dscores = probs.copy()\n",
    "        dscores[range(N), y] -= 1\n",
    "        dscores /= N\n",
    "        \n",
    "        # Layer 2 gradients\n",
    "        grads['W2'] = cache['h1'].T @ dscores + 2 * reg * self.params['W2']\n",
    "        grads['b2'] = np.sum(dscores, axis=0)\n",
    "        \n",
    "        # Backprop to hidden layer\n",
    "        dh1 = dscores @ self.params['W2'].T\n",
    "        \n",
    "        # ReLU backward\n",
    "        dz1 = dh1 * (cache['z1'] > 0)  # ReLU derivative\n",
    "        \n",
    "        # Layer 1 gradients\n",
    "        grads['W1'] = cache['X'].T @ dz1 + 2 * reg * self.params['W1']\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "        \n",
    "        return loss, grads\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        scores, _ = self.forward(X)\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "\n",
    "def train_neural_network(net: TwoLayerNet,\n",
    "                        X_train: np.ndarray,\n",
    "                        y_train: np.ndarray,\n",
    "                        X_val: np.ndarray,\n",
    "                        y_val: np.ndarray,\n",
    "                        learning_rate: float = 1e-3,\n",
    "                        reg: float = 1e-5,\n",
    "                        num_iters: int = 2000,\n",
    "                        batch_size: int = 200,\n",
    "                        verbose: bool = True) -> Dict:\n",
    "    \"\"\"Train neural network using SGD with momentum.\"\"\"\n",
    "    N = X_train.shape[0]\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    # Use momentum\n",
    "    velocity = {key: np.zeros_like(val) for key, val in net.params.items()}\n",
    "    momentum = 0.9\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        # Sample mini-batch\n",
    "        batch_indices = np.random.choice(N, batch_size)\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        \n",
    "        # Compute loss and gradients\n",
    "        loss, grads = net.loss(X_batch, y_batch, reg)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Update with momentum\n",
    "        for param_name in net.params:\n",
    "            velocity[param_name] = momentum * velocity[param_name] - learning_rate * grads[param_name]\n",
    "            net.params[param_name] += velocity[param_name]\n",
    "        \n",
    "        # Check accuracy periodically\n",
    "        if it % 200 == 0:\n",
    "            train_acc = net.accuracy(X_train[:1000], y_train[:1000])\n",
    "            val_acc = net.accuracy(X_val, y_val)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iter {it:4d}: Loss={loss:.4f}, Train={train_acc:.2%}, Val={val_acc:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'loss_history': loss_history,\n",
    "        'train_acc_history': train_acc_history,\n",
    "        'val_acc_history': val_acc_history\n",
    "    }\n",
    "\n",
    "\n",
    "# Train neural network\n",
    "print(\"Training 2-Layer Neural Network...\\n\")\n",
    "\n",
    "net = TwoLayerNet(input_dim=3072, hidden_dim=100, num_classes=10)\n",
    "nn_history = train_neural_network(\n",
    "    net, X_train_flat, y_train, X_val_flat, y_val,\n",
    "    learning_rate=1e-3,\n",
    "    reg=1e-5,\n",
    "    num_iters=2000\n",
    ")\n",
    "\n",
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(nn_history['loss_history'], linewidth=2, color='darkblue')\n",
    "axes[0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Training Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy curves\n",
    "iters_check = np.arange(0, 2000, 200)\n",
    "axes[1].plot(iters_check, nn_history['train_acc_history'], 'o-', label='Train', linewidth=2)\n",
    "axes[1].plot(iters_check, nn_history['val_acc_history'], 's-', label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].set_title('Train vs Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Visualize first layer weights\n",
    "W1 = net.params['W1'].T  # (H, D)\n",
    "W1_img = W1[:64].reshape(64, 32, 32, 3)  # First 64 neurons\n",
    "W1_grid = np.zeros((32*8, 32*8, 3))\n",
    "for i in range(64):\n",
    "    row, col = i // 8, i % 8\n",
    "    w = W1_img[i]\n",
    "    w_norm = (w - w.min()) / (w.max() - w.min() + 1e-10)\n",
    "    W1_grid[row*32:(row+1)*32, col*32:(col+1)*32] = w_norm\n",
    "\n",
    "axes[2].imshow(W1_grid)\n",
    "axes[2].set_title('First Layer Weights (Filters)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test accuracy\n",
    "test_acc_nn = net.accuracy(X_test_flat, y_test)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Test Accuracy Comparison:\")\n",
    "print(f\"  Neural Network: {test_acc_nn:.2%}\")\n",
    "print(f\"  Softmax:        {test_acc_softmax:.2%}\")\n",
    "print(f\"  kNN:            {max(max(accuracies_l1), max(accuracies_l2)):.2%}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ Nonlinearity (ReLU) enables learning complex functions\")\n",
    "print(\"   â€¢ Hidden layer learns features, output layer classifies\")\n",
    "print(\"   â€¢ Neural network >> linear classifier!\")\n",
    "print(\"   â€¢ First layer weights look like edge/color detectors\")\n",
    "print(\"   â€¢ More layers = more capacity (but also harder to train)\")\n",
    "print(\"\\nâœ“ Neural networks complete! Now let's add conv layers...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬6èŠ‚ï¼šå·ç§¯ç¥žç»ç½‘ç»œï¼ˆCNNï¼‰\n",
    "\n",
    "**å…³é”®è§è§£**ï¼šå›¾åƒå…·æœ‰ç©ºé—´ç»“æž„ï¼å…¨è¿žæŽ¥å±‚å¿½ç•¥äº†è¿™ä¸€ç‚¹ã€‚\n",
    "\n",
    "## å·ç§¯å±‚\n",
    "\n",
    "å¯¹å±€éƒ¨åŒºåŸŸåº”ç”¨æ»¤æ³¢å™¨ï¼ˆæ ¸ï¼‰ï¼š\n",
    "$$y[i,j] = \\sum_{m,n} W[m,n] \\cdot x[i+m, j+n]$$\n",
    "\n",
    "**å‚æ•°**ï¼š\n",
    "- æ»¤æ³¢å™¨å¤§å°ï¼š$K \\times K$ï¼ˆé€šå¸¸3Ã—3æˆ–5Ã—5ï¼‰\n",
    "- æ­¥å¹…ï¼šç§»åŠ¨æ»¤æ³¢å™¨çš„ç¨‹åº¦ï¼ˆé€šå¸¸1æˆ–2ï¼‰\n",
    "- å¡«å……ï¼šåœ¨è¾¹ç•Œå‘¨å›´æ·»åŠ é›¶ä»¥ä¿æŒå¤§å°\n",
    "\n",
    "**è¾“å‡ºå¤§å°**ï¼š\n",
    "$$H_{\\text{out}} = \\frac{H + 2P - K}{S} + 1$$\n",
    "\n",
    "å…¶ä¸­$P$ = å¡«å……ï¼Œ$S$ = æ­¥å¹…ã€‚\n",
    "\n",
    "## æœ€å¤§æ± åŒ–\n",
    "\n",
    "é€šè¿‡åœ¨æ¯ä¸ªåŒºåŸŸä¸­å–æœ€å¤§å€¼è¿›è¡Œä¸‹é‡‡æ ·ï¼š\n",
    "$$y[i,j] = \\max_{m,n \\in \\text{region}} x[m,n]$$\n",
    "\n",
    "**å¥½å¤„**ï¼š\n",
    "- å‡å°‘ç©ºé—´å¤§å°\n",
    "- å¹³ç§»ä¸å˜æ€§\n",
    "- æŽ§åˆ¶è¿‡æ‹Ÿåˆ\n",
    "\n",
    "## CNNä¸ºä»€ä¹ˆæœ‰æ•ˆ\n",
    "\n",
    "1. **å‚æ•°å…±äº«**ï¼šç›¸åŒæ»¤æ³¢å™¨åº”ç”¨åˆ°å„å¤„ï¼ˆæ¯”FCå°‘å¾—å¤šçš„å‚æ•°ï¼‰\n",
    "2. **å±€éƒ¨è¿žæŽ¥**ï¼šæ¯ä¸ªç¥žç»å…ƒåªçœ‹å±€éƒ¨patch\n",
    "3. **å¹³ç§»ä¸å˜æ€§**ï¼šå›¾åƒå„å¤„çš„ç›¸åŒç‰¹å¾\n",
    "4. **åˆ†å±‚ç‰¹å¾**ï¼šæ—©æœŸå±‚=è¾¹ç¼˜ï¼Œæ™šæœŸå±‚=ç‰©ä½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_forward(X: np.ndarray, W: np.ndarray, b: np.ndarray, \n",
    "                   stride: int = 1, pad: int = 0) -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"Forward pass for convolutional layer.\n",
    "    \n",
    "    Args:\n",
    "        X: (N, C_in, H, W) input\n",
    "        W: (C_out, C_in, K, K) filters\n",
    "        b: (C_out,) biases\n",
    "        stride: Stride\n",
    "        pad: Padding\n",
    "    \n",
    "    Returns:\n",
    "        out: (N, C_out, H_out, W_out) output\n",
    "        cache: Tuple for backprop\n",
    "    \"\"\"\n",
    "    N, C_in, H, W = X.shape\n",
    "    C_out, _, K, _ = W.shape\n",
    "    \n",
    "    # Add padding\n",
    "    X_pad = np.pad(X, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
    "    \n",
    "    # Output dimensions\n",
    "    H_out = (H + 2*pad - K) // stride + 1\n",
    "    W_out = (W + 2*pad - K) // stride + 1\n",
    "    \n",
    "    # Initialize output\n",
    "    out = np.zeros((N, C_out, H_out, W_out))\n",
    "    \n",
    "    # Naive implementation (loop-based, slow but clear)\n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + K\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + K\n",
    "            \n",
    "            # Extract patch\n",
    "            X_patch = X_pad[:, :, h_start:h_end, w_start:w_end]  # (N, C_in, K, K)\n",
    "            \n",
    "            # Convolve each filter\n",
    "            for c in range(C_out):\n",
    "                out[:, c, i, j] = np.sum(X_patch * W[c], axis=(1, 2, 3)) + b[c]\n",
    "    \n",
    "    cache = (X, W, b, stride, pad)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def maxpool2d_forward(X: np.ndarray, pool_size: int = 2, stride: int = 2) -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"Forward pass for max pooling layer.\n",
    "    \n",
    "    Args:\n",
    "        X: (N, C, H, W) input\n",
    "        pool_size: Size of pooling window\n",
    "        stride: Stride\n",
    "    \n",
    "    Returns:\n",
    "        out: (N, C, H_out, W_out) output\n",
    "        cache: Tuple for backprop\n",
    "    \"\"\"\n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    H_out = (H - pool_size) // stride + 1\n",
    "    W_out = (W - pool_size) // stride + 1\n",
    "    \n",
    "    out = np.zeros((N, C, H_out, W_out))\n",
    "    \n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + pool_size\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + pool_size\n",
    "            \n",
    "            # Max over spatial window\n",
    "            X_patch = X[:, :, h_start:h_end, w_start:w_end]\n",
    "            out[:, :, i, j] = np.max(X_patch, axis=(2, 3))\n",
    "    \n",
    "    cache = (X, pool_size, stride)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_forward(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Forward pass for ReLU.\"\"\"\n",
    "    out = np.maximum(0, X)\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "# Test CNN layers\n",
    "print(\"Testing CNN layers...\\n\")\n",
    "\n",
    "# Test convolutional layer\n",
    "X_test = X_train[:10].transpose(0, 3, 1, 2)  # (N, C, H, W)\n",
    "W_test = np.random.randn(16, 3, 5, 5) * 0.01  # 16 filters, 5Ã—5, 3 channels\n",
    "b_test = np.zeros(16)\n",
    "\n",
    "out_conv, _ = conv2d_forward(X_test, W_test, b_test, stride=1, pad=2)\n",
    "print(f\"Conv layer: Input {X_test.shape} â†’ Output {out_conv.shape}\")\n",
    "\n",
    "# Test max pooling\n",
    "out_pool, _ = maxpool2d_forward(out_conv, pool_size=2, stride=2)\n",
    "print(f\"Max pool:   Input {out_conv.shape} â†’ Output {out_pool.shape}\")\n",
    "\n",
    "# Test ReLU\n",
    "out_relu, _ = relu_forward(out_pool)\n",
    "print(f\"ReLU:       Input {out_pool.shape} â†’ Output {out_relu.shape}\")\n",
    "\n",
    "# Visualize learned filters (example)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    # Visualize filter (normalize each channel separately)\n",
    "    filt = W_test[i].transpose(1, 2, 0)  # (K, K, 3)\n",
    "    filt_norm = (filt - filt.min()) / (filt.max() - filt.min() + 1e-10)\n",
    "    ax.imshow(filt_norm)\n",
    "    ax.set_title(f'Filter {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Random Conv Filters (5Ã—5, 3 channels)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ Conv layer: Apply filters to local regions\")\n",
    "print(\"   â€¢ Parameter sharing: Same filter everywhere (far fewer params)\")\n",
    "print(\"   â€¢ Max pooling: Downsample, translation invariance\")\n",
    "print(\"   â€¢ ReLU: Nonlinearity, fast and effective\")\n",
    "print(\"   â€¢ Stacking: Conv â†’ ReLU â†’ Pool â†’ repeat\")\n",
    "print(\"\\nâœ“ CNN layers complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬7èŠ‚ï¼šå®Œæ•´CNNæž¶æž„ - è¿·ä½ AlexNet\n",
    "\n",
    "è®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„32Ã—32å›¾åƒæž„å»ºä¸€ä¸ªç®€åŒ–çš„AlexNetã€‚\n",
    "\n",
    "## AlexNetæž¶æž„ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "\n",
    "```\n",
    "è¾“å…¥ï¼š32Ã—32Ã—3\n",
    "â†“\n",
    "Conv1ï¼š32ä¸ªæ»¤æ³¢å™¨ï¼Œ5Ã—5ï¼Œæ­¥å¹…1ï¼Œå¡«å……2 â†’ 32Ã—32Ã—32\n",
    "ReLU â†’ MaxPool (2Ã—2ï¼Œæ­¥å¹…2) â†’ 16Ã—16Ã—32\n",
    "â†“\n",
    "Conv2ï¼š64ä¸ªæ»¤æ³¢å™¨ï¼Œ3Ã—3ï¼Œæ­¥å¹…1ï¼Œå¡«å……1 â†’ 16Ã—16Ã—64\n",
    "ReLU â†’ MaxPool (2Ã—2ï¼Œæ­¥å¹…2) â†’ 8Ã—8Ã—64\n",
    "â†“\n",
    "å±•å¹³ â†’ 4096\n",
    "â†“\n",
    "FC1ï¼š4096 â†’ 256\n",
    "ReLU\n",
    "â†“\n",
    "FC2ï¼š256 â†’ 10\n",
    "Softmax\n",
    "```\n",
    "\n",
    "## å‚æ•°è®¡æ•°\n",
    "\n",
    "**å·ç§¯å±‚**ï¼š \n",
    "- Conv1ï¼š32 Ã— (5Ã—5Ã—3 + 1) = 2,432\n",
    "- Conv2ï¼š64 Ã— (3Ã—3Ã—32 + 1) = 18,496\n",
    "\n",
    "**FCå±‚**ï¼š\n",
    "- FC1ï¼š4096 Ã— 256 = 1,048,576\n",
    "- FC2ï¼š256 Ã— 10 = 2,560\n",
    "\n",
    "**æ€»è®¡**ï¼š~1.07Må‚æ•°ï¼ˆvsçº¯FCçš„~30Mï¼ï¼‰\n",
    "\n",
    "**è§è§£**ï¼šCNNæ¯”ç”¨äºŽå›¾åƒçš„å…¨è¿žæŽ¥ç½‘ç»œå‚æ•°æ•ˆçŽ‡é«˜å¾—å¤šï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    \"\"\"Simple CNN for image classification (toy AlexNet).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with He initialization.\"\"\"\n",
    "        self.params = {}\n",
    "        \n",
    "        # Conv1: 3 â†’ 32, 5Ã—5\n",
    "        self.params['W1'] = np.random.randn(32, 3, 5, 5) * np.sqrt(2.0 / (3*5*5))\n",
    "        self.params['b1'] = np.zeros(32)\n",
    "        \n",
    "        # Conv2: 32 â†’ 64, 3Ã—3\n",
    "        self.params['W2'] = np.random.randn(64, 32, 3, 3) * np.sqrt(2.0 / (32*3*3))\n",
    "        self.params['b2'] = np.zeros(64)\n",
    "        \n",
    "        # FC1: 4096 â†’ 256\n",
    "        self.params['W3'] = np.random.randn(4096, 256) * np.sqrt(2.0 / 4096)\n",
    "        self.params['b3'] = np.zeros(256)\n",
    "        \n",
    "        # FC2: 256 â†’ 10\n",
    "        self.params['W4'] = np.random.randn(256, 10) * np.sqrt(2.0 / 256)\n",
    "        self.params['b4'] = np.zeros(10)\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass (inference mode, simplified).\n",
    "        \n",
    "        Args:\n",
    "            X: (N, H, W, C) input images\n",
    "        \n",
    "        Returns:\n",
    "            scores: (N, 10) class scores\n",
    "        \"\"\"\n",
    "        # Convert to (N, C, H, W) for conv layers\n",
    "        X = X.transpose(0, 3, 1, 2)\n",
    "        \n",
    "        # Conv1 â†’ ReLU â†’ Pool\n",
    "        out, _ = conv2d_forward(X, self.params['W1'], self.params['b1'], stride=1, pad=2)\n",
    "        out, _ = relu_forward(out)\n",
    "        out, _ = maxpool2d_forward(out, pool_size=2, stride=2)\n",
    "        \n",
    "        # Conv2 â†’ ReLU â†’ Pool\n",
    "        out, _ = conv2d_forward(out, self.params['W2'], self.params['b2'], stride=1, pad=1)\n",
    "        out, _ = relu_forward(out)\n",
    "        out, _ = maxpool2d_forward(out, pool_size=2, stride=2)\n",
    "        \n",
    "        # Flatten\n",
    "        N = out.shape[0]\n",
    "        out = out.reshape(N, -1)  # (N, 4096)\n",
    "        \n",
    "        # FC1 â†’ ReLU\n",
    "        out = out @ self.params['W3'] + self.params['b3']\n",
    "        out = np.maximum(0, out)\n",
    "        \n",
    "        # FC2\n",
    "        scores = out @ self.params['W4'] + self.params['b4']\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "\n",
    "# Test CNN (note: full training would be slow in pure NumPy, so we'll test architecture)\n",
    "print(\"Building SimpleCNN (toy AlexNet)...\\n\")\n",
    "\n",
    "cnn = SimpleCNN()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.size for p in cnn.params.values())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "X_sample = X_train[:5]\n",
    "scores = cnn.forward(X_sample)\n",
    "print(f\"\\nForward pass test:\")\n",
    "print(f\"  Input shape:  {X_sample.shape}\")\n",
    "print(f\"  Output shape: {scores.shape}\")\n",
    "print(f\"  Predictions:  {cnn.predict(X_sample)}\")\n",
    "\n",
    "# Random initialization accuracy\n",
    "random_acc = cnn.accuracy(X_val[:100], y_val[:100])\n",
    "print(f\"\\nRandom initialization accuracy: {random_acc:.2%} (expected ~10% for 10 classes)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CNN Architecture Summary\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Layer 1: Conv (3â†’32, 5Ã—5) + ReLU + MaxPool  â†’  16Ã—16Ã—32\")\n",
    "print(f\"Layer 2: Conv (32â†’64, 3Ã—3) + ReLU + MaxPool â†’  8Ã—8Ã—64\")\n",
    "print(f\"Layer 3: Flatten                             â†’  4096\")\n",
    "print(f\"Layer 4: FC (4096â†’256) + ReLU                â†’  256\")\n",
    "print(f\"Layer 5: FC (256â†’10)                         â†’  10\")\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Equivalent FC network: ~30,000,000 parameters (30Ã— more!)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ CNNs: Stack Conv+ReLU+Pool, then FC layers\")\n",
    "print(\"   â€¢ Parameter efficiency: 1M params vs 30M for FC\")\n",
    "print(\"   â€¢ Spatial hierarchy: Early = edges, Late = objects\")\n",
    "print(\"   â€¢ AlexNet (2012): First ImageNet breakthrough with CNNs\")\n",
    "print(\"   â€¢ Modern CNNs: ResNet, EfficientNet, etc. (same principles!)\")\n",
    "print(\"\\nâœ“ CNN architecture complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬8èŠ‚ï¼šå¯è§†åŒ–ã€æ˜¾è‘—æ€§å›¾å’Œè¿ç§»å­¦ä¹ \n",
    "\n",
    "## å¯è§†åŒ–æŠ€æœ¯\n",
    "\n",
    "### 1. æ»¤æ³¢å™¨å¯è§†åŒ–\n",
    "- æ˜¾ç¤ºç¬¬ä¸€å±‚æ»¤æ³¢å™¨çš„æ ·å­\n",
    "- æ—©æœŸå±‚ï¼šè¾¹ç¼˜ã€é¢œè‰²ã€çº¹ç†\n",
    "\n",
    "### 2. æ¿€æ´»å›¾\n",
    "- æ˜¾ç¤ºç»™å®šè¾“å…¥çš„å“ªäº›ç¥žç»å…ƒæ¿€æ´»\n",
    "- æŸ¥çœ‹ç½‘ç»œæ£€æµ‹åˆ°ä»€ä¹ˆç‰¹å¾\n",
    "\n",
    "### 3. æ˜¾è‘—æ€§å›¾\n",
    "- è®¡ç®—è¾“å‡ºå¯¹è¾“å…¥çš„æ¢¯åº¦ï¼š$\\frac{\\partial y_c}{\\partial x}$\n",
    "- æ˜¾ç¤ºå“ªäº›åƒç´ å¯¹é¢„æµ‹æœ€é‡è¦\n",
    "\n",
    "### 4. ç±»åˆ«å¯è§†åŒ–\n",
    "- ç”Ÿæˆä½¿ç±»åˆ«åˆ†æ•°æœ€å¤§åŒ–çš„å›¾åƒ\n",
    "- æ­ç¤ºç½‘ç»œè®¤ä¸ºæ¯ä¸ªç±»åˆ«çœ‹èµ·æ¥åƒä»€ä¹ˆ\n",
    "\n",
    "## è¿ç§»å­¦ä¹ \n",
    "\n",
    "**å…³é”®è§è§£**ï¼šImageNetçš„ç‰¹å¾å¯ä»¥è½¬ç§»åˆ°å…¶ä»–ä»»åŠ¡ï¼\n",
    "\n",
    "**ç­–ç•¥**ï¼š\n",
    "1. åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼ˆImageNetï¼‰\n",
    "2. ä¸ºæ–°ä»»åŠ¡æ›¿æ¢æœ€ç»ˆå±‚\n",
    "3. åœ¨å°æ•°æ®é›†ä¸Šå¾®è°ƒ\n",
    "\n",
    "**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼šæ—©æœŸå±‚å­¦ä¹ é€šç”¨ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_map(net: SimpleCNN, X: np.ndarray, y: int) -> np.ndarray:\n",
    "    \"\"\"Compute saliency map for a single image.\n",
    "    \n",
    "    Args:\n",
    "        net: Trained network\n",
    "        X: (H, W, C) single image\n",
    "        y: Target class\n",
    "    \n",
    "    Returns:\n",
    "        saliency: (H, W) saliency map\n",
    "    \"\"\"\n",
    "    X = X[np.newaxis, ...]  # Add batch dimension\n",
    "    \n",
    "    # Forward pass\n",
    "    scores = net.forward(X)\n",
    "    \n",
    "    # Approximate gradient using finite differences\n",
    "    # (Full backprop implementation omitted for brevity)\n",
    "    eps = 1e-5\n",
    "    saliency = np.zeros((32, 32))\n",
    "    \n",
    "    # Sample-based approximation (for speed)\n",
    "    for i in range(0, 32, 4):\n",
    "        for j in range(0, 32, 4):\n",
    "            # Perturb pixel\n",
    "            X_perturb = X.copy()\n",
    "            X_perturb[0, i, j, :] += eps\n",
    "            \n",
    "            # Compute score change\n",
    "            scores_perturb = net.forward(X_perturb)\n",
    "            grad_approx = (scores_perturb[0, y] - scores[0, y]) / eps\n",
    "            saliency[i:i+4, j:j+4] = abs(grad_approx)\n",
    "    \n",
    "    return saliency\n",
    "\n",
    "\n",
    "def visualize_filters_and_activations(cnn: SimpleCNN, X_sample: np.ndarray):\n",
    "    \"\"\"Visualize learned filters and activation maps.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(15, 12))\n",
    "    \n",
    "    # Row 1: Input images\n",
    "    for i in range(4):\n",
    "        axes[0, i].imshow(X_sample[i])\n",
    "        axes[0, i].set_title(f'Input {i}')\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Row 2: First layer filters (sample)\n",
    "    W1 = cnn.params['W1']  # (32, 3, 5, 5)\n",
    "    for i in range(4):\n",
    "        filt = W1[i].transpose(1, 2, 0)  # (5, 5, 3)\n",
    "        filt_norm = (filt - filt.min()) / (filt.max() - filt.min() + 1e-10)\n",
    "        axes[1, i].imshow(filt_norm)\n",
    "        axes[1, i].set_title(f'Filter {i}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    # Row 3: Saliency maps\n",
    "    for i in range(4):\n",
    "        y_pred = cnn.predict(X_sample[i:i+1])[0]\n",
    "        saliency = compute_saliency_map(cnn, X_sample[i], y_pred)\n",
    "        axes[2, i].imshow(saliency, cmap='hot')\n",
    "        axes[2, i].set_title(f'Saliency (pred={y_pred})')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('CNN Visualization: Filters, Activations, Saliency', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize CNN\n",
    "print(\"Visualizing CNN components...\\n\")\n",
    "\n",
    "visualize_filters_and_activations(cnn, X_val[:4])\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights from visualization:\")\n",
    "print(\"   â€¢ First layer filters: Learn edge/color/texture detectors\")\n",
    "print(\"   â€¢ Saliency maps: Show which pixels matter for prediction\")\n",
    "print(\"   â€¢ Activation maps: Reveal what features network detects\")\n",
    "print(\"   â€¢ Class visualization: Generate prototypical examples\")\n",
    "\n",
    "print(\"\\nðŸŽ“ Transfer Learning Strategy:\")\n",
    "print(\"   1. Pre-train on ImageNet (millions of images)\")\n",
    "print(\"   2. Keep conv layers (feature extractor)\")\n",
    "print(\"   3. Replace FC layers for new task\")\n",
    "print(\"   4. Fine-tune on small target dataset\")\n",
    "print(\"   â†’ Works because early features are universal!\")\n",
    "\n",
    "print(\"\\nâœ“ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬9èŠ‚ï¼šçœ‹æŠ¤å­¦ä¹ è¿‡ç¨‹ - å®žç”¨æŠ€å·§\n",
    "\n",
    "## CS231nè®­ç»ƒç¥žç»ç½‘ç»œçš„æ™ºæ…§\n",
    "\n",
    "### 1. æ•°æ®é¢„å¤„ç†\n",
    "- **å½’ä¸€åŒ–**ï¼šå‡å€¼0ï¼Œæ ‡å‡†å·®1\n",
    "- **å¢žå¼º**ï¼šç¿»è½¬ã€è£å‰ªã€é¢œè‰²æŠ–åŠ¨\n",
    "- **ç™½åŒ–**ï¼šåŽ»ç›¸å…³ç‰¹å¾ï¼ˆPCAï¼‰\n",
    "\n",
    "### 2. æƒé‡åˆå§‹åŒ–\n",
    "- **Xavier**ï¼š$W \\sim N(0, 1/\\sqrt{n_{\\text{in}}})$ç”¨äºŽtanh\n",
    "- **He**ï¼š$W \\sim N(0, 2/\\sqrt{n_{\\text{in}}})$ç”¨äºŽReLU\n",
    "- **åç½®**ï¼šé€šå¸¸ä¸º0\n",
    "\n",
    "### 3. å¥å…¨æ€§æ£€æŸ¥\n",
    "- **è¿‡æ‹Ÿåˆå°æ•°æ®é›†**ï¼šåº”è¾¾åˆ°~100%å‡†ç¡®çŽ‡\n",
    "- **æ£€æŸ¥æŸå¤±**ï¼šåˆå§‹æŸå¤±åº”ä¸Žç†è®ºåŒ¹é…\n",
    "  - Cç±»çš„Softmaxï¼š$-\\log(1/C)$\n",
    "- **æ¢¯åº¦æ£€æŸ¥**ï¼šæ•°å€¼vsåˆ†æžæ¢¯åº¦\n",
    "\n",
    "### 4. è¶…å‚æ•°è°ƒä¼˜\n",
    "- **å­¦ä¹ çŽ‡**ï¼šæœ€é‡è¦ï¼\n",
    "  - å¤ªé«˜ï¼šæŸå¤±çˆ†ç‚¸\n",
    "  - å¤ªä½Žï¼šæ— å­¦ä¹ \n",
    "  - æœ€ä½³ç‚¹ï¼šæŸå¤±å¹³ç¨³ä¸‹é™\n",
    "- **æ­£åˆ™åŒ–**ï¼šä»Ž1e-5å¼€å§‹ï¼Œåœ¨éªŒè¯é›†ä¸Šè°ƒä¼˜\n",
    "- **æ‰¹æ¬¡å¤§å°**ï¼šé€šå¸¸32-256\n",
    "\n",
    "### 5. ç›‘æŽ§è®­ç»ƒ\n",
    "- **æŸå¤±æ›²çº¿**ï¼šåº”å¹³ç¨³ä¸‹é™\n",
    "- **è®­ç»ƒ/éªŒè¯å·®è·**ï¼šæŒ‡ç¤ºè¿‡æ‹Ÿåˆ\n",
    "- **æƒé‡æ›´æ–°**ï¼šæ¯æ¬¡è¿­ä»£çº¦æƒé‡çš„1e-3\n",
    "- **æ¿€æ´»ç›´æ–¹å›¾**ï¼šæ£€æŸ¥æ­»äº¡ç¥žç»å…ƒ\n",
    "\n",
    "### 6. å¸¸è§é”™è¯¯\n",
    "- å¿˜è®°å½’ä¸€åŒ–æ•°æ®\n",
    "- å­¦ä¹ çŽ‡å¤ªé«˜/å¤ªä½Ž\n",
    "- æ­£åˆ™åŒ–å¤ªå¼º\n",
    "- æ‰¹æ¬¡å¤§å°å¤ªå°ï¼ˆæ¢¯åº¦å™ªå£°ï¼‰\n",
    "- æ²¡æœ‰æ­£ç¡®ä½¿ç”¨éªŒè¯é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate babysitting tips\n",
    "\n",
    "def sanity_check_loss(num_classes: int = 10) -> float:\n",
    "    \"\"\"Expected initial loss for softmax with random weights.\"\"\"\n",
    "    return -np.log(1.0 / num_classes)\n",
    "\n",
    "\n",
    "def overfit_small_dataset(net: TwoLayerNet, X_small: np.ndarray, y_small: np.ndarray, num_iters: int = 500):\n",
    "    \"\"\"Sanity check: Should be able to overfit small dataset.\"\"\"\n",
    "    print(\"Sanity check: Overfitting 50 samples...\")\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        loss, grads = net.loss(X_small, y_small, reg=0)  # No regularization\n",
    "        losses.append(loss)\n",
    "        accs.append(net.accuracy(X_small, y_small))\n",
    "        \n",
    "        # Large learning rate for overfitting\n",
    "        for param in net.params:\n",
    "            net.params[param] -= 1e-2 * grads[param]\n",
    "    \n",
    "    return losses, accs\n",
    "\n",
    "\n",
    "def plot_training_diagnostics(history: Dict):\n",
    "    \"\"\"Plot comprehensive training diagnostics.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Plot 1: Loss curve (log scale)\n",
    "    axes[0, 0].plot(history['loss_history'])\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss (log scale)')\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Train vs Val accuracy\n",
    "    iters = np.arange(0, len(history['loss_history']), len(history['loss_history'])//len(history['train_acc_history']))\n",
    "    axes[0, 1].plot(iters, history['train_acc_history'], label='Train')\n",
    "    axes[0, 1].plot(iters, history['val_acc_history'], label='Val')\n",
    "    axes[0, 1].set_xlabel('Iteration')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Train/Val Accuracy Gap')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Learning rate schedule\n",
    "    iters_range = np.arange(len(history['loss_history']))\n",
    "    lrs = [learning_rate_schedule(1e-3, it, 'step') for it in iters_range]\n",
    "    axes[0, 2].plot(iters_range, lrs)\n",
    "    axes[0, 2].set_xlabel('Iteration')\n",
    "    axes[0, 2].set_ylabel('Learning Rate')\n",
    "    axes[0, 2].set_title('Learning Rate Schedule')\n",
    "    axes[0, 2].set_yscale('log')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Loss histogram\n",
    "    axes[1, 0].hist(history['loss_history'][100:], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Loss')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Loss Distribution')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 5: Loss smoothness (gradient of loss)\n",
    "    loss_grad = np.diff(history['loss_history'])\n",
    "    axes[1, 1].plot(loss_grad, alpha=0.5)\n",
    "    axes[1, 1].plot(np.convolve(loss_grad, np.ones(50)/50, mode='valid'), linewidth=2, label='Smoothed')\n",
    "    axes[1, 1].set_xlabel('Iteration')\n",
    "    axes[1, 1].set_ylabel('Loss Gradient')\n",
    "    axes[1, 1].set_title('Loss Change Rate')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Overfitting indicator\n",
    "    train_val_gap = np.array(history['train_acc_history']) - np.array(history['val_acc_history'])\n",
    "    axes[1, 2].plot(iters, train_val_gap, linewidth=2, color='red')\n",
    "    axes[1, 2].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1, 2].set_xlabel('Iteration')\n",
    "    axes[1, 2].set_ylabel('Train - Val Accuracy')\n",
    "    axes[1, 2].set_title('Overfitting Indicator')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].fill_between(iters, 0, train_val_gap, where=(train_val_gap > 0), \n",
    "                            color='red', alpha=0.3, label='Overfitting')\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    plt.suptitle('Training Diagnostics: Babysitting the Learning Process', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run sanity checks\n",
    "print(\"=\" * 70)\n",
    "print(\"Babysitting Tips: Practical Training Checks\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: Expected initial loss\n",
    "expected_loss = sanity_check_loss(10)\n",
    "print(f\"\\n1. Expected initial loss (10 classes): {expected_loss:.4f}\")\n",
    "print(f\"   (Random softmax: -log(1/10) = -log(0.1) â‰ˆ 2.303)\")\n",
    "\n",
    "# Check 2: Overfit small dataset\n",
    "print(\"\\n2. Sanity check: Overfitting 50 samples...\")\n",
    "small_net = TwoLayerNet()\n",
    "X_small = X_train_flat[:50]\n",
    "y_small = y_train[:50]\n",
    "\n",
    "losses_overfit, accs_overfit = overfit_small_dataset(small_net, X_small, y_small)\n",
    "print(f\"   Initial accuracy: {accs_overfit[0]:.2%}\")\n",
    "print(f\"   Final accuracy:   {accs_overfit[-1]:.2%}\")\n",
    "print(f\"   âœ“ Can overfit! (Should reach ~100%)\")\n",
    "\n",
    "# Check 3: Plot diagnostics\n",
    "print(\"\\n3. Training diagnostics (using previous neural network training)...\")\n",
    "plot_training_diagnostics(nn_history)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CS231n Babysitting Checklist:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nâœ“ 1. Data preprocessing: Normalize, augment\")\n",
    "print(\"âœ“ 2. Weight initialization: Xavier/He\")\n",
    "print(\"âœ“ 3. Sanity checks: Overfit small set, check initial loss\")\n",
    "print(\"âœ“ 4. Learning rate: Start with 1e-3, tune carefully\")\n",
    "print(\"âœ“ 5. Monitor: Loss curves, train/val gap, gradients\")\n",
    "print(\"âœ“ 6. Regularization: Start weak, increase if overfitting\")\n",
    "print(\"\\nðŸ’¡ Rule of thumb: If loss doesn't decrease, check learning rate!\")\n",
    "print(\"\\nâœ“ Babysitting complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬10èŠ‚ï¼šçŽ°ä»£æž¶æž„åŠè¶…è¶Š\n",
    "\n",
    "CS231næä¾›äº†åŸºç¡€ã€‚çŽ°ä»£æž¶æž„å»ºç«‹åœ¨è¿™äº›åŽŸåˆ™ä¹‹ä¸Šã€‚\n",
    "\n",
    "## VGGï¼ˆ2014ï¼‰\n",
    "- **å…³é”®æƒ³æ³•**ï¼šå †å è®¸å¤šå°çš„ï¼ˆ3Ã—3ï¼‰å·ç§¯\n",
    "- æ›´æ·±çš„ç½‘ç»œ > æ›´å®½çš„ç½‘ç»œ\n",
    "- ç®€å•ã€ç»Ÿä¸€çš„æž¶æž„\n",
    "\n",
    "## ResNetï¼ˆ2015ï¼‰- è§è®ºæ–‡#10ï¼\n",
    "- **å…³é”®æƒ³æ³•**ï¼šè·³è·ƒè¿žæŽ¥\n",
    "- $F(x) = H(x) - x$ï¼ˆå­¦ä¹ æ®‹å·®ï¼‰\n",
    "- ä½¿è®­ç»ƒ1000+å±‚ç½‘ç»œæˆä¸ºå¯èƒ½\n",
    "- è§£å†³é€€åŒ–é—®é¢˜\n",
    "\n",
    "## çŽ°ä»£è¶‹åŠ¿ï¼ˆ2020sï¼‰\n",
    "\n",
    "### Vision Transformers (ViT)\n",
    "- ç”¨è‡ªæ³¨æ„åŠ›æ›¿æ¢å·ç§¯\n",
    "- å°†å›¾åƒè§†ä¸ºpatchåºåˆ—\n",
    "- åœ¨æ›´å¤§è§„æ¨¡ä¸Šæ‰©å±•æ›´å¥½\n",
    "\n",
    "### EfficientNet\n",
    "- å¤åˆç¼©æ”¾ï¼šæ·±åº¦+å®½åº¦+åˆ†è¾¨çŽ‡\n",
    "- ç¥žç»æž¶æž„æœç´¢\n",
    "- ç”¨æ›´å°‘å‚æ•°è¾¾åˆ°SOTA\n",
    "\n",
    "### æ‰©æ•£æ¨¡åž‹\n",
    "- ç”Ÿæˆæ¨¡åž‹ï¼ˆDALL-Eã€Stable Diffusionï¼‰\n",
    "- ä»ç„¶ä½¿ç”¨å·ç§¯éª¨å¹²ï¼\n",
    "\n",
    "## å¤§å±€è§‚\n",
    "\n",
    "CS231næ•™æŽˆ**æ—¶é—´æ€§åŽŸåˆ™**ï¼š\n",
    "1. **è¡¨ç¤ºå­¦ä¹ **ï¼šå­¦ä¹ ç‰¹å¾ï¼Œè€Œéžæ‰‹å·¥åˆ¶ä½œ\n",
    "2. **åˆ†å±‚ç‰¹å¾**ï¼šä½Žçº§â†’é«˜çº§\n",
    "3. **å½’çº³åå·®**ï¼šCNNç”¨äºŽå›¾åƒï¼ŒRNNç”¨äºŽåºåˆ—\n",
    "4. **ä¼˜åŒ–**ï¼šæ¢¯åº¦ã€åå‘ä¼ æ’­ã€SGD\n",
    "5. **æ­£åˆ™åŒ–**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "\n",
    "è¿™äº›é€‚ç”¨äºŽ**æ‰€æœ‰**æ·±åº¦å­¦ä¹ â€”â€”ä¸ä»…ä»…æ˜¯è§†è§‰ï¼\n",
    "\n",
    "---\n",
    "\n",
    "## Sutskever 30è”ç³»\n",
    "\n",
    "CS231nè¿žæŽ¥äº†å¤šç¯‡è®ºæ–‡ï¼š\n",
    "- **#7**ï¼šAlexNetï¼ˆCNNsç”¨äºŽImageNetï¼‰\n",
    "- **#10**ï¼šResNetï¼ˆè·³è·ƒè¿žæŽ¥ï¼‰\n",
    "- **#11**ï¼šæ‰©å¼ å·ç§¯ï¼ˆæ„Ÿå—é‡Žï¼‰\n",
    "- **#13**ï¼šTransformersï¼ˆè§†è§‰æ³¨æ„åŠ›ï¼‰\n",
    "\n",
    "**æœ¬ç¬”è®°æœ¬æ˜¯ä½ çš„è§†è§‰åŸºç¡€ï¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and comparison\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CS231n: Complete Computer Vision Pipeline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary table\n",
    "results_summary = {\n",
    "    'Method': ['kNN', 'Linear (Softmax)', 'Neural Network (2-layer)', 'CNN (Mini-AlexNet)'],\n",
    "    'Parameters': ['0 (memorize)', '~31K', '~1M', '~1M'],\n",
    "    'Accuracy': [f\"{max(max(accuracies_l1), max(accuracies_l2)):.1%}\", \n",
    "                f\"{test_acc_softmax:.1%}\",\n",
    "                f\"{test_acc_nn:.1%}\",\n",
    "                \"~60-70% (if trained)\"],\n",
    "    'Speed': ['Slow (test)', 'Fast', 'Fast', 'Medium'],\n",
    "    'Key Insight': ['No training', 'One template per class', 'Nonlinear features', 'Spatial structure']\n",
    "}\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"-\"*70)\n",
    "for i in range(len(results_summary['Method'])):\n",
    "    print(f\"{results_summary['Method'][i]:25s} | \"\n",
    "          f\"Params: {results_summary['Parameters'][i]:10s} | \"\n",
    "          f\"Acc: {results_summary['Accuracy'][i]:10s}\")\n",
    "    print(f\"{'':27s}   {results_summary['Key Insight'][i]}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Key Takeaways from CS231n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "takeaways = [\n",
    "    \"1. IMAGE CLASSIFICATION PIPELINE\",\n",
    "    \"   â€¢ Data â†’ Model â†’ Loss â†’ Optimization â†’ Prediction\",\n",
    "    \"   â€¢ Each component matters!\",\n",
    "    \"\",\n",
    "    \"2. MODEL EVOLUTION\",\n",
    "    \"   â€¢ kNN â†’ Linear â†’ NN â†’ CNN â†’ ResNet â†’ Transformers\",\n",
    "    \"   â€¢ Each step adds capacity and inductive bias\",\n",
    "    \"\",\n",
    "    \"3. CONVOLUTIONAL NETWORKS\",\n",
    "    \"   â€¢ Conv layers: Local connectivity, parameter sharing\",\n",
    "    \"   â€¢ Pooling: Downsampling, invariance\",\n",
    "    \"   â€¢ Hierarchy: Edges â†’ textures â†’ parts â†’ objects\",\n",
    "    \"\",\n",
    "    \"4. TRAINING TECHNIQUES\",\n",
    "    \"   â€¢ SGD with momentum, learning rate schedules\",\n",
    "    \"   â€¢ Xavier/He initialization\",\n",
    "    \"   â€¢ Regularization: L2, dropout, data augmentation\",\n",
    "    \"\",\n",
    "    \"5. BABYSITTING NEURAL NETS\",\n",
    "    \"   â€¢ Sanity checks: overfit small set, check initial loss\",\n",
    "    \"   â€¢ Monitor: loss curves, train/val gap, gradients\",\n",
    "    \"   â€¢ Hyperparameter tuning: learning rate is most important!\",\n",
    "    \"\",\n",
    "    \"6. VISUALIZATION\",\n",
    "    \"   â€¢ Understand what network learns\",\n",
    "    \"   â€¢ Filters, activations, saliency maps\",\n",
    "    \"   â€¢ Debugging tool and interpretability\",\n",
    "    \"\",\n",
    "    \"7. TRANSFER LEARNING\",\n",
    "    \"   â€¢ Pre-train on ImageNet, fine-tune on target task\",\n",
    "    \"   â€¢ Early features are universal\",\n",
    "    \"   â€¢ Enables learning from small datasets\",\n",
    "]\n",
    "\n",
    "for line in takeaways:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Beyond CS231n: Modern Vision\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâ€¢ ResNet (2015): Skip connections â†’ 1000+ layers\")\n",
    "print(\"â€¢ DenseNet (2016): Dense connections\")\n",
    "print(\"â€¢ EfficientNet (2019): NAS + compound scaling\")\n",
    "print(\"â€¢ Vision Transformers (2020): Attention for vision\")\n",
    "print(\"â€¢ ConvNeXt (2022): Modernized CNNs\")\n",
    "print(\"â€¢ Diffusion Models (2022): DALL-E, Stable Diffusion\")\n",
    "print(\"\\nâ†’ All build on CS231n foundations!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ“ CS231n: Complete! You've learned vision from first principles.\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nWhat you can do now:\")\n",
    "print(\"  âœ“ Understand how CNNs work (from scratch!)\")\n",
    "print(\"  âœ“ Train vision models (optimization, regularization)\")\n",
    "print(\"  âœ“ Debug neural networks (babysitting tips)\")\n",
    "print(\"  âœ“ Read modern papers (you have the foundation!)\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  â†’ Implement in PyTorch for real datasets\")\n",
    "print(\"  â†’ Study ResNet (Paper #10 in this repo!)\")\n",
    "print(\"  â†’ Explore transformers (Paper #13)\")\n",
    "print(\"  â†’ Build your own vision systems!\")\n",
    "print(\"\\nâœ¨ Welcome to computer vision! âœ¨\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
