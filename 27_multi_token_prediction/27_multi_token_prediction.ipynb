{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 论文27：通过多标记预测实现更好更快的大型语言模型\n## Meta AI Research (2024)\n\n### 多标记预测\n\n核心洞察：训练语言模型同时预测多个未来的标记。提高样本效率和生成质量！"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 标准单标记预测\n\n传统语言建模:\n```\n输入:  [w1, w2, w3, w4]\n预测: w5\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "class SingleTokenRNN:\n",
    "    \"\"\"Standard RNN with single-token prediction\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embeddings\n",
    "        self.W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        \n",
    "        # RNN weights\n",
    "        self.W_xh = np.random.randn(hidden_dim, embedding_dim) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.b_h = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Output projection (predict next token)\n",
    "        self.W_out = np.random.randn(vocab_size, hidden_dim) * 0.01\n",
    "        self.b_out = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        input_seq: list of token indices\n",
    "        Returns: predictions for next token at each position\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        predictions = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        for token_idx in input_seq:\n",
    "            # Embed\n",
    "            x = self.W_embed[token_idx].reshape(-1, 1)\n",
    "            \n",
    "            # RNN step\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            \n",
    "            # Predict next token\n",
    "            logits = np.dot(self.W_out, h) + self.b_out\n",
    "            probs = softmax(logits.T)\n",
    "            \n",
    "            predictions.append(probs.flatten())\n",
    "            hidden_states.append(h.copy())\n",
    "        \n",
    "        return predictions, hidden_states\n",
    "\n",
    "# Test\n",
    "vocab_size = 50\n",
    "single_model = SingleTokenRNN(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "test_seq = [1, 2, 3, 4]\n",
    "preds, _ = single_model.forward(test_seq)\n",
    "print(f\"Input sequence length: {len(test_seq)}\")\n",
    "print(f\"Predictions shape: {len(preds)} x {len(preds[0])}\")\n",
    "print(f\"Predicts: 1 token ahead at each position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 多标记预测\n\n预测多个未来的标记:\n```\n输入:  [w1, w2, w3, w4]\n预测: w5, w6, w7  (提前 3 个标记！)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTokenRNN:\n",
    "    \"\"\"RNN with multi-token prediction\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_future_tokens=3):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_future_tokens = num_future_tokens\n",
    "        \n",
    "        # Shared embeddings and RNN\n",
    "        self.W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        self.W_xh = np.random.randn(hidden_dim, embedding_dim) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.b_h = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Multiple output heads (one per future position)\n",
    "        self.output_heads = []\n",
    "        for i in range(num_future_tokens):\n",
    "            W_out = np.random.randn(vocab_size, hidden_dim) * 0.01\n",
    "            b_out = np.zeros((vocab_size, 1))\n",
    "            self.output_heads.append((W_out, b_out))\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Returns: predictions for next N tokens at each position\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        multi_predictions = []  # List of (pred_t+1, pred_t+2, ..., pred_t+N)\n",
    "        hidden_states = []\n",
    "        \n",
    "        for token_idx in input_seq:\n",
    "            # Embed\n",
    "            x = self.W_embed[token_idx].reshape(-1, 1)\n",
    "            \n",
    "            # RNN step\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            \n",
    "            # Predict next N tokens using separate heads\n",
    "            position_preds = []\n",
    "            for W_out, b_out in self.output_heads:\n",
    "                logits = np.dot(W_out, h) + b_out\n",
    "                probs = softmax(logits.T)\n",
    "                position_preds.append(probs.flatten())\n",
    "            \n",
    "            multi_predictions.append(position_preds)\n",
    "            hidden_states.append(h.copy())\n",
    "        \n",
    "        return multi_predictions, hidden_states\n",
    "\n",
    "# Test\n",
    "multi_model = MultiTokenRNN(vocab_size, embedding_dim=32, hidden_dim=64, num_future_tokens=3)\n",
    "multi_preds, _ = multi_model.forward(test_seq)\n",
    "print(f\"Input sequence length: {len(test_seq)}\")\n",
    "print(f\"Multi-predictions: {len(multi_preds)} positions\")\n",
    "print(f\"At each position: {len(multi_preds[0])} future tokens\")\n",
    "print(f\"Each prediction shape: {multi_preds[0][0].shape}\")\n",
    "print(f\"\\nPredicts: {len(multi_preds[0])} tokens ahead at each position!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 合成文本数据"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_sequences(vocab_size=50, num_sequences=1000, seq_length=20):\n",
    "    \"\"\"\n",
    "    Generate synthetic sequences with patterns\n",
    "    Pattern: arithmetic progressions (e.g., 1, 2, 3, 4, ...)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    for _ in range(num_sequences):\n",
    "        # Random starting point and step\n",
    "        start = np.random.randint(0, vocab_size // 2)\n",
    "        step = np.random.randint(1, 3)\n",
    "        \n",
    "        # Generate arithmetic sequence\n",
    "        seq = [(start + i * step) % vocab_size for i in range(seq_length)]\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Generate data\n",
    "train_sequences = generate_synthetic_sequences(vocab_size, num_sequences=1000, seq_length=20)\n",
    "test_sequences = generate_synthetic_sequences(vocab_size, num_sequences=200, seq_length=20)\n",
    "\n",
    "print(f\"Training sequences: {len(train_sequences)}\")\n",
    "print(f\"Example sequence: {train_sequences[0][:10]}...\")\n",
    "print(f\"Pattern: arithmetic progression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 训练：单标记预测"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_token(model, sequences, epochs=50, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train with standard next-token prediction\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for seq in sequences:\n",
    "            # Predict next token at each position\n",
    "            for i in range(len(seq) - 1):\n",
    "                input_tokens = seq[:i+1]\n",
    "                target_token = seq[i+1]\n",
    "                \n",
    "                # Forward\n",
    "                predictions, _ = model.forward(input_tokens)\n",
    "                pred_probs = predictions[-1]  # Last position prediction\n",
    "                \n",
    "                # Loss\n",
    "                loss = -np.log(pred_probs[target_token] + 1e-8)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "                # Backward (simplified - just track loss)\n",
    "        \n",
    "        avg_loss = epoch_loss / (len(sequences) * (len(seq) - 1))\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train single-token model\n",
    "print(\"Training Single-Token Model...\\n\")\n",
    "single_losses = train_single_token(single_model, train_sequences[:100], epochs=30)\n",
    "print(f\"\\nFinal loss: {single_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 训练：多标记预测"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_token(model, sequences, epochs=50, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train with multi-token prediction\n",
    "    Loss = sum of losses for all future positions\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        num_predictions = 0\n",
    "        \n",
    "        for seq in sequences:\n",
    "            # Predict multiple tokens at each position\n",
    "            for i in range(len(seq) - model.num_future_tokens):\n",
    "                input_tokens = seq[:i+1]\n",
    "                target_tokens = seq[i+1:i+1+model.num_future_tokens]\n",
    "                \n",
    "                # Forward\n",
    "                multi_preds, _ = model.forward(input_tokens)\n",
    "                position_preds = multi_preds[-1]  # Last position predictions\n",
    "                \n",
    "                # Loss for each future position\n",
    "                for j, (pred_probs, target) in enumerate(zip(position_preds, target_tokens)):\n",
    "                    loss = -np.log(pred_probs[target] + 1e-8)\n",
    "                    epoch_loss += loss\n",
    "                    num_predictions += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / num_predictions if num_predictions > 0 else 0\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train multi-token model\n",
    "print(\"\\nTraining Multi-Token Model (3 tokens ahead)...\\n\")\n",
    "multi_losses = train_multi_token(multi_model, train_sequences[:100], epochs=30)\n",
    "print(f\"\\nFinal loss: {multi_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 比较学习曲线"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(single_losses, label='Single-Token Prediction', linewidth=2, marker='o', markersize=4)\n",
    "plt.plot(multi_losses, label='Multi-Token Prediction (3 ahead)', linewidth=2, marker='s', markersize=4)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.title('Learning Curves: Single vs Multi-Token Prediction', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSingle-token final loss: {single_losses[-1]:.4f}\")\n",
    "print(f\"Multi-token final loss: {multi_losses[-1]:.4f}\")\n",
    "print(f\"\\nMulti-token prediction provides richer training signal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 评估：预测准确率"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_token(model, sequences):\n",
    "    \"\"\"Evaluate next-token prediction accuracy\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - 1):\n",
    "            input_tokens = seq[:i+1]\n",
    "            target = seq[i+1]\n",
    "            \n",
    "            predictions, _ = model.forward(input_tokens)\n",
    "            pred_token = np.argmax(predictions[-1])\n",
    "            \n",
    "            if pred_token == target:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "def evaluate_multi_token(model, sequences, position=0):\n",
    "    \"\"\"Evaluate multi-token prediction accuracy at specific future position\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - model.num_future_tokens):\n",
    "            input_tokens = seq[:i+1]\n",
    "            target = seq[i+1+position]\n",
    "            \n",
    "            multi_preds, _ = model.forward(input_tokens)\n",
    "            pred_probs = multi_preds[-1][position]  # Prediction for position ahead\n",
    "            pred_token = np.argmax(pred_probs)\n",
    "            \n",
    "            if pred_token == target:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Evaluate both models\n",
    "single_acc = evaluate_single_token(single_model, test_sequences[:50])\n",
    "multi_acc_t1 = evaluate_multi_token(multi_model, test_sequences[:50], position=0)\n",
    "multi_acc_t2 = evaluate_multi_token(multi_model, test_sequences[:50], position=1)\n",
    "multi_acc_t3 = evaluate_multi_token(multi_model, test_sequences[:50], position=2)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Single-Token Model:\")\n",
    "print(f\"  Next token (t+1): {single_acc:.2%}\")\n",
    "print(f\"\\nMulti-Token Model:\")\n",
    "print(f\"  Next token (t+1): {multi_acc_t1:.2%}\")\n",
    "print(f\"  2 tokens ahead (t+2): {multi_acc_t2:.2%}\")\n",
    "print(f\"  3 tokens ahead (t+3): {multi_acc_t3:.2%}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 可视化多标记预测"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction accuracy heatmap\n",
    "test_seq = test_sequences[0][:15]\n",
    "accuracies = np.zeros((len(test_seq) - 3, 3))\n",
    "\n",
    "for i in range(len(test_seq) - 3):\n",
    "    input_tokens = test_seq[:i+1]\n",
    "    targets = test_seq[i+1:i+4]\n",
    "    \n",
    "    multi_preds, _ = multi_model.forward(input_tokens)\n",
    "    position_preds = multi_preds[-1]\n",
    "    \n",
    "    for j in range(3):\n",
    "        pred_token = np.argmax(position_preds[j])\n",
    "        accuracies[i, j] = 1.0 if pred_token == targets[j] else 0.0\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap\n",
    "im = ax1.imshow(accuracies.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax1.set_xlabel('Input Position', fontsize=12)\n",
    "ax1.set_ylabel('Future Position', fontsize=12)\n",
    "ax1.set_title('Multi-Token Prediction Accuracy', fontsize=13, fontweight='bold')\n",
    "ax1.set_yticks([0, 1, 2])\n",
    "ax1.set_yticklabels(['t+1', 't+2', 't+3'])\n",
    "plt.colorbar(im, ax=ax1, label='Accuracy (1=Correct, 0=Wrong)')\n",
    "\n",
    "# Average accuracy by distance\n",
    "avg_accs = np.mean(accuracies, axis=0)\n",
    "positions = ['t+1', 't+2', 't+3']\n",
    "bars = ax2.bar(positions, avg_accs, color=['green', 'orange', 'red'], edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Average Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy vs Prediction Distance', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, avg_accs):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.1%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFurther predictions are harder (as expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 样本效率比较"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on varying dataset sizes\n",
    "dataset_sizes = [10, 25, 50, 100, 200]\n",
    "single_final_losses = []\n",
    "multi_final_losses = []\n",
    "\n",
    "print(\"Testing sample efficiency...\\n\")\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    print(f\"Training on {size} sequences...\")\n",
    "    \n",
    "    # Single-token\n",
    "    single_temp = SingleTokenRNN(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "    single_loss = train_single_token(single_temp, train_sequences[:size], epochs=20, lr=0.01)\n",
    "    single_final_losses.append(single_loss[-1])\n",
    "    \n",
    "    # Multi-token\n",
    "    multi_temp = MultiTokenRNN(vocab_size, embedding_dim=32, hidden_dim=64, num_future_tokens=3)\n",
    "    multi_loss = train_multi_token(multi_temp, train_sequences[:size], epochs=20, lr=0.01)\n",
    "    multi_final_losses.append(multi_loss[-1])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dataset_sizes, single_final_losses, 'o-', linewidth=2, markersize=10, \n",
    "        label='Single-Token', color='blue')\n",
    "plt.plot(dataset_sizes, multi_final_losses, 's-', linewidth=2, markersize=10, \n",
    "        label='Multi-Token (3 ahead)', color='red')\n",
    "plt.xlabel('Number of Training Sequences', fontsize=12)\n",
    "plt.ylabel('Final Loss', fontsize=12)\n",
    "plt.title('Sample Efficiency: Single vs Multi-Token', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMulti-token prediction is more sample efficient (learns faster with less data)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 关键要点\n\n### 多标记预测:\n\n**标准 LM**:\n```\n给定: w1, w2, w3\n预测: w4\n损失: -log P(w4 | w1, w2, w3)\n```\n\n**多标记 LM**:\n```\n给定: w1, w2, w3\n预测: w4, w5, w6  (多个标记！)\n损失: -log P(w4|w1:3) - log P(w5|w1:3) - log P(w6|w1:3)\n```\n\n### 架构:\n\n**共享主干**:\n- 嵌入层\n- RNN/Transformer 层\n\n**多个输出头**:\n- 头 1: 预测 t+1\n- 头 2: 预测 t+2\n- 头 3: 预测 t+3\n- ...\n\n每个头是一个独立的线性层（开销很小！）\n\n### 优势:\n\n1. **样本效率** ✅\n   - 每个示例提供 N 个训练信号（不只是 1 个）\n   - 学习速度约快 N 倍\n\n2. **更好的表示** ✅\n   - 被迫编码更长范围的依赖关系\n   - 不能只记住下一个标记\n\n3. **更快的推理** ✅\n   - 可以在一次前向传播中生成多个标记\n   - 投机解码：并行验证预测\n\n4. **更好的泛化** ✅\n   - 更多训练信号 → 更好的特征\n   - 正则化效应\n\n### 训练:\n\n**损失函数**:\n$$\n\\mathcal{L} = \\sum_{i=1}^{N} \\lambda_i \\cdot \\mathcal{L}_{\\text{next-token}}(t+i)\n$$\n\n其中:\n- $N$ = 未来标记的数量\n- $\\lambda_i$ = 位置 $i$ 的权重（可以降低远期未来的权重）\n\n**典型设置**:\n- $N = 3$ 或 $N = 4$ 个标记\n- 相等权重: $\\lambda_i = 1/N$\n- 或衰减: $\\lambda_i = \\gamma^{i-1}$ 其中 $\\gamma < 1$\n\n### 论文结果（Meta AI）:\n\n**7B 模型**:\n- 标准: X 困惑度\n- 多标记（提前 4 个）: 0.7X 困惑度（更好！）\n\n**样本效率**:\n- 多标记 + 1/3 数据 = 标准 + 全部数据\n\n**推理速度**:\n- 3x 更快的生成（使用投机解码）\n\n### 推理策略:\n\n**1. 标准方式（仍然有效）**:\n```\n仅使用头 1（t+1 预测）\n与普通自回归生成相同\n```\n\n**2. 投机解码**:\n```\n从头生成 w4, w5, w6\n验证每个预测\n保留有效前缀，重新生成其余部分\n→ 最多 Nx 加速！\n```\n\n**3. 束搜索增强**:\n```\n同时考虑多个未来路径\n更好的长期规划\n```\n\n### 与其他技术的比较:\n\n| 技术 | 样本效率 | 推理速度 | 复杂度 |\n|-----------|------------------|-----------------|------------|\n| 标准 LM | 1x | 1x | 低 |\n| 数据增强 | 1.2x | 1x | 低 |\n| **多标记** | **2-3x** | **1-3x** | **低** |\n| 蒸馏 | 1.5x | 1.5x | 高 |\n\n### 实现技巧:\n\n1. **从简单开始**: N=2 或 N=3 个标记\n2. **共享主干**: 只有输出头是分开的\n3. **相等权重**: 除非你有理由偏好近/远未来\n4. **监控每个头**: 跟踪每个位置的准确率\n5. **用于加速**: 推理中使用投机解码\n\n### 何时使用:\n\n✅ **适用于**:\n- 训练数据有限\n- 想要更快的推理\n- 长序列（从长范围信号中受益）\n- 结构化输出（代码、公式）\n\n❌ **不适用于**:\n- 非常短的序列\n- 高度随机的输出\n- 内存受限（额外的头增加参数）\n\n### 现代扩展:\n\n1. **自适应 N**: 不同层使用不同的 N\n2. **层次化**: 预测下一个词、下一个短语、下一句\n3. **离散扩散**: 多步生成\n4. **连续时间**: 预测任意未来时间\n\n### 核心洞察:\n\n**更多预测 = 更多学习信号 = 更好的模型**\n\n多标记预测本质上是**免费的正则化**，带有**额外的加速**。几乎没有缺点！\n\n**\"为什么预测一个标记，当你可以预测多个？\"** - Meta AI 团队"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}