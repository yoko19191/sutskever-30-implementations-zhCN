{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 论文14：通过联合学习对齐和翻译进行神经机器翻译\n## Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio (2014)\n\n### 原始注意力机制\n\n这篇论文介绍了**注意力**——深度学习最重要的创新之一。比 Transformer 早了 3 年！"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 问题：固定长度上下文向量\n\n传统的 seq2seq 将整个输入压缩为单个向量 → 信息瓶颈！"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "class EncoderRNN:\n",
    "    \"\"\"Bidirectional RNN encoder\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Forward RNN\n",
    "        self.W_fwd = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b_fwd = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Backward RNN\n",
    "        self.W_bwd = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b_bwd = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: list of (input_size, 1) vectors\n",
    "        Returns: list of bidirectional hidden states (2*hidden_size, 1)\n",
    "        \"\"\"\n",
    "        seq_len = len(inputs)\n",
    "        \n",
    "        # Forward pass\n",
    "        h_fwd = []\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for x in inputs:\n",
    "            concat = np.vstack([x, h])\n",
    "            h = np.tanh(np.dot(self.W_fwd, concat) + self.b_fwd)\n",
    "            h_fwd.append(h)\n",
    "        \n",
    "        # Backward pass\n",
    "        h_bwd = []\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for x in reversed(inputs):\n",
    "            concat = np.vstack([x, h])\n",
    "            h = np.tanh(np.dot(self.W_bwd, concat) + self.b_bwd)\n",
    "            h_bwd.append(h)\n",
    "        h_bwd = list(reversed(h_bwd))\n",
    "        \n",
    "        # Concatenate forward and backward\n",
    "        annotations = [np.vstack([h_f, h_b]) for h_f, h_b in zip(h_fwd, h_bwd)]\n",
    "        \n",
    "        return annotations\n",
    "\n",
    "print(\"Bidirectional Encoder created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bahdanau 注意力机制\n\n关键创新：联合对齐和翻译！\n\n**注意力分数**：$e_{ij} = a(s_{i-1}, h_j)$，其中 $s$ 是解码器状态，$h$ 是编码器标注\n\n**注意力权重**：$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_k \\exp(e_{ik})}$\n\n**上下文向量**：$c_i = \\sum_j \\alpha_{ij} h_j$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention:\n",
    "    \"\"\"Additive attention mechanism\"\"\"\n",
    "    def __init__(self, hidden_size, annotation_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Attention parameters\n",
    "        self.W_a = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.U_a = np.random.randn(hidden_size, annotation_size) * 0.01\n",
    "        self.v_a = np.random.randn(1, hidden_size) * 0.01\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_annotations):\n",
    "        \"\"\"\n",
    "        decoder_hidden: (hidden_size, 1) - current decoder state s_{i-1}\n",
    "        encoder_annotations: list of (annotation_size, 1) - all encoder states h_j\n",
    "        \n",
    "        Returns:\n",
    "        context: (annotation_size, 1) - weighted sum of annotations\n",
    "        attention_weights: (seq_len,) - attention distribution\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Compute attention scores for each position\n",
    "        for h_j in encoder_annotations:\n",
    "            # e_ij = v_a^T * tanh(W_a * s_{i-1} + U_a * h_j)\n",
    "            score = np.dot(self.v_a, np.tanh(\n",
    "                np.dot(self.W_a, decoder_hidden) + \n",
    "                np.dot(self.U_a, h_j)\n",
    "            ))\n",
    "            scores.append(score[0, 0])\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        scores = np.array(scores)\n",
    "        attention_weights = softmax(scores)\n",
    "        \n",
    "        # Compute context vector as weighted sum\n",
    "        context = sum(alpha * h for alpha, h in zip(attention_weights, encoder_annotations))\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "print(\"Bahdanau Attention mechanism created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 带注意力的解码器"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    \"\"\"RNN decoder with Bahdanau attention\"\"\"\n",
    "    def __init__(self, output_size, hidden_size, annotation_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = BahdanauAttention(hidden_size, annotation_size)\n",
    "        \n",
    "        # RNN: takes previous output + context\n",
    "        input_size = output_size + annotation_size\n",
    "        self.W_dec = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b_dec = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Output layer\n",
    "        self.W_out = np.random.randn(output_size, hidden_size + annotation_size + output_size) * 0.01\n",
    "        self.b_out = np.zeros((output_size, 1))\n",
    "    \n",
    "    def step(self, prev_output, decoder_hidden, encoder_annotations):\n",
    "        \"\"\"\n",
    "        Single decoding step\n",
    "        \n",
    "        prev_output: (output_size, 1) - previous output word\n",
    "        decoder_hidden: (hidden_size, 1) - previous decoder state\n",
    "        encoder_annotations: list of (annotation_size, 1) - encoder states\n",
    "        \n",
    "        Returns:\n",
    "        output: (output_size, 1) - predicted output distribution\n",
    "        new_hidden: (hidden_size, 1) - new decoder state\n",
    "        attention_weights: attention distribution\n",
    "        \"\"\"\n",
    "        # Compute attention and context\n",
    "        context, attention_weights = self.attention.forward(decoder_hidden, encoder_annotations)\n",
    "        \n",
    "        # Decoder RNN: s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "        rnn_input = np.vstack([prev_output, context])\n",
    "        concat = np.vstack([rnn_input, decoder_hidden])\n",
    "        new_hidden = np.tanh(np.dot(self.W_dec, concat) + self.b_dec)\n",
    "        \n",
    "        # Output: y_i = g(s_i, y_{i-1}, c_i)\n",
    "        output_input = np.vstack([new_hidden, context, prev_output])\n",
    "        output = np.dot(self.W_out, output_input) + self.b_out\n",
    "        \n",
    "        return output, new_hidden, attention_weights\n",
    "    \n",
    "    def forward(self, encoder_annotations, max_length=20, start_token=None):\n",
    "        \"\"\"\n",
    "        Full decoding\n",
    "        \"\"\"\n",
    "        if start_token is None:\n",
    "            start_token = np.zeros((self.output_size, 1))\n",
    "        \n",
    "        outputs = []\n",
    "        attention_history = []\n",
    "        \n",
    "        # Initialize\n",
    "        decoder_hidden = np.zeros((self.hidden_size, 1))\n",
    "        prev_output = start_token\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output, decoder_hidden, attention_weights = self.step(\n",
    "                prev_output, decoder_hidden, encoder_annotations\n",
    "            )\n",
    "            \n",
    "            outputs.append(output)\n",
    "            attention_history.append(attention_weights)\n",
    "            \n",
    "            # Next input is current output (greedy decoding)\n",
    "            prev_output = output\n",
    "        \n",
    "        return outputs, attention_history\n",
    "\n",
    "print(\"Attention Decoder created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 完整的 Seq2Seq + 注意力"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention:\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size=32):\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.input_embedding = np.random.randn(input_vocab_size, hidden_size) * 0.01\n",
    "        self.output_embedding = np.random.randn(output_vocab_size, hidden_size) * 0.01\n",
    "        \n",
    "        # Encoder (bidirectional, so annotation size is 2*hidden_size)\n",
    "        self.encoder = EncoderRNN(hidden_size, hidden_size)\n",
    "        \n",
    "        # Decoder with attention\n",
    "        annotation_size = 2 * hidden_size\n",
    "        self.decoder = AttentionDecoder(hidden_size, hidden_size, annotation_size)\n",
    "    \n",
    "    def translate(self, input_sequence, max_output_length=15):\n",
    "        \"\"\"\n",
    "        Translate input sequence to output sequence\n",
    "        \n",
    "        input_sequence: list of token indices\n",
    "        \"\"\"\n",
    "        # Embed input\n",
    "        embedded = [self.input_embedding[idx:idx+1].T for idx in input_sequence]\n",
    "        \n",
    "        # Encode\n",
    "        annotations = self.encoder.forward(embedded)\n",
    "        \n",
    "        # Decode\n",
    "        start_token = self.output_embedding[0:1].T  # Use first token as start\n",
    "        outputs, attention_history = self.decoder.forward(\n",
    "            annotations, max_length=max_output_length, start_token=start_token\n",
    "        )\n",
    "        \n",
    "        return outputs, attention_history, annotations\n",
    "\n",
    "# Create model\n",
    "input_vocab_size = 20   # Source language vocab\n",
    "output_vocab_size = 20  # Target language vocab\n",
    "model = Seq2SeqWithAttention(input_vocab_size, output_vocab_size, hidden_size=16)\n",
    "\n",
    "print(f\"Seq2Seq with Attention created\")\n",
    "print(f\"Input vocab: {input_vocab_size}\")\n",
    "print(f\"Output vocab: {output_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 在合成翻译任务上测试"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple synthetic task: reverse sequence\n",
    "# Input: [1, 2, 3, 4, 5]\n",
    "# Output: [5, 4, 3, 2, 1]\n",
    "\n",
    "input_seq = [1, 2, 3, 4, 5, 6, 7]\n",
    "outputs, attention_history, annotations = model.translate(input_seq, max_output_length=len(input_seq))\n",
    "\n",
    "print(f\"Input sequence: {input_seq}\")\n",
    "print(f\"Number of output steps: {len(outputs)}\")\n",
    "print(f\"Number of attention distributions: {len(attention_history)}\")\n",
    "print(f\"Encoder annotations shape: {len(annotations)} x {annotations[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 可视化注意力权重\n\n关键洞察：查看模型关注什么！"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert attention history to matrix\n",
    "attention_matrix = np.array(attention_history)  # (output_len, input_len)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_matrix, cmap='Blues', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Input Position (Source)')\n",
    "plt.ylabel('Output Position (Target)')\n",
    "plt.title('Bahdanau Attention Alignment Matrix')\n",
    "\n",
    "# Add grid\n",
    "plt.xticks(range(len(input_seq)), [f'x{i+1}' for i in input_seq])\n",
    "plt.yticks(range(len(outputs)), [f'y{i+1}' for i in range(len(outputs))])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAttention patterns show which input positions influence each output.\")\n",
    "print(\"Brighter cells = higher attention weight.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 每个解码步骤的注意力"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention distribution at specific decoder steps\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "steps_to_show = min(8, len(attention_history))\n",
    "\n",
    "for i in range(steps_to_show):\n",
    "    axes[i].bar(range(len(input_seq)), attention_history[i])\n",
    "    axes[i].set_title(f'Output Step {i+1}')\n",
    "    axes[i].set_xlabel('Input Position')\n",
    "    axes[i].set_ylabel('Attention Weight')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].set_xticks(range(len(input_seq)))\n",
    "    axes[i].set_xticklabels([f'x{j+1}' for j in input_seq], fontsize=8)\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Attention Distribution at Each Decoding Step', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each decoder step focuses on different input positions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 对比：有注意力 vs 无注意力"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fixed-context seq2seq (no attention)\n",
    "def fixed_context_attention(seq_len):\n",
    "    \"\"\"Simulates attending only to last encoder state\"\"\"\n",
    "    weights = np.zeros(seq_len)\n",
    "    weights[-1] = 1.0  # Only attend to last position\n",
    "    return weights\n",
    "\n",
    "# Create comparison\n",
    "input_length = len(input_seq)\n",
    "output_length = len(outputs)\n",
    "\n",
    "# Fixed context\n",
    "fixed_attention = np.array([fixed_context_attention(input_length) for _ in range(output_length)])\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without attention (fixed context)\n",
    "im1 = ax1.imshow(fixed_attention, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "ax1.set_xlabel('Input Position')\n",
    "ax1.set_ylabel('Output Position')\n",
    "ax1.set_title('Without Attention (Fixed Context)\\nAll decoder steps see only last encoder state')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# With Bahdanau attention\n",
    "im2 = ax2.imshow(attention_matrix, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_xlabel('Input Position')\n",
    "ax2.set_ylabel('Output Position')\n",
    "ax2.set_title('With Bahdanau Attention\\nEach decoder step attends to different positions')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Difference:\")\n",
    "print(\"  Without attention: Information bottleneck at last encoder state\")\n",
    "print(\"  With attention: Dynamic access to all encoder states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 注意力机制变体"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bahdanau_score(s, h, W_a, U_a, v_a):\n",
    "    \"\"\"Additive/Concat attention (Bahdanau)\"\"\"\n",
    "    return np.dot(v_a.T, np.tanh(np.dot(W_a, s) + np.dot(U_a, h)))[0, 0]\n",
    "\n",
    "def dot_product_score(s, h):\n",
    "    \"\"\"Dot product attention (Luong)\"\"\"\n",
    "    return np.dot(s.T, h)[0, 0]\n",
    "\n",
    "def scaled_dot_product_score(s, h):\n",
    "    \"\"\"Scaled dot product (Transformer-style)\"\"\"\n",
    "    d_k = s.shape[0]\n",
    "    return np.dot(s.T, h)[0, 0] / np.sqrt(d_k)\n",
    "\n",
    "# Compare scoring functions\n",
    "s = np.random.randn(16, 1)\n",
    "h = np.random.randn(32, 1)\n",
    "W_a = np.random.randn(16, 16)\n",
    "U_a = np.random.randn(16, 32)\n",
    "v_a = np.random.randn(1, 16)\n",
    "\n",
    "print(\"Attention Score Functions:\")\n",
    "print(f\"  Bahdanau (additive): score = v^T tanh(W*s + U*h)\")\n",
    "print(f\"  Dot product: score = s^T h\")\n",
    "print(f\"  Scaled dot product: score = s^T h / sqrt(d_k)\")\n",
    "print(f\"\\nBahdanau is more expressive but has more parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 关键要点\n\n### 注意力解决的问题：\n- **固定长度上下文**：整个输入压缩为单个向量\n- **信息瓶颈**：长序列丢失信息\n- **无对齐**：解码器不知道应该关注哪个输入\n\n### Bahdanau 注意力创新：\n1. **动态上下文**：每个解码步骤不同\n2. **软对齐**：学习对齐源和目标\n3. **所有编码器状态**：解码器可访问所有状态，而不仅仅是最后一个\n\n### 工作原理：\n```\n1. 编码器产生标注 h_1, ..., h_T\n2. 对于每个解码步骤 i：\n   a. 计算注意力分数：e_ij = score(s_{i-1}, h_j)\n   b. 归一化为权重：α_ij = softmax(e_ij)\n   c. 计算上下文：c_i = Σ α_ij * h_j\n   d. 生成输出：y_i = f(s_i, c_i, y_{i-1})\n```\n\n### Bahdanau vs Luong 注意力：\n| 特性 | Bahdanau (2014) | Luong (2015) |\n|---------|----------------|---------------|\n| 分数 | 加性：v·tanh(W·s + U·h) | 乘性：s·h |\n| 时机 | 使用 s_{i-1}（前一个） | 使用 s_i（当前） |\n| 全局/局部 | 仅全局 | 两者皆有 |\n\n### 数学公式：\n\n**注意力分数（对齐模型）**：\n$$e_{ij} = v_a^T \\tanh(W_a s_{i-1} + U_a h_j)$$\n\n**注意力权重**：\n$$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}$$\n\n**上下文向量**：\n$$c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j$$\n\n**解码器**：\n$$s_i = f(s_{i-1}, y_{i-1}, c_i)$$\n$$p(y_i | y_{<i}, x) = g(s_i, y_{i-1}, c_i)$$\n\n### 影响：\n- **革命化了 NMT**：BLEU 分数显著提升\n- **可解释性**：可以可视化对齐\n- **Transformer 的基础**：纯注意力（2017）\n- **超越 NMT**：用于视觉、语音等\n\n### 为什么有效：\n1. **解决瓶颈**：可变长度上下文\n2. **学习对齐**：无需单独的对齐模型\n3. **可微分**：端到端训练\n4. **适用于长序列**：注意力不衰减\n\n### 现代视角：\n- Transformer 使用**自注意力**（关注同一序列）\n- 缩放点积现在是标准（更简单、更快）\n- 多头注意力捕获不同关系\n- 但 Bahdanau 的核心思想仍然存在：**关注相关内容**\n\n### 注意力机制的统一视角：\n\n| 类型 | 查询 | 键 | 值 | 用途 |\n|------|------|-----|-----|------|\n| Bahdanau | Decoder state | Encoder states | Additive | 翻译 |\n| Luong | Decoder state | Encoder states | Multiplicative | 翻译 |\n| Self-Attn | All positions | All positions | Scaled dot | Transformer |\n| Cross-Attn | Query | Keys, Values | Scaled dot | RAG, T5 |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}