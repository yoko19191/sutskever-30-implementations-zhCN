{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文3：理解LSTM网络\n",
    "## Christopher Olah\n",
    "\n",
    "### 带有门控可视化的LSTM实现\n",
    "\n",
    "LSTM（长短期记忆）网络通过门控记忆细胞解决梯度消失问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM单元实现\n",
    "\n",
    "LSTM有三个门：\n",
    "1. **遗忘门**：从细胞状态中遗忘什么\n",
    "2. **输入门**：添加什么新信息\n",
    "3. **输出门**：基于细胞状态输出什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 为效率而连接的权重: [input; hidden] -> gates\n",
    "        concat_size = input_size + hidden_size\n",
    "        \n",
    "        # 遗忘门\n",
    "        self.Wf = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # 输入门\n",
    "        self.Wi = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # 候选细胞状态\n",
    "        self.Wc = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # 输出门\n",
    "        self.Wo = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        LSTM单元的前向传播\n",
    "        \n",
    "        x: 输入 (input_size, 1)\n",
    "        h_prev: 前一个隐藏状态 (hidden_size, 1)\n",
    "        c_prev: 前一个细胞状态 (hidden_size, 1)\n",
    "        \n",
    "        返回:\n",
    "        h_next: 下一个隐藏状态\n",
    "        c_next: 下一个细胞状态\n",
    "        cache: 反向传播所需的值\n",
    "        \"\"\"\n",
    "        # 连接输入和前一个隐藏状态\n",
    "        concat = np.vstack([x, h_prev])\n",
    "        \n",
    "        # 遗忘门: 决定从细胞状态中遗忘什么\n",
    "        f = sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
    "        \n",
    "        # 输入门: 决定存储什么新信息\n",
    "        i = sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
    "        \n",
    "        # 候选细胞状态: 可能添加的新信息\n",
    "        c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
    "        \n",
    "        # 更新细胞状态: 遗忘 + 输入新信息\n",
    "        c_next = f * c_prev + i * c_tilde\n",
    "        \n",
    "        # 输出门: 决定输出什么\n",
    "        o = sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
    "        \n",
    "        # 隐藏状态: 过滤的细胞状态\n",
    "        h_next = o * np.tanh(c_next)\n",
    "        \n",
    "        # 为反向传播缓存\n",
    "        cache = (x, h_prev, c_prev, concat, f, i, c_tilde, c_next, o, h_next)\n",
    "        \n",
    "        return h_next, c_next, cache\n",
    "\n",
    "# 测试LSTM单元\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "lstm_cell = LSTMCell(input_size, hidden_size)\n",
    "\n",
    "x = np.random.randn(input_size, 1)\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "h_next, c_next, cache = lstm_cell.forward(x, h, c)\n",
    "print(f\"LSTM单元已初始化: input_size={input_size}, hidden_size={hidden_size}\")\n",
    "print(f\"隐藏状态形状: {h_next.shape}\")\n",
    "print(f\"细胞状态形状: {c_next.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于序列处理的全LSTM网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "        # 输出层\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        通过LSTM处理序列\n",
    "        inputs: 输入向量列表\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # 存储状态以供可视化\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "        gate_values = {'f': [], 'i': [], 'o': []}\n",
    "        \n",
    "        for x in inputs:\n",
    "            h, c, cache = self.cell.forward(x, h, c)\n",
    "            h_states.append(h.copy())\n",
    "            c_states.append(c.copy())\n",
    "            \n",
    "            # 从缓存中提取门值\n",
    "            _, _, _, _, f, i, _, _, o, _ = cache\n",
    "            gate_values['f'].append(f.copy())\n",
    "            gate_values['i'].append(i.copy())\n",
    "            gate_values['o'].append(o.copy())\n",
    "        \n",
    "        # 最终输出\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "        \n",
    "        return y, h_states, c_states, gate_values\n",
    "\n",
    "# 创建LSTM模型\n",
    "input_size = 5\n",
    "hidden_size = 16\n",
    "output_size = 5\n",
    "lstm = LSTM(input_size, hidden_size, output_size)\n",
    "print(f\"\\nLSTM模型已创建: {input_size} -> {hidden_size} -> {output_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在合成序列任务上测试：长期依赖\n",
    "\n",
    "任务：记住序列开头的值并在结尾输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_long_term_dependency_data(seq_length=20, num_samples=100):\n",
    "    \"\"\"\n",
    "    生成序列，其中第一个元素必须被记住直到结尾\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # 创建序列\n",
    "        sequence = []\n",
    "        \n",
    "        # 第一个元素是重要的（独热编码）\n",
    "        first_elem = np.random.randint(0, input_size)\n",
    "        first_vec = np.zeros((input_size, 1))\n",
    "        first_vec[first_elem] = 1\n",
    "        sequence.append(first_vec)\n",
    "        \n",
    "        # 其余是随机噪声\n",
    "        for _ in range(seq_length - 1):\n",
    "            noise = np.random.randn(input_size, 1) * 0.1\n",
    "            sequence.append(noise)\n",
    "        \n",
    "        X.append(sequence)\n",
    "        \n",
    "        # 目标: 记住第一个元素\n",
    "        target = np.zeros((output_size, 1))\n",
    "        target[first_elem] = 1\n",
    "        y.append(target)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 生成测试数据\n",
    "X_test, y_test = generate_long_term_dependency_data(seq_length=15, num_samples=10)\n",
    "\n",
    "# 测试前向传播\n",
    "output, h_states, c_states, gate_values = lstm.forward(X_test[0])\n",
    "\n",
    "print(f\"\\n测试序列长度: {len(X_test[0])}\")\n",
    "print(f\"第一个元素（要记住）: {np.argmax(X_test[0][0])}\")\n",
    "print(f\"期望输出: {np.argmax(y_test[0])}\")\n",
    "print(f\"模型输出（未训练）: {output.flatten()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化LSTM门\n",
    "\n",
    "理解LSTM的关键是看门如何随时间操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理序列并可视化门\n",
    "test_seq = X_test[0]\n",
    "output, h_states, c_states, gate_values = lstm.forward(test_seq)\n",
    "\n",
    "# 转换为数组以供绘图\n",
    "forget_gates = np.hstack(gate_values['f'])\n",
    "input_gates = np.hstack(gate_values['i'])\n",
    "output_gates = np.hstack(gate_values['o'])\n",
    "cell_states = np.hstack(c_states)\n",
    "hidden_states = np.hstack(h_states)\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "\n",
    "# 遗忘门\n",
    "axes[0].imshow(forget_gates, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[0].set_title('遗忘门 (1=保留, 0=遗忘)')\n",
    "axes[0].set_ylabel('隐藏单元')\n",
    "axes[0].set_xlabel('时间步')\n",
    "\n",
    "# 输入门\n",
    "axes[1].imshow(input_gates, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1].set_title('输入门 (1=接受新信息, 0=忽略新信息)')\n",
    "axes[1].set_ylabel('隐藏单元')\n",
    "axes[1].set_xlabel('时间步')\n",
    "\n",
    "# 输出门\n",
    "axes[2].imshow(output_gates, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[2].set_title('输出门 (1=暴露, 0=隐藏)')\n",
    "axes[2].set_ylabel('隐藏单元')\n",
    "axes[2].set_xlabel('时间步')\n",
    "\n",
    "# 细胞状态\n",
    "im3 = axes[3].imshow(cell_states, cmap='RdBu', aspect='auto')\n",
    "axes[3].set_title('细胞状态（长期记忆）')\n",
    "axes[3].set_ylabel('隐藏单元')\n",
    "axes[3].set_xlabel('时间步')\n",
    "plt.colorbar(im3, ax=axes[3])\n",
    "\n",
    "# 隐藏状态\n",
    "im4 = axes[4].imshow(hidden_states, cmap='RdBu', aspect='auto')\n",
    "axes[4].set_title('隐藏状态（输出到下一层）')\n",
    "axes[4].set_ylabel('隐藏单元')\n",
    "axes[4].set_xlabel('时间步')\n",
    "plt.colorbar(im4, ax=axes[4])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n门解释:\")\n",
    "print(\"- 遗忘门控制从细胞状态中丢弃什么信息\")\n",
    "print(\"- 输入门控制向细胞状态添加什么新信息\")\n",
    "print(\"- 输出门控制从细胞状态输出什么\")\n",
    "print(\"- 细胞状态是长期记忆高速公路\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较LSTM与原始RNN在长序列上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        concat_size = input_size + hidden_size\n",
    "        self.Wh = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        concat = np.vstack([x, h_prev])\n",
    "        h_next = np.tanh(np.dot(self.Wh, concat) + self.bh)\n",
    "        return h_next\n",
    "\n",
    "# 创建用于比较的原始RNN\n",
    "rnn_cell = VanillaRNNCell(input_size, hidden_size)\n",
    "\n",
    "def process_with_vanilla_rnn(inputs):\n",
    "    h = np.zeros((hidden_size, 1))\n",
    "    h_states = []\n",
    "    \n",
    "    for x in inputs:\n",
    "        h = rnn_cell.forward(x, h)\n",
    "        h_states.append(h.copy())\n",
    "    \n",
    "    return h_states\n",
    "\n",
    "# 用两者处理相同的序列\n",
    "rnn_h_states = process_with_vanilla_rnn(test_seq)\n",
    "rnn_hidden = np.hstack(rnn_h_states)\n",
    "\n",
    "# 比较隐藏状态演化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "im1 = ax1.imshow(rnn_hidden, cmap='RdBu', aspect='auto')\n",
    "ax1.set_title('原始RNN隐藏状态')\n",
    "ax1.set_ylabel('隐藏单元')\n",
    "ax1.set_xlabel('时间步')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "im2 = ax2.imshow(hidden_states, cmap='RdBu', aspect='auto')\n",
    "ax2.set_title('LSTM隐藏状态')\n",
    "ax2.set_ylabel('隐藏单元')\n",
    "ax2.set_xlabel('时间步')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n关键差异:\")\n",
    "print(\"- LSTM将细胞状态与隐藏状态分开维护\")\n",
    "print(\"- 门允许选择性信息流\")\n",
    "print(\"- 更好的梯度通过时间的流动（解决梯度消失）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度流比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟梯度幅度\n",
    "def simulate_gradient_flow(seq_length=30):\n",
    "    \"\"\"\n",
    "    模拟梯度在原始RNN与LSTM中如何衰减\n",
    "    \"\"\"\n",
    "    # 原始RNN: 梯度指数衰减\n",
    "    rnn_grads = []\n",
    "    grad = 1.0\n",
    "    decay_factor = 0.85  # 原始RNN中的典型衰减\n",
    "    \n",
    "    for t in range(seq_length):\n",
    "        rnn_grads.append(grad)\n",
    "        grad *= decay_factor\n",
    "    \n",
    "    # LSTM: 通过细胞状态高速公路保持梯度\n",
    "    lstm_grads = []\n",
    "    grad = 1.0\n",
    "    forget_gate_avg = 0.95  # 高遗忘门 = 保持梯度\n",
    "    \n",
    "    for t in range(seq_length):\n",
    "        lstm_grads.append(grad)\n",
    "        grad *= forget_gate_avg  # 遗忘门控制梯度流\n",
    "    \n",
    "    return np.array(rnn_grads), np.array(lstm_grads)\n",
    "\n",
    "rnn_grads, lstm_grads = simulate_gradient_flow()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(rnn_grads[::-1], label='原始RNN', linewidth=2)\n",
    "plt.plot(lstm_grads[::-1], label='LSTM', linewidth=2)\n",
    "plt.xlabel('过去的时间步')\n",
    "plt.ylabel('梯度幅度')\n",
    "plt.title('梯度流: LSTM vs 原始RNN')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n30步后的梯度:\")\n",
    "print(f\"原始RNN: {rnn_grads[-1]:.6f} (消失)\")\n",
    "print(f\"LSTM: {lstm_grads[-1]:.6f} (保持)\")\n",
    "print(f\"\\n这就是为什么LSTM可以学习长期依赖！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键要点\n",
    "\n",
    "### LSTM架构：\n",
    "1. **细胞状态**：跨时间信息流的高速公路\n",
    "2. **遗忘门**：控制从记忆中删除什么\n",
    "3. **输入门**：控制添加什么新信息\n",
    "4. **输出门**：控制从记忆中输出什么\n",
    "\n",
    "### 为什么LSTM有效：\n",
    "- **恒定误差轮播**：细胞状态提供不间断的梯度流\n",
    "- **乘性门**：允许网络学习何时记忆/遗忘\n",
    "- **加性更新**：细胞状态通过加法更新（f*c + i*c_tilde）\n",
    "- **梯度保持**：接近1的遗忘门保持梯度\n",
    "\n",
    "### 相比原始RNN的优势：\n",
    "- 解决梯度消失问题\n",
    "- 学习长期依赖（100+时间步）\n",
    "- 更稳定的训练\n",
    "- 在真实序列任务上表现更好"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
