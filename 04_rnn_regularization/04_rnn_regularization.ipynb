{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文4：循环神经网络正则化\n",
    "## Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals (2014)\n",
    "\n",
    "### RNN的Dropout\n",
    "\n",
    "关键洞察：仅在**非循环连接**上应用dropout，而非循环连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, dropout_rate=0.5, training=True):\n",
    "    \"\"\"\n",
    "    标准dropout\n",
    "    训练期间：以概率dropout_rate随机将元素置零\n",
    "    测试期间：按 (1 - dropout_rate) 缩放\n",
    "    \"\"\"\n",
    "    if not training or dropout_rate == 0:\n",
    "        return x\n",
    "    \n",
    "    # 反向dropout（训练期间缩放）\n",
    "    mask = (np.random.rand(*x.shape) > dropout_rate).astype(float)\n",
    "    return x * mask / (1 - dropout_rate)\n",
    "\n",
    "# 测试dropout\n",
    "x = np.ones((5, 1))\n",
    "print(\"原始:\", x.T)\n",
    "print(\"使用dropout (p=0.5):\", dropout(x, 0.5).T)\n",
    "print(\"使用dropout (p=0.5):\", dropout(x, 0.5).T)\n",
    "print(\"测试模式:\", dropout(x, 0.5, training=False).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带有适当Dropout的RNN\n",
    "\n",
    "**关键**：在**输入**和**输出**上使用Dropout，而非循环连接！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithDropout:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 权重\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs, dropout_rate=0.0, training=True):\n",
    "        \"\"\"\n",
    "        带dropout的前向传播\n",
    "        \n",
    "        Dropout应用于：\n",
    "        1. 输入连接 (x -> h)\n",
    "        2. 输出连接 (h -> y)\n",
    "        \n",
    "        不应用于：\n",
    "        - 循环连接 (h -> h)\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        for x in inputs:\n",
    "            # 对INPUT应用dropout\n",
    "            x_dropped = dropout(x, dropout_rate, training)\n",
    "            \n",
    "            # RNN更新（循环连接上无dropout）\n",
    "            h = np.tanh(\n",
    "                np.dot(self.W_xh, x_dropped) +  # 在这里应用dropout\n",
    "                np.dot(self.W_hh, h) +           # 这里不应用dropout\n",
    "                self.bh\n",
    "            )\n",
    "            \n",
    "            # 在输出前对HIDDEN状态应用dropout\n",
    "            h_dropped = dropout(h, dropout_rate, training)\n",
    "            \n",
    "            # 输出\n",
    "            y = np.dot(self.W_hy, h_dropped) + self.by  # 在这里应用dropout\n",
    "            \n",
    "            outputs.append(y)\n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "\n",
    "# 测试\n",
    "rnn = RNNWithDropout(input_size=10, hidden_size=20, output_size=10)\n",
    "test_inputs = [np.random.randn(10, 1) for _ in range(5)]\n",
    "\n",
    "outputs_train, _ = rnn.forward(test_inputs, dropout_rate=0.5, training=True)\n",
    "outputs_test, _ = rnn.forward(test_inputs, dropout_rate=0.5, training=False)\n",
    "\n",
    "print(f\"训练输出[0]均值: {outputs_train[0].mean():.4f}\")\n",
    "print(f\"测试输出[0]均值: {outputs_test[0].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变分Dropout\n",
    "\n",
    "**关键创新**：在所有时间步使用**相同**的dropout掩码！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithVariationalDropout:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 权重（与之前相同）\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs, dropout_rate=0.0, training=True):\n",
    "        \"\"\"\n",
    "        变分dropout：所有时间步使用相同掩码\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        # 为整个序列生成一次掩码\n",
    "        if training and dropout_rate > 0:\n",
    "            input_mask = (np.random.rand(self.input_size, 1) > dropout_rate).astype(float) / (1 - dropout_rate)\n",
    "            hidden_mask = (np.random.rand(self.hidden_size, 1) > dropout_rate).astype(float) / (1 - dropout_rate)\n",
    "        else:\n",
    "            input_mask = np.ones((self.input_size, 1))\n",
    "            hidden_mask = np.ones((self.hidden_size, 1))\n",
    "        \n",
    "        for x in inputs:\n",
    "            # 对每个输入应用相同掩码\n",
    "            x_dropped = x * input_mask\n",
    "            \n",
    "            # RNN更新\n",
    "            h = np.tanh(\n",
    "                np.dot(self.W_xh, x_dropped) +\n",
    "                np.dot(self.W_hh, h) +\n",
    "                self.bh\n",
    "            )\n",
    "            \n",
    "            # 对每个隐藏状态应用相同掩码\n",
    "            h_dropped = h * hidden_mask\n",
    "            \n",
    "            # 输出\n",
    "            y = np.dot(self.W_hy, h_dropped) + self.by\n",
    "            \n",
    "            outputs.append(y)\n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "\n",
    "# 测试变分dropout\n",
    "var_rnn = RNNWithVariationalDropout(input_size=10, hidden_size=20, output_size=10)\n",
    "outputs_var, _ = var_rnn.forward(test_inputs, dropout_rate=0.5, training=True)\n",
    "\n",
    "print(\"变分dropout在所有时间步使用一致的掩码\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较Dropout策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sequence data\n",
    "seq_length = 20\n",
    "test_sequence = [np.random.randn(10, 1) for _ in range(seq_length)]\n",
    "\n",
    "# Run with different strategies\n",
    "_, h_no_dropout = rnn.forward(test_sequence, dropout_rate=0.0, training=False)\n",
    "_, h_standard = rnn.forward(test_sequence, dropout_rate=0.5, training=True)\n",
    "_, h_variational = var_rnn.forward(test_sequence, dropout_rate=0.5, training=True)\n",
    "\n",
    "# Convert to arrays\n",
    "h_no_dropout = np.hstack([h.flatten() for h in h_no_dropout]).T\n",
    "h_standard = np.hstack([h.flatten() for h in h_standard]).T\n",
    "h_variational = np.hstack([h.flatten() for h in h_variational]).T\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].imshow(h_no_dropout, cmap='RdBu', aspect='auto')\n",
    "axes[0].set_title('No Dropout')\n",
    "axes[0].set_xlabel('Hidden Unit')\n",
    "axes[0].set_ylabel('Time Step')\n",
    "\n",
    "axes[1].imshow(h_standard, cmap='RdBu', aspect='auto')\n",
    "axes[1].set_title('Standard Dropout (different masks per timestep)')\n",
    "axes[1].set_xlabel('Hidden Unit')\n",
    "axes[1].set_ylabel('Time Step')\n",
    "\n",
    "axes[2].imshow(h_variational, cmap='RdBu', aspect='auto')\n",
    "axes[2].set_title('Variational Dropout (same mask all timesteps)')\n",
    "axes[2].set_xlabel('Hidden Unit')\n",
    "axes[2].set_ylabel('Time Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Variational dropout shows consistent patterns (same units dropped throughout)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout的位置很重要！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize where dropout is applied\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Create a simple RNN diagram\n",
    "def draw_rnn_cell(ax, title, show_input_dropout, show_hidden_dropout, show_recurrent_dropout):\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw boxes\n",
    "    # Input\n",
    "    ax.add_patch(plt.Rectangle((1, 2), 1.5, 1, fill=True, color='lightblue', ec='black'))\n",
    "    ax.text(1.75, 2.5, 'x_t', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Hidden (current)\n",
    "    ax.add_patch(plt.Rectangle((4, 4.5), 2, 2, fill=True, color='lightgreen', ec='black'))\n",
    "    ax.text(5, 5.5, 'h_t', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    # Hidden (previous)\n",
    "    ax.add_patch(plt.Rectangle((7, 4.5), 2, 2, fill=True, color='lightyellow', ec='black'))\n",
    "    ax.text(8, 5.5, 'h_{t-1}', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Output\n",
    "    ax.add_patch(plt.Rectangle((4, 7.5), 2, 1, fill=True, color='lightcoral', ec='black'))\n",
    "    ax.text(5, 8, 'y_t', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Arrows\n",
    "    # Input to hidden\n",
    "    color_input = 'red' if show_input_dropout else 'black'\n",
    "    width_input = 3 if show_input_dropout else 1\n",
    "    ax.arrow(2.5, 2.5, 1.3, 2, head_width=0.3, color=color_input, lw=width_input)\n",
    "    if show_input_dropout:\n",
    "        ax.text(3.2, 3.5, 'DROPOUT', fontsize=8, color='red', fontweight='bold')\n",
    "    \n",
    "    # Recurrent\n",
    "    color_rec = 'red' if show_recurrent_dropout else 'black'\n",
    "    width_rec = 3 if show_recurrent_dropout else 1\n",
    "    ax.arrow(7, 5.5, -0.8, 0, head_width=0.3, color=color_rec, lw=width_rec)\n",
    "    if show_recurrent_dropout:\n",
    "        ax.text(6.5, 6.2, 'DROPOUT', fontsize=8, color='red', fontweight='bold')\n",
    "    \n",
    "    # Hidden to output\n",
    "    color_hidden = 'red' if show_hidden_dropout else 'black'\n",
    "    width_hidden = 3 if show_hidden_dropout else 1\n",
    "    ax.arrow(5, 6.6, 0, 0.7, head_width=0.3, color=color_hidden, lw=width_hidden)\n",
    "    if show_hidden_dropout:\n",
    "        ax.text(5.5, 7, 'DROPOUT', fontsize=8, color='red', fontweight='bold')\n",
    "\n",
    "# Wrong: dropout everywhere\n",
    "draw_rnn_cell(axes[0, 0], 'WRONG: Dropout Everywhere\\n(Disrupts temporal flow)', \n",
    "             show_input_dropout=True, show_hidden_dropout=True, show_recurrent_dropout=True)\n",
    "\n",
    "# Wrong: only recurrent\n",
    "draw_rnn_cell(axes[0, 1], 'WRONG: Only Recurrent\\n(Loses gradient flow)', \n",
    "             show_input_dropout=False, show_hidden_dropout=False, show_recurrent_dropout=True)\n",
    "\n",
    "# Correct: Zaremba et al.\n",
    "draw_rnn_cell(axes[1, 0], 'CORRECT: Zaremba et al.\\n(Input & Output only)', \n",
    "             show_input_dropout=True, show_hidden_dropout=True, show_recurrent_dropout=False)\n",
    "\n",
    "# No dropout\n",
    "draw_rnn_cell(axes[1, 1], 'Baseline: No Dropout\\n(May overfit)', \n",
    "             show_input_dropout=False, show_hidden_dropout=False, show_recurrent_dropout=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键要点\n",
    "\n",
    "### 问题：\n",
    "- RNN上的朴素dropout效果不佳\n",
    "- 丢弃循环连接会破坏时间信息流\n",
    "- 标准dropout在每个时间步改变掩码（噪声大）\n",
    "\n",
    "### Zaremba等人的解决方案：\n",
    "\n",
    "**对以下项应用dropout：**\n",
    "- ✅ 输入到隐藏连接 (W_xh)\n",
    "- ✅ 隐藏到输出连接 (W_hy)\n",
    "\n",
    "**不要对以下项应用：**\n",
    "- ❌ 循环连接 (W_hh)\n",
    "\n",
    "### 变分Dropout：\n",
    "- 在所有时间步使用**相同的dropout掩码**\n",
    "- 比改变掩码更稳定\n",
    "- 更好的理论证明（贝叶斯）\n",
    "\n",
    "### 结果：\n",
    "- 语言建模显著改进\n",
    "- Penn Treebank: 测试困惑度从78.4改进到68.7\n",
    "- 也适用于LSTM和GRU\n",
    "\n",
    "### 实现技巧：\n",
    "1. 使用比前馈网络更高的dropout率（0.5-0.7）\n",
    "2. 在双向RNN的**两个**方向上应用dropout\n",
    "3. 可以堆叠多个LSTM层，在它们之间使用dropout\n",
    "4. 变分dropout：每个序列生成一次掩码\n",
    "\n",
    "### 为什么有效：\n",
    "- 保持时间依赖（循环上无dropout）\n",
    "- 正则化非时间变换\n",
    "- 强制对缺失输入特征的鲁棒性\n",
    "- 一致掩码（变分）减少方差"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
