{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文20：神经图灵机\n",
    "## Alex Graves, Greg Wayne, Ivo Danihelka (2014)\n",
    "\n",
    "### 可微分读写的外部记忆\n",
    "\n",
    "NTM通过注意力机制增强了神经网络，使其能够读写外部记忆。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 外部记忆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, num_slots, slot_size):\n",
    "        \"\"\"\n",
    "        External memory bank\n",
    "        \n",
    "        num_slots: Number of memory locations (N)\n",
    "        slot_size: Size of each memory vector (M)\n",
    "        \"\"\"\n",
    "        self.num_slots = num_slots\n",
    "        self.slot_size = slot_size\n",
    "        \n",
    "        # Initialize memory to small random values\n",
    "        self.memory = np.random.randn(num_slots, slot_size) * 0.01\n",
    "    \n",
    "    def read(self, weights):\n",
    "        \"\"\"\n",
    "        Read from memory using attention weights\n",
    "        \n",
    "        weights: (num_slots,) attention distribution\n",
    "        Returns: (slot_size,) weighted combination of memory rows\n",
    "        \"\"\"\n",
    "        return np.dot(weights, self.memory)\n",
    "    \n",
    "    def write(self, weights, erase_vector, add_vector):\n",
    "        \"\"\"\n",
    "        Write to memory using erase and add operations\n",
    "        \n",
    "        weights: (num_slots,) where to write\n",
    "        erase_vector: (slot_size,) what to erase\n",
    "        add_vector: (slot_size,) what to add\n",
    "        \"\"\"\n",
    "        # Erase: M_t = M_{t-1} * (1 - w_t ⊗ e_t)\n",
    "        erase = np.outer(weights, erase_vector)\n",
    "        self.memory = self.memory * (1 - erase)\n",
    "        \n",
    "        # Add: M_t = M_t + w_t ⊗ a_t\n",
    "        add = np.outer(weights, add_vector)\n",
    "        self.memory = self.memory + add\n",
    "    \n",
    "    def get_memory(self):\n",
    "        return self.memory.copy()\n",
    "\n",
    "# Test memory\n",
    "memory = Memory(num_slots=8, slot_size=4)\n",
    "print(f\"Memory initialized: {memory.num_slots} slots × {memory.slot_size} dimensions\")\n",
    "print(f\"Memory shape: {memory.memory.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于内容的寻址\n",
    "\n",
    "基于内容相似度来关注记忆位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"Cosine similarity between vectors\"\"\"\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v) + 1e-8)\n",
    "\n",
    "def softmax(x, beta=1.0):\n",
    "    \"\"\"Softmax with temperature beta\"\"\"\n",
    "    x = beta * x\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def content_addressing(memory, key, beta):\n",
    "    \"\"\"\n",
    "    Content-based addressing\n",
    "    \n",
    "    memory: (num_slots, slot_size)\n",
    "    key: (slot_size,) query vector\n",
    "    beta: sharpness parameter (> 0)\n",
    "    \n",
    "    Returns: (num_slots,) attention weights\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity with each memory row\n",
    "    similarities = np.array([\n",
    "        cosine_similarity(key, memory[i]) \n",
    "        for i in range(len(memory))\n",
    "    ])\n",
    "    \n",
    "    # Apply softmax with sharpness\n",
    "    weights = softmax(similarities, beta=beta)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Test content addressing\n",
    "key = np.random.randn(memory.slot_size)\n",
    "beta = 2.0\n",
    "\n",
    "weights = content_addressing(memory.memory, key, beta)\n",
    "print(f\"\\nContent-based addressing:\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Attention weights: {weights}\")\n",
    "print(f\"Sum of weights: {weights.sum():.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(len(weights)), weights)\n",
    "plt.xlabel('Memory Slot')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.title('Content-Based Addressing Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于位置的寻址\n",
    "\n",
    "基于相对位置转移注意力（用于顺序访问）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(weights_content, weights_prev, g):\n",
    "    \"\"\"\n",
    "    Interpolate between content and previous weights\n",
    "    \n",
    "    g: gate in [0, 1]\n",
    "      g=1: use only content weights\n",
    "      g=0: use only previous weights\n",
    "    \"\"\"\n",
    "    return g * weights_content + (1 - g) * weights_prev\n",
    "\n",
    "def convolutional_shift(weights, shift_weights):\n",
    "    \"\"\"\n",
    "    Rotate attention weights by shift distribution\n",
    "    \n",
    "    shift_weights: distribution over [-1, 0, +1] shifts\n",
    "    \"\"\"\n",
    "    num_slots = len(weights)\n",
    "    shifted = np.zeros_like(weights)\n",
    "    \n",
    "    # Apply each shift\n",
    "    for shift_idx, shift_amount in enumerate([-1, 0, 1]):\n",
    "        rolled = np.roll(weights, shift_amount)\n",
    "        shifted += shift_weights[shift_idx] * rolled\n",
    "    \n",
    "    return shifted\n",
    "\n",
    "def sharpening(weights, gamma):\n",
    "    \"\"\"\n",
    "    Sharpen attention distribution\n",
    "    \n",
    "    gamma >= 1: larger values = sharper distribution\n",
    "    \"\"\"\n",
    "    weights = weights ** gamma\n",
    "    return weights / (np.sum(weights) + 1e-8)\n",
    "\n",
    "# Test location-based operations\n",
    "weights_prev = np.array([0.05, 0.1, 0.2, 0.3, 0.2, 0.1, 0.04, 0.01])\n",
    "weights_content = content_addressing(memory.memory, key, beta=2.0)\n",
    "\n",
    "# Interpolation\n",
    "g = 0.7  # Favor content\n",
    "weights_gated = interpolation(weights_content, weights_prev, g)\n",
    "\n",
    "# Shift\n",
    "shift_weights = np.array([0.1, 0.8, 0.1])  # Mostly stay, little shift\n",
    "weights_shifted = convolutional_shift(weights_gated, shift_weights)\n",
    "\n",
    "# Sharpen\n",
    "gamma = 2.0\n",
    "weights_sharp = sharpening(weights_shifted, gamma)\n",
    "\n",
    "# Visualize addressing pipeline\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "axes[0, 0].bar(range(len(weights_prev)), weights_prev)\n",
    "axes[0, 0].set_title('Previous Weights')\n",
    "axes[0, 0].set_ylim(0, 0.5)\n",
    "\n",
    "axes[0, 1].bar(range(len(weights_content)), weights_content)\n",
    "axes[0, 1].set_title('Content Weights')\n",
    "axes[0, 1].set_ylim(0, 0.5)\n",
    "\n",
    "axes[0, 2].bar(range(len(weights_gated)), weights_gated)\n",
    "axes[0, 2].set_title(f'Gated (g={g})')\n",
    "axes[0, 2].set_ylim(0, 0.5)\n",
    "\n",
    "axes[1, 0].bar(range(len(shift_weights)), shift_weights, color='orange')\n",
    "axes[1, 0].set_title('Shift Distribution')\n",
    "axes[1, 0].set_xticks([0, 1, 2])\n",
    "axes[1, 0].set_xticklabels(['-1', '0', '+1'])\n",
    "\n",
    "axes[1, 1].bar(range(len(weights_shifted)), weights_shifted, color='green')\n",
    "axes[1, 1].set_title('After Shift')\n",
    "axes[1, 1].set_ylim(0, 0.5)\n",
    "\n",
    "axes[1, 2].bar(range(len(weights_sharp)), weights_sharp, color='red')\n",
    "axes[1, 2].set_title(f'Sharpened (γ={gamma})')\n",
    "axes[1, 2].set_ylim(0, 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAddressing pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整NTM读写头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTMHead:\n",
    "    def __init__(self, memory_slots, memory_size, controller_size):\n",
    "        self.memory_slots = memory_slots\n",
    "        self.memory_size = memory_size\n",
    "        \n",
    "        # Parameters produced by controller\n",
    "        # Key for content addressing\n",
    "        self.W_key = np.random.randn(memory_size, controller_size) * 0.1\n",
    "        \n",
    "        # Strength (beta)\n",
    "        self.W_beta = np.random.randn(1, controller_size) * 0.1\n",
    "        \n",
    "        # Gate (g)\n",
    "        self.W_g = np.random.randn(1, controller_size) * 0.1\n",
    "        \n",
    "        # Shift weights\n",
    "        self.W_shift = np.random.randn(3, controller_size) * 0.1\n",
    "        \n",
    "        # Sharpening (gamma)\n",
    "        self.W_gamma = np.random.randn(1, controller_size) * 0.1\n",
    "        \n",
    "        # For write head: erase and add vectors\n",
    "        self.W_erase = np.random.randn(memory_size, controller_size) * 0.1\n",
    "        self.W_add = np.random.randn(memory_size, controller_size) * 0.1\n",
    "        \n",
    "        # Previous weights\n",
    "        self.weights_prev = np.ones(memory_slots) / memory_slots\n",
    "    \n",
    "    def address(self, memory, controller_output):\n",
    "        \"\"\"\n",
    "        Compute addressing weights from controller output\n",
    "        \"\"\"\n",
    "        # Content addressing\n",
    "        key = np.tanh(np.dot(self.W_key, controller_output))\n",
    "        beta = np.exp(np.dot(self.W_beta, controller_output))[0] + 1e-4\n",
    "        weights_content = content_addressing(memory, key, beta)\n",
    "        \n",
    "        # Interpolation\n",
    "        g = 1 / (1 + np.exp(-np.dot(self.W_g, controller_output)))[0]  # sigmoid\n",
    "        weights_gated = interpolation(weights_content, self.weights_prev, g)\n",
    "        \n",
    "        # Shift\n",
    "        shift_logits = np.dot(self.W_shift, controller_output)\n",
    "        shift_weights = softmax(shift_logits)\n",
    "        weights_shifted = convolutional_shift(weights_gated, shift_weights)\n",
    "        \n",
    "        # Sharpen\n",
    "        gamma = np.exp(np.dot(self.W_gamma, controller_output))[0] + 1.0\n",
    "        weights = sharpening(weights_shifted, gamma)\n",
    "        \n",
    "        self.weights_prev = weights\n",
    "        return weights\n",
    "    \n",
    "    def read(self, memory, weights):\n",
    "        \"\"\"Read from memory\"\"\"\n",
    "        return memory.read(weights)\n",
    "    \n",
    "    def write(self, memory, weights, controller_output):\n",
    "        \"\"\"Write to memory\"\"\"\n",
    "        erase = 1 / (1 + np.exp(-np.dot(self.W_erase, controller_output)))  # sigmoid\n",
    "        add = np.tanh(np.dot(self.W_add, controller_output))\n",
    "        memory.write(weights, erase, add)\n",
    "\n",
    "print(\"NTM Head created with full addressing mechanism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试任务：复制序列\n",
    "\n",
    "NTM的经典任务：将序列从输入复制到输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple copy task\n",
    "memory = Memory(num_slots=8, slot_size=4)\n",
    "controller_size = 16\n",
    "head = NTMHead(memory.num_slots, memory.slot_size, controller_size)\n",
    "\n",
    "# Input sequence\n",
    "sequence = [\n",
    "    np.array([1, 0, 0, 0]),\n",
    "    np.array([0, 1, 0, 0]),\n",
    "    np.array([0, 0, 1, 0]),\n",
    "    np.array([0, 0, 0, 1]),\n",
    "]\n",
    "\n",
    "# Write phase: store sequence in memory\n",
    "memory_states = [memory.get_memory()]\n",
    "write_weights_history = []\n",
    "\n",
    "for i, item in enumerate(sequence):\n",
    "    # Simulate controller output (random for demo)\n",
    "    controller_out = np.random.randn(controller_size)\n",
    "    \n",
    "    # Get write weights\n",
    "    weights = head.address(memory.memory, controller_out)\n",
    "    write_weights_history.append(weights)\n",
    "    \n",
    "    # Write to memory\n",
    "    head.write(memory, weights, controller_out)\n",
    "    memory_states.append(memory.get_memory())\n",
    "\n",
    "# Visualize write process\n",
    "fig, axes = plt.subplots(1, len(sequence) + 1, figsize=(16, 4))\n",
    "\n",
    "# Initial memory\n",
    "axes[0].imshow(memory_states[0], cmap='RdBu', aspect='auto')\n",
    "axes[0].set_title('Initial Memory')\n",
    "axes[0].set_ylabel('Memory Slot')\n",
    "axes[0].set_xlabel('Dimension')\n",
    "\n",
    "# After each write\n",
    "for i in range(len(sequence)):\n",
    "    axes[i+1].imshow(memory_states[i+1], cmap='RdBu', aspect='auto')\n",
    "    axes[i+1].set_title(f'After Write {i+1}')\n",
    "    axes[i+1].set_xlabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Memory Evolution During Write', y=1.05)\n",
    "plt.show()\n",
    "\n",
    "# Show write attention patterns\n",
    "write_weights = np.array(write_weights_history).T\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(write_weights, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Write Weight')\n",
    "plt.xlabel('Write Step')\n",
    "plt.ylabel('Memory Slot')\n",
    "plt.title('Write Attention Patterns')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nWrote {len(sequence)} items to memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心要点\n",
    "\n",
    "### NTM架构：\n",
    "1. **控制器**：神经网络（LSTM/FF），产生控制信号\n",
    "2. **记忆矩阵**：外部记忆（N × M）\n",
    "3. **读头**：基于注意力的读取\n",
    "4. **写头**：基于注意力的写入，使用擦除+添加\n",
    "\n",
    "### 寻址机制：\n",
    "1. **基于内容**：与记忆内容的相似度\n",
    "2. **基于位置**：相对转移（顺序访问）\n",
    "3. **组合**：在内容和位置之间进行插值\n",
    "\n",
    "### 寻址流程：\n",
    "```\n",
    "内容寻址 → 插值 → 转移 → 锐化\n",
    "```\n",
    "\n",
    "### 写操作：\n",
    "- **擦除**：M_t = M_{t-1} ⊙ (1 - w_t ⊗ e_t)\n",
    "- **添加**：M_t = M_t + (w_t ⊗ a_t)\n",
    "- 允许选择性修改\n",
    "\n",
    "### 能力：\n",
    "- 复制和召回序列\n",
    "- 学习算法（排序、复制等）\n",
    "- 泛化到更长的序列\n",
    "- 可微分的记忆访问\n",
    "\n",
    "### 局限性：\n",
    "- 计算开销大（需要对所有记忆进行注意力）\n",
    "- 难以训练\n",
    "- 记忆大小固定\n",
    "\n",
    "### 影响：\n",
    "- 启发了可微分记忆研究\n",
    "- 导致：可微分神经计算机（DNC）、记忆网络\n",
    "- 展示了神经网络可以学习算法\n",
    "- 现代外部记忆系统的前驱"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
