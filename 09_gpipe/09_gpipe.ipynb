{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文9：GPipe - 使用流水线并行高效训练巨型神经网络\n",
    "\n",
    "**论文**: Huang et al. (2019) - GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism\n",
    "\n",
    "**核心洞察**: 训练非常大的神经网络需要将其拆分到多个设备上。GPipe 引入了**流水线并行**（pipeline parallelism），结合**微批处理**（micro-batching）和**重计算**（re-materialization），可以高效训练无法放入单个加速器的模型。\n",
    "\n",
    "## 核心概念\n",
    "\n",
    "### 1. 流水线并行\n",
    "- 将模型拆分为 **K 个分区**分布在 K 个设备上\n",
    "- 每个设备保存连续的层\n",
    "- 数据在流水线中流动: Device 1 → Device 2 → ... → Device K\n",
    "\n",
    "### 2. 微批处理\n",
    "- 将大小为 N 的小批（mini-batch）拆分为 M 个大小为 N/M 的微批（micro-batches）\n",
    "- 顺序地在流水线中处理微批\n",
    "- **减少气泡时间**（设备空闲时间）\n",
    "\n",
    "### 3. F-then-B 调度\n",
    "```\n",
    "前向所有 M 个微批，然后反向所有 M 个微批\n",
    "Device 1: F1 F2 F3 F4 ........... B4 B3 B2 B1\n",
    "Device 2: .. F1 F2 F3 F4 ....... B4 B3 B2 B1\n",
    "Device 3: .... F1 F2 F3 F4 ..... B4 B3 B2 B1\n",
    "Device 4: ...... F1 F2 F3 F4 ... B4 B3 B2 B1\n",
    "```\n",
    "\n",
    "### 4. 重计算（梯度检查点）\n",
    "- 不存储所有激活值（内存密集）\n",
    "- 仅在分区边界设置检查点\n",
    "- 在反向传播时重新计算中间激活值\n",
    "- **以计算换内存**\n",
    "\n",
    "### 5. 气泡时间\n",
    "- 设备空闲的时间比例: **(K-1) / (K-1 + M)**\n",
    "- 更多微批 M → 更少气泡时间\n",
    "- 更多设备 K → 更多气泡时间\n",
    "\n",
    "---\n",
    "\n",
    "## 实现概述\n",
    "\n",
    "我们将实现:\n",
    "1. 模型在\"模拟\"设备上的分区\n",
    "2. 微批拆分和调度\n",
    "3. 通过流水线的前向和反向传播\n",
    "4. 梯度累积\n",
    "5. 内存高效的重计算\n",
    "6. 与数据并行的比较\n",
    "7. 气泡时间分析\n",
    "\n",
    "让我们开始实现！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1节：模型分区和流水线结构\n",
    "\n",
    "GPipe 的第一步是将大模型分为 K 个段，每个分配给不同的设备。\n",
    "\n",
    "## 分区策略\n",
    "\n",
    "对于有 L 层的模型:\n",
    "- **均匀分区**: 每个分区获得约 L/K 层\n",
    "- **平衡分区**: 按计算时间或内存分区\n",
    "\n",
    "我们将实现一个简单的多层网络并均匀分区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Layer:\n",
    "    \"\"\"A single neural network layer.\"\"\"\n",
    "    W: np.ndarray  # Weight matrix\n",
    "    b: np.ndarray  # Bias vector\n",
    "    activation: str = 'relu'  # 'relu', 'tanh', or 'linear'\n",
    "    \n",
    "    def forward(self, x: np.ndarray, store_activation: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Forward pass: z = W @ x + b, a = activation(z)\"\"\"\n",
    "        z = x @ self.W + self.b  # Linear transformation\n",
    "        \n",
    "        # Apply activation function\n",
    "        if self.activation == 'relu':\n",
    "            a = np.maximum(0, z)\n",
    "        elif self.activation == 'tanh':\n",
    "            a = np.tanh(z)\n",
    "        elif self.activation == 'linear':\n",
    "            a = z\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
    "        \n",
    "        return a, z if store_activation else None\n",
    "    \n",
    "    def backward(self, da: np.ndarray, z: np.ndarray, x: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Backward pass: compute gradients.\"\"\"\n",
    "        # Activation gradient\n",
    "        if self.activation == 'relu':\n",
    "            dz = da * (z > 0)\n",
    "        elif self.activation == 'tanh':\n",
    "            dz = da * (1 - np.tanh(z)**2)\n",
    "        elif self.activation == 'linear':\n",
    "            dz = da\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
    "        \n",
    "        # Parameter gradients\n",
    "        dW = x.T @ dz\n",
    "        db = np.sum(dz, axis=0)\n",
    "        \n",
    "        # Input gradient (for previous layer)\n",
    "        dx = dz @ self.W.T\n",
    "        \n",
    "        return dx, dW, db\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Partition:\n",
    "    \"\"\"A partition of the model (subset of layers assigned to one device).\"\"\"\n",
    "    device_id: int\n",
    "    layers: List[Layer]\n",
    "    \n",
    "    def forward(self, x: np.ndarray, store_activations: bool = True) -> Tuple[np.ndarray, List[Tuple]]:\n",
    "        \"\"\"Forward pass through all layers in this partition.\"\"\"\n",
    "        activations = []  # Store (x, z) for each layer if needed\n",
    "        \n",
    "        current = x\n",
    "        for layer in self.layers:\n",
    "            if store_activations:\n",
    "                activations.append(current)  # Store input to this layer\n",
    "            \n",
    "            current, z = layer.forward(current, store_activation=store_activations)\n",
    "            \n",
    "            if store_activations:\n",
    "                activations.append(z)  # Store pre-activation\n",
    "        \n",
    "        return current, activations\n",
    "    \n",
    "    def backward(self, dout: np.ndarray, activations: List) -> Tuple[np.ndarray, List[Tuple]]:\n",
    "        \"\"\"Backward pass through all layers in this partition.\"\"\"\n",
    "        gradients = []  # Store (dW, db) for each layer\n",
    "        \n",
    "        da = dout\n",
    "        # Go through layers in reverse\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            \n",
    "            # Get stored activations\n",
    "            x = activations[2*i]      # Input to this layer\n",
    "            z = activations[2*i + 1]  # Pre-activation\n",
    "            \n",
    "            # Compute gradients\n",
    "            da, dW, db = layer.backward(da, z, x)\n",
    "            gradients.insert(0, (dW, db))\n",
    "        \n",
    "        return da, gradients  # da is gradient w.r.t. partition input\n",
    "\n",
    "\n",
    "def create_model(layer_dims: List[int], activations: List[str]) -> List[Layer]:\n",
    "    \"\"\"Create a multi-layer neural network.\n",
    "    \n",
    "    Args:\n",
    "        layer_dims: [input_dim, hidden1, hidden2, ..., output_dim]\n",
    "        activations: Activation for each layer\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(len(layer_dims) - 1):\n",
    "        W = np.random.randn(layer_dims[i], layer_dims[i+1]) * np.sqrt(2.0 / layer_dims[i])\n",
    "        b = np.zeros(layer_dims[i+1])\n",
    "        layers.append(Layer(W, b, activations[i]))\n",
    "    return layers\n",
    "\n",
    "\n",
    "def partition_model(layers: List[Layer], num_partitions: int) -> List[Partition]:\n",
    "    \"\"\"Partition layers uniformly across devices.\"\"\"\n",
    "    num_layers = len(layers)\n",
    "    layers_per_partition = num_layers // num_partitions\n",
    "    \n",
    "    partitions = []\n",
    "    for k in range(num_partitions):\n",
    "        start = k * layers_per_partition\n",
    "        if k == num_partitions - 1:\n",
    "            # Last partition gets any remaining layers\n",
    "            end = num_layers\n",
    "        else:\n",
    "            end = (k + 1) * layers_per_partition\n",
    "        \n",
    "        partition_layers = layers[start:end]\n",
    "        partitions.append(Partition(device_id=k, layers=partition_layers))\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "\n",
    "# Example: Create and partition a 12-layer network\n",
    "layer_dims = [128] + [256] * 10 + [10]  # Input=128, 10 hidden layers of 256, output=10\n",
    "activations = ['relu'] * 10 + ['linear']  # ReLU for hidden, linear for output\n",
    "\n",
    "model_layers = create_model(layer_dims, activations)\n",
    "print(f\"Created model with {len(model_layers)} layers\")\n",
    "\n",
    "# Partition across 4 \"devices\"\n",
    "K = 4\n",
    "partitions = partition_model(model_layers, K)\n",
    "\n",
    "print(f\"\\nPartitioned model into {K} partitions:\")\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"  Device {i}: {len(partition.layers)} layers\")\n",
    "\n",
    "print(\"\\n✓ Model partitioning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2节：微批处理策略\n",
    "\n",
    "GPipe 将每个小批拆分为 M 个**微批**以提高流水线利用率。\n",
    "\n",
    "## 为什么需要微批处理？\n",
    "\n",
    "没有微批处理:\n",
    "```\n",
    "Device 1: [Forward] .................... [Backward]\n",
    "Device 2:          [Forward] .......... [Backward]\n",
    "Device 3:                   [Forward] [Backward]\n",
    "          ^^^^^^^^                     ^^^^^^^^^^\n",
    "          Bubble                       Bubble\n",
    "```\n",
    "\n",
    "有 M 个微批:\n",
    "```\n",
    "Device 1: F1 F2 F3 F4 ........... B4 B3 B2 B1\n",
    "Device 2:    F1 F2 F3 F4 ....... B4 B3 B2 B1\n",
    "Device 3:       F1 F2 F3 F4 .... B4 B3 B2 B1\n",
    "          ^^                              ^^\n",
    "          Smaller bubble\n",
    "```\n",
    "\n",
    "**气泡比例**: (K-1) / (K-1 + M)\n",
    "- 更多微批 → 更少气泡时间\n",
    "- 但更多微批 → 更多开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_microbatches(X: np.ndarray, y: np.ndarray, num_microbatches: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Split mini-batch into micro-batches.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data (batch_size, features)\n",
    "        y: Labels (batch_size, ...)\n",
    "        num_microbatches: M (number of micro-batches)\n",
    "    \n",
    "    Returns:\n",
    "        List of (X_micro, y_micro) tuples\n",
    "    \"\"\"\n",
    "    batch_size = X.shape[0]\n",
    "    microbatch_size = batch_size // num_microbatches\n",
    "    \n",
    "    if batch_size % num_microbatches != 0:\n",
    "        raise ValueError(f\"Batch size {batch_size} must be divisible by num_microbatches {num_microbatches}\")\n",
    "    \n",
    "    microbatches = []\n",
    "    for m in range(num_microbatches):\n",
    "        start = m * microbatch_size\n",
    "        end = (m + 1) * microbatch_size\n",
    "        microbatches.append((X[start:end], y[start:end]))\n",
    "    \n",
    "    return microbatches\n",
    "\n",
    "\n",
    "def compute_bubble_fraction(K: int, M: int) -> float:\n",
    "    \"\"\"Theoretical bubble fraction for GPipe.\n",
    "    \n",
    "    Formula: (K - 1) / (K - 1 + M)\n",
    "    \n",
    "    Args:\n",
    "        K: Number of devices/partitions\n",
    "        M: Number of micro-batches\n",
    "    \"\"\"\n",
    "    return (K - 1) / (K - 1 + M)\n",
    "\n",
    "\n",
    "# Example: Analyze bubble fraction\n",
    "K_values = [2, 4, 8, 16]\n",
    "M_values = [1, 2, 4, 8, 16, 32, 64]\n",
    "\n",
    "print(\"Bubble Fraction Analysis:\")\n",
    "print(\"\\nM (micro-batches) →\")\n",
    "print(\"K ↓\\t\" + \"\\t\".join(f\"{M:d}\" for M in M_values))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for K in K_values:\n",
    "    row = f\"{K}\\t\"\n",
    "    for M in M_values:\n",
    "        bubble = compute_bubble_fraction(K, M)\n",
    "        row += f\"{bubble:.3f}\\t\"\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  - More devices (K) → more bubble time (devices wait for pipeline)\")\n",
    "print(\"  - More micro-batches (M) → less bubble time (pipeline stays full)\")\n",
    "print(\"  - With K=4, M=8: bubble fraction = 27.3% (device idle 27% of time)\")\n",
    "print(\"  - With K=4, M=32: bubble fraction = 8.6% (much better!)\")\n",
    "\n",
    "# Example micro-batching\n",
    "batch_size = 32\n",
    "M = 8\n",
    "X_batch = np.random.randn(batch_size, 128)\n",
    "y_batch = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "microbatches = split_into_microbatches(X_batch, y_batch, M)\n",
    "print(f\"\\n\\nSplit batch of {batch_size} into {M} micro-batches:\")\n",
    "for i, (X_m, y_m) in enumerate(microbatches):\n",
    "    print(f\"  Micro-batch {i}: X shape {X_m.shape}, y shape {y_m.shape}\")\n",
    "\n",
    "print(\"\\n✓ Micro-batching complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3节：通过流水线的前向传播（F-then-B 调度）\n",
    "\n",
    "GPipe 使用**F-then-B 调度**:\n",
    "1. 前向传播所有 M 个微批通过流水线\n",
    "2. 反向传播所有 M 个微批通过流水线（反向顺序）\n",
    "\n",
    "## 时间线示例 (K=3 设备, M=4 微批):\n",
    "\n",
    "```\n",
    "时间 →  0   1   2   3   4   5   6   7   8   9   10  11  12\n",
    "设备0:  F0  F1  F2  F3  ... ... ... B3  B2  B1  B0\n",
    "设备1:  ... F0  F1  F2  F3  ... ... ... B3  B2  B1  B0\n",
    "设备2:  ... ... F0  F1  F2  F3  ... ... ... B3  B2  B1  B0\n",
    "```\n",
    "\n",
    "图例:\n",
    "- **F0** = 前向传播微批 0\n",
    "- **B3** = 反向传播微批 3\n",
    "- **...** = 气泡（设备空闲）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineEvent:\n",
    "    \"\"\"Records when a device executes an operation.\"\"\"\n",
    "    time_step: int\n",
    "    device_id: int\n",
    "    operation: str  # 'forward' or 'backward'\n",
    "    microbatch_id: int\n",
    "\n",
    "\n",
    "class GPipePipeline:\n",
    "    \"\"\"GPipe pipeline with F-then-B schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, partitions: List[Partition]):\n",
    "        self.partitions = partitions\n",
    "        self.K = len(partitions)  # Number of devices\n",
    "        \n",
    "        # For tracking execution timeline\n",
    "        self.events = []  # List of PipelineEvent\n",
    "    \n",
    "    def forward_pipeline(self, microbatches: List[Tuple[np.ndarray, np.ndarray]], \n",
    "                        store_activations: bool = True) -> Tuple[List[np.ndarray], List[List]]:\n",
    "        \"\"\"Forward pass: process all micro-batches through pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            outputs: List of final outputs for each micro-batch\n",
    "            all_activations: List of activation lists (one per micro-batch)\n",
    "        \"\"\"\n",
    "        M = len(microbatches)\n",
    "        \n",
    "        # Storage for outputs and activations\n",
    "        outputs = [None] * M\n",
    "        all_activations = [[None] * self.K for _ in range(M)]  # [microbatch][partition]\n",
    "        \n",
    "        # F-then-B schedule: Forward all micro-batches\n",
    "        time_step = 0\n",
    "        \n",
    "        for m in range(M):\n",
    "            X_micro, y_micro = microbatches[m]\n",
    "            current = X_micro\n",
    "            \n",
    "            # Forward through each partition\n",
    "            for k, partition in enumerate(self.partitions):\n",
    "                self.events.append(PipelineEvent(time_step, k, 'forward', m))\n",
    "                \n",
    "                current, activations = partition.forward(current, store_activations)\n",
    "                all_activations[m][k] = activations\n",
    "                \n",
    "                time_step += 1\n",
    "            \n",
    "            outputs[m] = current\n",
    "        \n",
    "        return outputs, all_activations\n",
    "    \n",
    "    def backward_pipeline(self, outputs: List[np.ndarray], \n",
    "                         labels: List[np.ndarray],\n",
    "                         all_activations: List[List]) -> List[List[List[Tuple]]]:\n",
    "        \"\"\"Backward pass: process all micro-batches in reverse.\n",
    "        \n",
    "        Returns:\n",
    "            all_gradients: [microbatch][partition][(dW, db) for each layer]\n",
    "        \"\"\"\n",
    "        M = len(outputs)\n",
    "        \n",
    "        # Storage for gradients\n",
    "        all_gradients = [[None] * self.K for _ in range(M)]\n",
    "        \n",
    "        # Find current time step (after forward passes)\n",
    "        time_step = max(e.time_step for e in self.events) + 1\n",
    "        \n",
    "        # Backward all micro-batches in reverse order\n",
    "        for m in range(M - 1, -1, -1):\n",
    "            # Compute loss gradient (simple MSE for demonstration)\n",
    "            dout = 2 * (outputs[m] - labels[m]) / labels[m].shape[0]\n",
    "            \n",
    "            # Backward through each partition in reverse\n",
    "            for k in range(self.K - 1, -1, -1):\n",
    "                partition = self.partitions[k]\n",
    "                activations = all_activations[m][k]\n",
    "                \n",
    "                self.events.append(PipelineEvent(time_step, k, 'backward', m))\n",
    "                \n",
    "                dout, gradients = partition.backward(dout, activations)\n",
    "                all_gradients[m][k] = gradients\n",
    "                \n",
    "                time_step += 1\n",
    "        \n",
    "        return all_gradients\n",
    "    \n",
    "    def get_timeline_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Convert events to a K×T matrix for visualization.\n",
    "        \n",
    "        Matrix values:\n",
    "            0 = bubble (idle)\n",
    "            m+1 = forward micro-batch m\n",
    "            -(m+1) = backward micro-batch m\n",
    "        \"\"\"\n",
    "        max_time = max(e.time_step for e in self.events) + 1\n",
    "        timeline = np.zeros((self.K, max_time))\n",
    "        \n",
    "        for event in self.events:\n",
    "            value = event.microbatch_id + 1\n",
    "            if event.operation == 'backward':\n",
    "                value = -value\n",
    "            timeline[event.device_id, event.time_step] = value\n",
    "        \n",
    "        return timeline\n",
    "\n",
    "\n",
    "# Test forward pass\n",
    "print(\"Testing GPipe forward pass...\\n\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = GPipePipeline(partitions)\n",
    "\n",
    "# Create micro-batches\n",
    "M = 4\n",
    "batch_size = 16\n",
    "X_batch = np.random.randn(batch_size, 128)\n",
    "y_batch_onehot = np.eye(10)[np.random.randint(0, 10, batch_size)]\n",
    "\n",
    "microbatches = split_into_microbatches(X_batch, y_batch_onehot, M)\n",
    "\n",
    "# Forward pass\n",
    "outputs, all_activations = pipeline.forward_pipeline(microbatches)\n",
    "\n",
    "print(f\"Processed {M} micro-batches through {pipeline.K} devices\")\n",
    "print(f\"Output shapes: {[out.shape for out in outputs]}\")\n",
    "print(f\"Total forward events: {len([e for e in pipeline.events if e.operation == 'forward'])}\")\n",
    "\n",
    "# Backward pass\n",
    "labels = [mb[1] for mb in microbatches]\n",
    "all_gradients = pipeline.backward_pipeline(outputs, labels, all_activations)\n",
    "\n",
    "print(f\"Total backward events: {len([e for e in pipeline.events if e.operation == 'backward'])}\")\n",
    "print(f\"\\nTotal time steps: {max(e.time_step for e in pipeline.events) + 1}\")\n",
    "\n",
    "print(\"\\n✓ Pipeline forward and backward passes complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4节：跨微批的梯度累积\n",
    "\n",
    "处理完所有 M 个微批后，我们需要:\n",
    "1. **累积**来自所有微批的梯度\n",
    "2. **平均**它们（因为它们来自同一个小批）\n",
    "3. **应用**累积的梯度来更新参数\n",
    "\n",
    "这等同于一次性处理整个小批，但流水线利用率更高！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_gradients(all_gradients: List[List[List[Tuple]]]) -> List[List[Tuple]]:\n",
    "    \"\"\"Accumulate and average gradients from all micro-batches.\n",
    "    \n",
    "    Args:\n",
    "        all_gradients: [microbatch][partition][(dW, db) per layer]\n",
    "    \n",
    "    Returns:\n",
    "        accumulated: [partition][(dW, db) per layer] - averaged over micro-batches\n",
    "    \"\"\"\n",
    "    M = len(all_gradients)  # Number of micro-batches\n",
    "    K = len(all_gradients[0])  # Number of partitions\n",
    "    \n",
    "    # Initialize accumulated gradients (copy structure from first micro-batch)\n",
    "    accumulated = []\n",
    "    for k in range(K):\n",
    "        partition_grads = []\n",
    "        for layer_idx in range(len(all_gradients[0][k])):\n",
    "            # Sum gradients across micro-batches\n",
    "            dW_sum = sum(all_gradients[m][k][layer_idx][0] for m in range(M))\n",
    "            db_sum = sum(all_gradients[m][k][layer_idx][1] for m in range(M))\n",
    "            \n",
    "            # Average (since micro-batches are part of same mini-batch)\n",
    "            dW_avg = dW_sum / M\n",
    "            db_avg = db_sum / M\n",
    "            \n",
    "            partition_grads.append((dW_avg, db_avg))\n",
    "        \n",
    "        accumulated.append(partition_grads)\n",
    "    \n",
    "    return accumulated\n",
    "\n",
    "\n",
    "def apply_gradients(partitions: List[Partition], gradients: List[List[Tuple]], learning_rate: float):\n",
    "    \"\"\"Apply accumulated gradients to update parameters.\n",
    "    \n",
    "    Args:\n",
    "        partitions: List of model partitions\n",
    "        gradients: [partition][(dW, db) per layer]\n",
    "        learning_rate: Learning rate for SGD\n",
    "    \"\"\"\n",
    "    for k, partition in enumerate(partitions):\n",
    "        partition_grads = gradients[k]\n",
    "        \n",
    "        for layer_idx, layer in enumerate(partition.layers):\n",
    "            dW, db = partition_grads[layer_idx]\n",
    "            \n",
    "            # SGD update\n",
    "            layer.W -= learning_rate * dW\n",
    "            layer.b -= learning_rate * db\n",
    "\n",
    "\n",
    "# Test gradient accumulation\n",
    "print(\"Testing gradient accumulation...\\n\")\n",
    "\n",
    "# We already have all_gradients from previous cell\n",
    "accumulated_grads = accumulate_gradients(all_gradients)\n",
    "\n",
    "print(f\"Accumulated gradients for {len(accumulated_grads)} partitions:\")\n",
    "for k, partition_grads in enumerate(accumulated_grads):\n",
    "    print(f\"  Partition {k}: {len(partition_grads)} layers\")\n",
    "    for i, (dW, db) in enumerate(partition_grads[:2]):  # Show first 2 layers\n",
    "        print(f\"    Layer {i}: dW shape {dW.shape}, db shape {db.shape}\")\n",
    "        print(f\"             dW norm: {np.linalg.norm(dW):.6f}, db norm: {np.linalg.norm(db):.6f}\")\n",
    "\n",
    "# Apply gradients\n",
    "learning_rate = 0.01\n",
    "old_W = partitions[0].layers[0].W.copy()\n",
    "\n",
    "apply_gradients(partitions, accumulated_grads, learning_rate)\n",
    "\n",
    "new_W = partitions[0].layers[0].W\n",
    "weight_change = np.linalg.norm(new_W - old_W)\n",
    "\n",
    "print(f\"\\nApplied gradients with learning rate {learning_rate}\")\n",
    "print(f\"Weight change (first layer): {weight_change:.6f}\")\n",
    "\n",
    "print(\"\\n✓ Gradient accumulation and application complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5节：重计算（梯度检查点）\n",
    "\n",
    "**问题**: 存储所有 M 个微批在 K 个分区上的激活值需要 O(M × K × layer_memory) 内存。\n",
    "\n",
    "**解决方案**: **重计算**（梯度检查点）\n",
    "- 仅在**分区边界**检查点激活值\n",
    "- 反向传播期间，**重新计算**中间激活值\n",
    "- 权衡: ~33% 额外计算换取 ~K× 内存节省\n",
    "\n",
    "## 内存对比\n",
    "\n",
    "**无重计算**:\n",
    "- 存储所有分区中所有层的激活值\n",
    "- 内存: O(M × L)，其中 L = 总层数\n",
    "\n",
    "**有重计算**:\n",
    "- 仅在分区边界存储激活值\n",
    "- 内存: O(M × K)，其中 K = 分区数 (K << L)\n",
    "- 按需重新计算中间激活值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPipePipelineWithRemat:\n",
    "    \"\"\"GPipe with re-materialization (gradient checkpointing).\"\"\"\n",
    "    \n",
    "    def __init__(self, partitions: List[Partition]):\n",
    "        self.partitions = partitions\n",
    "        self.K = len(partitions)\n",
    "        self.events = []\n",
    "    \n",
    "    def forward_pipeline_remat(self, microbatches: List[Tuple[np.ndarray, np.ndarray]]) -> Tuple[List, List]:\n",
    "        \"\"\"Forward pass with re-materialization: only store partition boundary activations.\n",
    "        \n",
    "        Returns:\n",
    "            outputs: Final outputs for each micro-batch\n",
    "            boundary_inputs: Inputs to each partition (for recomputation)\n",
    "        \"\"\"\n",
    "        M = len(microbatches)\n",
    "        \n",
    "        outputs = [None] * M\n",
    "        # Only store inputs to each partition (boundary activations)\n",
    "        boundary_inputs = [[None] * self.K for _ in range(M)]\n",
    "        \n",
    "        time_step = 0\n",
    "        \n",
    "        for m in range(M):\n",
    "            X_micro, y_micro = microbatches[m]\n",
    "            current = X_micro\n",
    "            \n",
    "            for k, partition in enumerate(self.partitions):\n",
    "                # Store input to this partition (boundary)\n",
    "                boundary_inputs[m][k] = current.copy()\n",
    "                \n",
    "                self.events.append(PipelineEvent(time_step, k, 'forward', m))\n",
    "                \n",
    "                # Forward pass WITHOUT storing intermediate activations\n",
    "                current, _ = partition.forward(current, store_activations=False)\n",
    "                \n",
    "                time_step += 1\n",
    "            \n",
    "            outputs[m] = current\n",
    "        \n",
    "        return outputs, boundary_inputs\n",
    "    \n",
    "    def backward_pipeline_remat(self, outputs: List[np.ndarray],\n",
    "                                labels: List[np.ndarray],\n",
    "                                boundary_inputs: List[List]) -> List[List[List[Tuple]]]:\n",
    "        \"\"\"Backward pass with re-materialization: recompute activations as needed.\"\"\"\n",
    "        M = len(outputs)\n",
    "        all_gradients = [[None] * self.K for _ in range(M)]\n",
    "        \n",
    "        time_step = max(e.time_step for e in self.events) + 1\n",
    "        \n",
    "        for m in range(M - 1, -1, -1):\n",
    "            dout = 2 * (outputs[m] - labels[m]) / labels[m].shape[0]\n",
    "            \n",
    "            for k in range(self.K - 1, -1, -1):\n",
    "                partition = self.partitions[k]\n",
    "                \n",
    "                self.events.append(PipelineEvent(time_step, k, 'backward', m))\n",
    "                \n",
    "                # RECOMPUTE activations for this partition\n",
    "                partition_input = boundary_inputs[m][k]\n",
    "                _, activations = partition.forward(partition_input, store_activations=True)\n",
    "                \n",
    "                # Now compute gradients using recomputed activations\n",
    "                dout, gradients = partition.backward(dout, activations)\n",
    "                all_gradients[m][k] = gradients\n",
    "                \n",
    "                time_step += 1\n",
    "        \n",
    "        return all_gradients\n",
    "\n",
    "\n",
    "def estimate_memory_usage(M: int, K: int, layers_per_partition: int, \n",
    "                         activation_size_mb: float, with_remat: bool) -> float:\n",
    "    \"\"\"Estimate memory usage with and without re-materialization.\n",
    "    \n",
    "    Args:\n",
    "        M: Number of micro-batches\n",
    "        K: Number of partitions\n",
    "        layers_per_partition: Average layers per partition\n",
    "        activation_size_mb: Memory for one layer's activations (MB)\n",
    "        with_remat: Use re-materialization?\n",
    "    \n",
    "    Returns:\n",
    "        Estimated memory in MB\n",
    "    \"\"\"\n",
    "    if with_remat:\n",
    "        # Only store boundary inputs (K per micro-batch)\n",
    "        return M * K * activation_size_mb\n",
    "    else:\n",
    "        # Store all intermediate activations\n",
    "        total_layers = K * layers_per_partition\n",
    "        return M * total_layers * activation_size_mb\n",
    "\n",
    "\n",
    "# Test re-materialization\n",
    "print(\"Testing re-materialization...\\n\")\n",
    "\n",
    "# Create fresh pipeline with remat\n",
    "pipeline_remat = GPipePipelineWithRemat(partitions)\n",
    "\n",
    "# Forward with remat\n",
    "outputs_remat, boundary_inputs = pipeline_remat.forward_pipeline_remat(microbatches)\n",
    "\n",
    "print(\"Forward pass with re-materialization:\")\n",
    "print(f\"  Stored boundary inputs: {len(boundary_inputs)} micro-batches × {len(boundary_inputs[0])} partitions\")\n",
    "print(f\"  Boundary input shapes: {[bi[0].shape for bi in boundary_inputs]}\")\n",
    "\n",
    "# Backward with remat\n",
    "gradients_remat = pipeline_remat.backward_pipeline_remat(outputs_remat, labels, boundary_inputs)\n",
    "\n",
    "print(f\"\\nBackward pass with re-materialization:\")\n",
    "print(f\"  Gradients computed: {len(gradients_remat)} micro-batches × {len(gradients_remat[0])} partitions\")\n",
    "\n",
    "# Memory analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Memory Usage Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "M_test = 8\n",
    "K_test = 4\n",
    "layers_per_partition = 3\n",
    "activation_size_mb = 10  # MB per layer activation\n",
    "\n",
    "mem_without = estimate_memory_usage(M_test, K_test, layers_per_partition, activation_size_mb, with_remat=False)\n",
    "mem_with = estimate_memory_usage(M_test, K_test, layers_per_partition, activation_size_mb, with_remat=True)\n",
    "\n",
    "print(f\"\\nConfiguration: M={M_test}, K={K_test}, {layers_per_partition} layers/partition\")\n",
    "print(f\"  Without re-materialization: {mem_without:.1f} MB\")\n",
    "print(f\"  With re-materialization:    {mem_with:.1f} MB\")\n",
    "print(f\"  Memory savings:             {mem_without / mem_with:.1f}×\")\n",
    "\n",
    "print(\"\\n✓ Re-materialization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6节：流水线调度可视化和气泡分析\n",
    "\n",
    "让我们可视化 F-then-B 调度并量化气泡时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pipeline_schedule(pipeline: GPipePipeline, title: str = \"GPipe Schedule (F-then-B)\"):\n",
    "    \"\"\"Visualize pipeline execution timeline.\"\"\"\n",
    "    timeline = pipeline.get_timeline_matrix()\n",
    "    K, T = timeline.shape\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Create color map\n",
    "    # Positive = forward (warm colors), negative = backward (cool colors), 0 = bubble (white)\n",
    "    M = int(np.max(np.abs(timeline)))\n",
    "    colors_forward = plt.cm.Reds(np.linspace(0.3, 0.9, M))\n",
    "    colors_backward = plt.cm.Blues(np.linspace(0.3, 0.9, M))\n",
    "    \n",
    "    # Plot timeline\n",
    "    for k in range(K):\n",
    "        for t in range(T):\n",
    "            val = timeline[k, t]\n",
    "            if val > 0:  # Forward\n",
    "                color = colors_forward[int(val) - 1]\n",
    "                label = f'F{int(val)-1}'\n",
    "            elif val < 0:  # Backward\n",
    "                color = colors_backward[int(-val) - 1]\n",
    "                label = f'B{int(-val)-1}'\n",
    "            else:  # Bubble\n",
    "                color = 'white'\n",
    "                label = ''\n",
    "            \n",
    "            rect = plt.Rectangle((t, k), 1, 1, facecolor=color, edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            if label:\n",
    "                ax.text(t + 0.5, k + 0.5, label, ha='center', va='center', \n",
    "                       fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, T)\n",
    "    ax.set_ylim(0, K)\n",
    "    ax.set_xlabel('Time Step', fontsize=12)\n",
    "    ax.set_ylabel('Device', fontsize=12)\n",
    "    ax.set_yticks(np.arange(K) + 0.5)\n",
    "    ax.set_yticklabels([f'Device {k}' for k in range(K)])\n",
    "    ax.set_xticks(np.arange(T) + 0.5)\n",
    "    ax.set_xticklabels(np.arange(T))\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='salmon', label='Forward pass'),\n",
    "        Patch(facecolor='lightblue', label='Backward pass'),\n",
    "        Patch(facecolor='white', edgecolor='black', label='Bubble (idle)')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_actual_bubble_time(timeline: np.ndarray) -> float:\n",
    "    \"\"\"Compute actual bubble fraction from timeline.\"\"\"\n",
    "    total_steps = timeline.size\n",
    "    bubble_steps = np.sum(timeline == 0)\n",
    "    return bubble_steps / total_steps\n",
    "\n",
    "\n",
    "# Visualize the pipeline we created earlier\n",
    "print(\"Visualizing GPipe pipeline schedule...\\n\")\n",
    "\n",
    "visualize_pipeline_schedule(pipeline_remat, f\"GPipe: K={K} devices, M={M} micro-batches\")\n",
    "\n",
    "# Analyze bubble time\n",
    "timeline = pipeline_remat.get_timeline_matrix()\n",
    "actual_bubble = compute_actual_bubble_time(timeline)\n",
    "theoretical_bubble = compute_bubble_fraction(K, M)\n",
    "\n",
    "print(f\"\\nBubble Time Analysis (K={K}, M={M}):\")\n",
    "print(f\"  Theoretical bubble fraction: {theoretical_bubble:.3f} ({theoretical_bubble*100:.1f}%)\")\n",
    "print(f\"  Actual bubble fraction:      {actual_bubble:.3f} ({actual_bubble*100:.1f}%)\")\n",
    "print(f\"  Pipeline efficiency:         {(1-actual_bubble)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n✓ Schedule visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7节：比较 - 流水线 vs 数据并行\n",
    "\n",
    "让我们比较 GPipe（流水线并行）与传统数据并行。\n",
    "\n",
    "## 数据并行\n",
    "- 在每个设备上复制整个模型\n",
    "- 在设备间拆分批次\n",
    "- 同步梯度（all-reduce）\n",
    "- **限制**: 模型必须能放入单个设备\n",
    "\n",
    "## 流水线并行 (GPipe)\n",
    "- 在设备间拆分模型\n",
    "- 所有设备在同一批次上工作（不同微批）\n",
    "- 无需梯度同步\n",
    "- **优势**: 可以训练超过单个设备内存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_parallelism(model_layers: List[Layer], \n",
    "                             batch_size: int, \n",
    "                             num_devices: int) -> Dict[str, float]:\n",
    "    \"\"\"Simulate data parallelism timing.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with timing breakdown\n",
    "    \"\"\"\n",
    "    # Each device processes batch_size/num_devices examples\n",
    "    local_batch_size = batch_size // num_devices\n",
    "    \n",
    "    # Timing (arbitrary units)\n",
    "    forward_time = len(model_layers) * 1.0  # One unit per layer\n",
    "    backward_time = len(model_layers) * 1.0\n",
    "    allreduce_time = 2.0  # Communication overhead\n",
    "    \n",
    "    total_time = forward_time + backward_time + allreduce_time\n",
    "    \n",
    "    return {\n",
    "        'forward': forward_time,\n",
    "        'backward': backward_time,\n",
    "        'communication': allreduce_time,\n",
    "        'total': total_time,\n",
    "        'efficiency': (forward_time + backward_time) / total_time\n",
    "    }\n",
    "\n",
    "\n",
    "def simulate_pipeline_parallelism(model_layers: List[Layer],\n",
    "                                 batch_size: int,\n",
    "                                 num_devices: int,\n",
    "                                 num_microbatches: int) -> Dict[str, float]:\n",
    "    \"\"\"Simulate pipeline parallelism timing.\"\"\"\n",
    "    layers_per_device = len(model_layers) // num_devices\n",
    "    \n",
    "    # Time for one micro-batch through one partition\n",
    "    forward_time_per_micro = layers_per_device * 1.0\n",
    "    backward_time_per_micro = layers_per_device * 1.0\n",
    "    \n",
    "    # Total pipeline time\n",
    "    # Fill pipeline: (K-1) + M micro-batches\n",
    "    # Each step: forward or backward through one partition\n",
    "    total_forward_steps = (num_devices - 1) + num_microbatches\n",
    "    total_backward_steps = (num_devices - 1) + num_microbatches\n",
    "    \n",
    "    total_time = (total_forward_steps + total_backward_steps) * layers_per_device\n",
    "    \n",
    "    # Compute time (excluding bubbles)\n",
    "    compute_time = 2 * num_microbatches * layers_per_device * num_devices\n",
    "    \n",
    "    return {\n",
    "        'forward': total_forward_steps * layers_per_device,\n",
    "        'backward': total_backward_steps * layers_per_device,\n",
    "        'communication': 0,  # No inter-device communication!\n",
    "        'total': total_time,\n",
    "        'efficiency': compute_time / (total_time * num_devices),\n",
    "        'bubble_fraction': compute_bubble_fraction(num_devices, num_microbatches)\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare both approaches\n",
    "print(\"Comparing Pipeline Parallelism vs Data Parallelism\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_layers = 12\n",
    "batch_size = 32\n",
    "num_devices = 4\n",
    "num_microbatches = 8\n",
    "\n",
    "# Simulate data parallelism\n",
    "data_parallel_stats = simulate_data_parallelism(model_layers, batch_size, num_devices)\n",
    "\n",
    "print(\"Data Parallelism:\")\n",
    "print(f\"  Configuration: {num_devices} devices, batch size {batch_size}\")\n",
    "print(f\"  Forward time:        {data_parallel_stats['forward']:.1f} units\")\n",
    "print(f\"  Backward time:       {data_parallel_stats['backward']:.1f} units\")\n",
    "print(f\"  Communication time:  {data_parallel_stats['communication']:.1f} units (all-reduce)\")\n",
    "print(f\"  Total time:          {data_parallel_stats['total']:.1f} units\")\n",
    "print(f\"  Efficiency:          {data_parallel_stats['efficiency']*100:.1f}%\")\n",
    "print(f\"  ⚠️  Limitation: Model must fit on single device!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Simulate pipeline parallelism\n",
    "pipeline_stats = simulate_pipeline_parallelism(model_layers, batch_size, num_devices, num_microbatches)\n",
    "\n",
    "print(\"Pipeline Parallelism (GPipe):\")\n",
    "print(f\"  Configuration: {num_devices} devices, {num_microbatches} micro-batches\")\n",
    "print(f\"  Forward time:        {pipeline_stats['forward']:.1f} units\")\n",
    "print(f\"  Backward time:       {pipeline_stats['backward']:.1f} units\")\n",
    "print(f\"  Communication time:  {pipeline_stats['communication']:.1f} units (none!)\")\n",
    "print(f\"  Total time:          {pipeline_stats['total']:.1f} units\")\n",
    "print(f\"  Efficiency:          {pipeline_stats['efficiency']*100:.1f}%\")\n",
    "print(f\"  Bubble fraction:     {pipeline_stats['bubble_fraction']*100:.1f}%\")\n",
    "print(f\"  ✓ Advantage: Can train models {num_devices}× larger!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"  • Data parallel: Fast, but model must fit on one device\")\n",
    "print(\"  • Pipeline parallel: Enables training of giant models\")\n",
    "print(\"  • GPipe: No communication overhead (unlike data parallel)\")\n",
    "print(\"  • Trade-off: Pipeline has bubble time, data parallel has communication\")\n",
    "\n",
    "print(\"\\n✓ Comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8节：完整的 GPipe 训练循环\n",
    "\n",
    "让我们整合所有内容: 使用 GPipe 的完整训练循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(outputs: List[np.ndarray], labels: List[np.ndarray]) -> float:\n",
    "    \"\"\"Compute average loss across micro-batches (MSE for simplicity).\"\"\"\n",
    "    total_loss = 0.0\n",
    "    for output, label in zip(outputs, labels):\n",
    "        total_loss += np.mean((output - label) ** 2)\n",
    "    return total_loss / len(outputs)\n",
    "\n",
    "\n",
    "def train_gpipe_epoch(pipeline: GPipePipelineWithRemat,\n",
    "                     X_train: np.ndarray,\n",
    "                     y_train: np.ndarray,\n",
    "                     batch_size: int,\n",
    "                     num_microbatches: int,\n",
    "                     learning_rate: float) -> List[float]:\n",
    "    \"\"\"Train one epoch with GPipe.\n",
    "    \n",
    "    Returns:\n",
    "        List of losses for each mini-batch\n",
    "    \"\"\"\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        # Get mini-batch\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        # Split into micro-batches\n",
    "        microbatches = split_into_microbatches(X_batch, y_batch, num_microbatches)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, boundary_inputs = pipeline.forward_pipeline_remat(microbatches)\n",
    "        \n",
    "        # Compute loss\n",
    "        labels = [mb[1] for mb in microbatches]\n",
    "        loss = compute_loss(outputs, labels)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        all_gradients = pipeline.backward_pipeline_remat(outputs, labels, boundary_inputs)\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        accumulated_grads = accumulate_gradients(all_gradients)\n",
    "        \n",
    "        # Update parameters\n",
    "        apply_gradients(pipeline.partitions, accumulated_grads, learning_rate)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Generate synthetic dataset\n",
    "print(\"Creating synthetic dataset...\\n\")\n",
    "\n",
    "num_train = 256\n",
    "input_dim = 128\n",
    "output_dim = 10\n",
    "\n",
    "X_train = np.random.randn(num_train, input_dim)\n",
    "y_train_labels = np.random.randint(0, output_dim, num_train)\n",
    "y_train = np.eye(output_dim)[y_train_labels]\n",
    "\n",
    "print(f\"Dataset: {num_train} samples, input dim {input_dim}, output dim {output_dim}\")\n",
    "\n",
    "# Create fresh model and pipeline\n",
    "print(\"\\nInitializing GPipe model...\")\n",
    "\n",
    "layer_dims = [input_dim] + [256] * 10 + [output_dim]\n",
    "activations = ['relu'] * 10 + ['linear']\n",
    "model_layers = create_model(layer_dims, activations)\n",
    "\n",
    "K = 4\n",
    "partitions = partition_model(model_layers, K)\n",
    "pipeline = GPipePipelineWithRemat(partitions)\n",
    "\n",
    "print(f\"  Model: {len(model_layers)} layers\")\n",
    "print(f\"  Partitions: {K} devices\")\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 32\n",
    "num_microbatches = 8\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Micro-batches: {num_microbatches}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training GPipe model...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pipeline.events = []  # Reset events for this epoch\n",
    "    \n",
    "    losses = train_gpipe_epoch(pipeline, X_train, y_train, \n",
    "                               batch_size, num_microbatches, learning_rate)\n",
    "    \n",
    "    avg_loss = np.mean(losses)\n",
    "    all_losses.extend(losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Average Loss = {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9节：可视化和分析\n",
    "\n",
    "让我们创建 GPipe 性能的全面可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Training Loss Curve\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Training loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(all_losses, linewidth=2, color='darkblue')\n",
    "ax.set_xlabel('Mini-batch', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('GPipe Training Loss', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Bubble fraction vs M (micro-batches)\n",
    "ax = axes[0, 1]\n",
    "M_range = np.arange(1, 65)\n",
    "K_values_plot = [2, 4, 8, 16]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "for K_val, color in zip(K_values_plot, colors):\n",
    "    bubbles = [compute_bubble_fraction(K_val, M) for M in M_range]\n",
    "    ax.plot(M_range, bubbles, label=f'K={K_val}', linewidth=2, color=color)\n",
    "\n",
    "ax.set_xlabel('Number of Micro-batches (M)', fontsize=11)\n",
    "ax.set_ylabel('Bubble Fraction', fontsize=11)\n",
    "ax.set_title('Bubble Time vs Micro-batches', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Plot 3: Memory savings with re-materialization\n",
    "ax = axes[1, 0]\n",
    "K_range = np.arange(2, 17)\n",
    "layers_per_partition = 3\n",
    "M_fixed = 8\n",
    "activation_size_mb = 10\n",
    "\n",
    "mem_without_remat = [estimate_memory_usage(M_fixed, K_val, layers_per_partition, \n",
    "                                            activation_size_mb, False) \n",
    "                     for K_val in K_range]\n",
    "mem_with_remat = [estimate_memory_usage(M_fixed, K_val, layers_per_partition, \n",
    "                                        activation_size_mb, True) \n",
    "                  for K_val in K_range]\n",
    "\n",
    "ax.plot(K_range, mem_without_remat, label='Without Remat', linewidth=2, \n",
    "        marker='o', color='red', markersize=6)\n",
    "ax.plot(K_range, mem_with_remat, label='With Remat', linewidth=2, \n",
    "        marker='s', color='green', markersize=6)\n",
    "ax.set_xlabel('Number of Partitions (K)', fontsize=11)\n",
    "ax.set_ylabel('Memory (MB)', fontsize=11)\n",
    "ax.set_title('Memory Usage: Re-materialization Impact', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Pipeline efficiency vs configuration\n",
    "ax = axes[1, 1]\n",
    "M_configs = [4, 8, 16, 32]\n",
    "K_configs = np.arange(2, 17)\n",
    "\n",
    "for M_val in M_configs:\n",
    "    efficiencies = [1 - compute_bubble_fraction(K_val, M_val) for K_val in K_configs]\n",
    "    ax.plot(K_configs, efficiencies, label=f'M={M_val}', linewidth=2, marker='o', markersize=5)\n",
    "\n",
    "ax.set_xlabel('Number of Devices (K)', fontsize=11)\n",
    "ax.set_ylabel('Pipeline Efficiency', fontsize=11)\n",
    "ax.set_title('Pipeline Efficiency vs Configuration', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10节：关键洞察和现代扩展\n",
    "\n",
    "## GPipe 总结\n",
    "\n",
    "### 核心思想\n",
    "1. **流水线并行**: 按层在设备间拆分模型\n",
    "2. **微批处理**: 拆分小批以减少气泡时间\n",
    "3. **重计算**: 以计算换内存效率\n",
    "4. **F-then-B 调度**: 前向所有微批，然后反向所有\n",
    "\n",
    "### 数学洞察\n",
    "\n",
    "**气泡比例**:\n",
    "$$\\text{气泡} = \\frac{K-1}{K-1+M}$$\n",
    "\n",
    "**内存节省**（使用重计算）:\n",
    "$$\\text{内存}_{\\text{remat}} = \\frac{K}{L} \\times \\text{内存}_{\\text{标准}}$$\n",
    "\n",
    "其中 L = 总层数，K = 分区数。\n",
    "\n",
    "**加速**（相比单个设备）:\n",
    "$$\\text{加速} \\approx \\frac{K}{1 + \\frac{K-1}{M}}$$\n",
    "\n",
    "### 何时使用 GPipe\n",
    "\n",
    "**使用 GPipe 当**:\n",
    "- 模型无法放入单个设备\n",
    "- 顺序模型结构（层）\n",
    "- 设备间带宽有限\n",
    "- 可以使用大的 M（许多微批）\n",
    "\n",
    "**避免 GPipe 当**:\n",
    "- 模型能放入单个设备（改用数据并行）\n",
    "- M 很小（气泡时间占主导）\n",
    "- 非顺序架构（例如，大量跳跃连接）\n",
    "\n",
    "---\n",
    "\n",
    "## 现代扩展\n",
    "\n",
    "### 1. PipeDream (Harlap et al., 2018)\n",
    "- **1F1B 调度**: 交织前向和反向\n",
    "- 减少流水线深度\n",
    "- 更好的内存效率\n",
    "\n",
    "### 2. Megatron-LM (Shoeybi et al., 2019)\n",
    "- 结合流水线 + 张量并行\n",
    "- 水平拆分层（层内）\n",
    "- 用于 530B 参数模型\n",
    "\n",
    "### 3. ZeRO (Rajbhandari et al., 2020)\n",
    "- 分区优化器状态、梯度、参数\n",
    "- 补充流水线并行\n",
    "- 无复制地减少内存\n",
    "\n",
    "### 4. Varuna (Athlur et al., 2022)\n",
    "- 自动流水线调度优化\n",
    "- 自适应微批处理\n",
    "- 处理异构设备\n",
    "\n",
    "---\n",
    "\n",
    "## 实际考虑\n",
    "\n",
    "### 最优 M（微批数）\n",
    "- **太小**: 高气泡比例\n",
    "- **太大**: 微批管理的开销\n",
    "- **经验法则**: M ≈ 4×K\n",
    "\n",
    "### 分区策略\n",
    "- 均匀: 每个设备等量层\n",
    "- 平衡: 每个设备等量计算时间\n",
    "- 内存感知: 平衡内存使用\n",
    "\n",
    "### 批次大小\n",
    "- 大批次提高流水线利用率\n",
    "- 但可能损害泛化\n",
    "- 通过学习率缩放补偿\n",
    "\n",
    "---\n",
    "\n",
    "## 与其他论文的联系\n",
    "\n",
    "**Paper 5 (Optimal Brain Damage)**: 剪枝减少模型大小 → 需要更少流水线阶段\n",
    "\n",
    "**Paper 23 (MDL)**: 模型复杂度 vs 数据拟合 → 选择 K（分区）涉及权衡\n",
    "\n",
    "**Paper 14 (Neural Architecture Search)**: 可以使用 GPipe 搜索对单个设备来说太大的架构\n",
    "\n",
    "---\n",
    "\n",
    "## 实际影响\n",
    "\n",
    "GPipe 使得:\n",
    "- **AmoebaNet-B**: 557M 参数（比之前最好的大 8 倍）\n",
    "- **在 ImageNet 上训练**，达到 84.4% top-1 准确率\n",
    "- **GPT-3**: 175B 参数（包括流水线并行在内的技术组合）\n",
    "- **大型语言模型**: 现代 LLM 使用流水线 + 张量 + 数据并行\n",
    "\n",
    "---\n",
    "\n",
    "**GPipe 的遗产**: 表明**模型并行是实用的**，为训练具有数千亿参数的模型铺平了道路。与张量并行和 ZeRO 结合，它构成了现代大规模训练的基础！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: Show trade-off between K and M\n",
    "print(\"=\"*70)\n",
    "print(\"GPipe Configuration Guide\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Choosing K (number of devices):\")\n",
    "print(\"   • Limited by: Number of available accelerators\")\n",
    "print(\"   • More K = Can train larger models\")\n",
    "print(\"   • More K = More bubble time (need larger M to compensate)\")\n",
    "\n",
    "print(\"\\n2. Choosing M (number of micro-batches):\")\n",
    "print(\"   • Rule of thumb: M ≈ 4×K\")\n",
    "print(\"   • Larger M = Less bubble time\")\n",
    "print(\"   • Larger M = More overhead\")\n",
    "print(\"   • Must divide batch size evenly\")\n",
    "\n",
    "print(\"\\n3. Example configurations:\")\n",
    "configs = [\n",
    "    (2, 8, 32),\n",
    "    (4, 16, 64),\n",
    "    (8, 32, 128),\n",
    "    (16, 64, 256),\n",
    "]\n",
    "\n",
    "for K, M, batch in configs:\n",
    "    bubble = compute_bubble_fraction(K, M)\n",
    "    efficiency = 1 - bubble\n",
    "    print(f\"   K={K:2d}, M={M:2d}, batch={batch:3d} → \"\n",
    "          f\"Efficiency={efficiency*100:.1f}%, Bubble={bubble*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ GPipe implementation complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
