{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文10：用于图像识别的深度残差学习\n",
    "## Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015)\n",
    "\n",
    "### ResNet：跳跃连接使超深网络成为可能\n",
    "\n",
    "ResNet 引入了残差连接，使得训练 100+ 层的网络成为可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题：深度网络的退化\n",
    "\n",
    "在 ResNet 之前，增加更多层实际上会使网络性能变差（不是由于过拟合，而是优化困难）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "class PlainLayer:\n",
    "    \"\"\"标准神经网络层\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.randn(output_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.z = np.dot(self.W, x) + self.b\n",
    "        self.a = relu(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        da = dout * relu_derivative(self.z)\n",
    "        self.dW = np.dot(da, self.x.T)\n",
    "        self.db = np.sum(da, axis=1, keepdims=True)\n",
    "        dx = np.dot(self.W.T, da)\n",
    "        return dx\n",
    "\n",
    "class ResidualBlock:\n",
    "    \"\"\"带跳跃连接的残差块: y = F(x) + x\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.layer1 = PlainLayer(size, size)\n",
    "        self.layer2 = PlainLayer(size, size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "        # 残差路径 F(x)\n",
    "        out = self.layer1.forward(x)\n",
    "        out = self.layer2.forward(out)\n",
    "        \n",
    "        # 跳跃连接: F(x) + x\n",
    "        self.out = out + x\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # 梯度通过两条路径流动\n",
    "        # 跳跃连接提供直接路径\n",
    "        dx_residual = self.layer2.backward(dout)\n",
    "        dx_residual = self.layer1.backward(dx_residual)\n",
    "        \n",
    "        # 总梯度: 残差路径 + 跳跃连接\n",
    "        dx = dx_residual + dout  # 这是关键！\n",
    "        return dx\n",
    "\n",
    "print(\"ResNet components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建普通网络 vs ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainNetwork:\n",
    "    \"\"\"没有跳跃连接的普通深度网络\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        self.layers = []\n",
    "        \n",
    "        # 第一层\n",
    "        self.layers.append(PlainLayer(input_size, hidden_size))\n",
    "        \n",
    "        # 隐藏层\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PlainLayer(hidden_size, hidden_size))\n",
    "        \n",
    "        # 输出层\n",
    "        self.layers.append(PlainLayer(hidden_size, input_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "class ResidualNetwork:\n",
    "    \"\"\"带残差连接的深度网络\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_blocks):\n",
    "        # 投影到隐藏层大小\n",
    "        self.input_proj = PlainLayer(input_size, hidden_size)\n",
    "        \n",
    "        # 残差块\n",
    "        self.blocks = [ResidualBlock(hidden_size) for _ in range(num_blocks)]\n",
    "        \n",
    "        # 投影回输出\n",
    "        self.output_proj = PlainLayer(hidden_size, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj.forward(x)\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        x = self.output_proj.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.output_proj.backward(dout)\n",
    "        for block in reversed(self.blocks):\n",
    "            dout = block.backward(dout)\n",
    "        dout = self.input_proj.backward(dout)\n",
    "        return dout\n",
    "\n",
    "# 创建网络\n",
    "input_size = 16\n",
    "hidden_size = 16\n",
    "depth = 10\n",
    "\n",
    "plain_net = PlainNetwork(input_size, hidden_size, depth)\n",
    "resnet = ResidualNetwork(input_size, hidden_size, depth)\n",
    "\n",
    "print(f\"Created Plain Network with {depth} layers\")\n",
    "print(f\"Created ResNet with {depth} residual blocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演示梯度流\n",
    "\n",
    "关键优势：梯度通过跳跃连接更容易流动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_gradient_flow(network, name):\n",
    "    \"\"\"测量不同深度的梯度幅度\"\"\"\n",
    "    # 随机输入\n",
    "    x = np.random.randn(input_size, 1)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = network.forward(x)\n",
    "    \n",
    "    # 创建梯度信号\n",
    "    dout = np.ones_like(output)\n",
    "    \n",
    "    # 反向传播\n",
    "    network.backward(dout)\n",
    "    \n",
    "    # 收集梯度幅度\n",
    "    grad_norms = []\n",
    "    \n",
    "    if isinstance(network, PlainNetwork):\n",
    "        for layer in network.layers:\n",
    "            grad_norm = np.linalg.norm(layer.dW)\n",
    "            grad_norms.append(grad_norm)\n",
    "    else:  # ResNet\n",
    "        grad_norms.append(np.linalg.norm(network.input_proj.dW))\n",
    "        for block in network.blocks:\n",
    "            grad_norm1 = np.linalg.norm(block.layer1.dW)\n",
    "            grad_norm2 = np.linalg.norm(block.layer2.dW)\n",
    "            grad_norms.append(np.mean([grad_norm1, grad_norm2]))\n",
    "        grad_norms.append(np.linalg.norm(network.output_proj.dW))\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "# 测量两个网络的梯度流\n",
    "plain_grads = measure_gradient_flow(plain_net, \"Plain Network\")\n",
    "resnet_grads = measure_gradient_flow(resnet, \"ResNet\")\n",
    "\n",
    "# 绘制比较\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(plain_grads)), plain_grads, 'o-', label='Plain Network', linewidth=2)\n",
    "plt.plot(range(len(resnet_grads)), resnet_grads, 's-', label='ResNet', linewidth=2)\n",
    "plt.xlabel('Layer Depth (deeper →)')\n",
    "plt.ylabel('Gradient Magnitude')\n",
    "plt.title('Gradient Flow: ResNet vs Plain Network')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlain Network - First layer gradient: {plain_grads[0]:.6f}\")\n",
    "print(f\"Plain Network - Last layer gradient: {plain_grads[-1]:.6f}\")\n",
    "print(f\"Gradient ratio (first/last): {plain_grads[0]/plain_grads[-1]:.2f}x\\n\")\n",
    "\n",
    "print(f\"ResNet - First layer gradient: {resnet_grads[0]:.6f}\")\n",
    "print(f\"ResNet - Last layer gradient: {resnet_grads[-1]:.6f}\")\n",
    "print(f\"Gradient ratio (first/last): {resnet_grads[0]/resnet_grads[-1]:.2f}x\")\n",
    "\n",
    "print(f\"\\nResNet maintains gradient flow {(plain_grads[0]/plain_grads[-1]) / (resnet_grads[0]/resnet_grads[-1]):.1f}x better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化学习到的表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成合成类图像数据\n",
    "def generate_patterns(num_samples=100, size=8):\n",
    "    \"\"\"生成简单的 2D 模式\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        pattern = np.zeros((size, size))\n",
    "        \n",
    "        if i % 3 == 0:\n",
    "            # 水平线\n",
    "            pattern[2:3, :] = 1\n",
    "            label = 0\n",
    "        elif i % 3 == 1:\n",
    "            # 垂直线\n",
    "            pattern[:, 3:4] = 1\n",
    "            label = 1\n",
    "        else:\n",
    "            # 对角线\n",
    "            np.fill_diagonal(pattern, 1)\n",
    "            label = 2\n",
    "        \n",
    "        # 添加噪声\n",
    "        pattern += np.random.randn(size, size) * 0.1\n",
    "        \n",
    "        X.append(pattern.flatten())\n",
    "        y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = generate_patterns(num_samples=30, size=4)\n",
    "\n",
    "# 可视化样本模式\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    sample = X[i].reshape(4, 4)\n",
    "    ax.imshow(sample, cmap='gray')\n",
    "    ax.set_title(f'Pattern Type {y[i]}')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(X)} pattern samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恒等映射：核心洞察\n",
    "\n",
    "**关键洞察**：如果恒等映射是最优的，残差应该学习 F(x) = 0，这比学习 H(x) = x 更容易"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示恒等映射\n",
    "x = np.random.randn(hidden_size, 1)\n",
    "\n",
    "# 初始化残差块\n",
    "block = ResidualBlock(hidden_size)\n",
    "\n",
    "# 如果权重接近零，F(x) ≈ 0\n",
    "block.layer1.W *= 0.001\n",
    "block.layer2.W *= 0.001\n",
    "\n",
    "# 前向传播\n",
    "output = block.forward(x)\n",
    "\n",
    "# 检查输出是否 ≈ 输入（恒等）\n",
    "identity_error = np.linalg.norm(output - x)\n",
    "\n",
    "print(\"Identity Mapping Demonstration:\")\n",
    "print(f\"Input norm: {np.linalg.norm(x):.4f}\")\n",
    "print(f\"Output norm: {np.linalg.norm(output):.4f}\")\n",
    "print(f\"Identity error ||F(x) + x - x||: {identity_error:.6f}\")\n",
    "print(f\"\\nWith near-zero weights, residual block ≈ identity function!\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x.flatten(), 'o-', label='Input x', alpha=0.7)\n",
    "plt.plot(output.flatten(), 's-', label='Output (x + F(x))', alpha=0.7)\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Identity Mapping: Output ≈ Input')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "residual = output - x\n",
    "plt.bar(range(len(residual)), residual.flatten())\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Residual F(x)')\n",
    "plt.title('Learned Residual ≈ 0')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较网络深度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_depth_scaling():\n",
    "    \"\"\"测试梯度流如何随深度变化\"\"\"\n",
    "    depths = [5, 10, 20, 30, 40]\n",
    "    plain_ratios = []\n",
    "    resnet_ratios = []\n",
    "    \n",
    "    for depth in depths:\n",
    "        # 创建网络\n",
    "        plain = PlainNetwork(input_size, hidden_size, depth)\n",
    "        res = ResidualNetwork(input_size, hidden_size, depth)\n",
    "        \n",
    "        # 测量梯度\n",
    "        plain_grads = measure_gradient_flow(plain, \"Plain\")\n",
    "        res_grads = measure_gradient_flow(res, \"ResNet\")\n",
    "        \n",
    "        # 计算比率（第一层/最后一层梯度）\n",
    "        plain_ratio = plain_grads[0] / (plain_grads[-1] + 1e-10)\n",
    "        res_ratio = res_grads[0] / (res_grads[-1] + 1e-10)\n",
    "        \n",
    "        plain_ratios.append(plain_ratio)\n",
    "        resnet_ratios.append(res_ratio)\n",
    "    \n",
    "    # 绘制\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(depths, plain_ratios, 'o-', label='Plain Network', linewidth=2, markersize=8)\n",
    "    plt.plot(depths, resnet_ratios, 's-', label='ResNet', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Network Depth')\n",
    "    plt.ylabel('Gradient Ratio (first/last layer)')\n",
    "    plt.title('Gradient Flow Degradation with Depth')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nGradient Ratio (first/last) - Higher = Worse gradient flow:\")\n",
    "    for i, d in enumerate(depths):\n",
    "        print(f\"Depth {d:2d}: Plain={plain_ratios[i]:8.2f}, ResNet={resnet_ratios[i]:6.2f} \"\n",
    "              f\"(ResNet is {plain_ratios[i]/resnet_ratios[i]:.1f}x better)\")\n",
    "\n",
    "test_depth_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键要点\n",
    "\n",
    "### 退化问题：\n",
    "- 向普通网络添加更多层会损害性能\n",
    "- **不是**由于过拟合（训练误差也会增加）\n",
    "- 由于优化困难：梯度消失/爆炸\n",
    "\n",
    "### ResNet 解决方案：跳跃连接\n",
    "```\n",
    "y = F(x, {Wi}) + x\n",
    "```\n",
    "\n",
    "**不是学习**：H(x) = 期望映射  \n",
    "**学习残差**：F(x) = H(x) - x  \n",
    "**然后**：H(x) = F(x) + x\n",
    "\n",
    "### 为什么有效：\n",
    "1. **恒等映射更容易**：如果最优映射是恒等，学习 F(x) = 0 比学习 H(x) = x 更容易\n",
    "2. **梯度高速公路**：跳跃连接提供直接的梯度路径\n",
    "3. **加性梯度流**：梯度通过残差和跳跃两条路径流动\n",
    "4. **无额外参数**：跳跃连接是无参数的\n",
    "\n",
    "### 影响：\n",
    "- 使 152 层网络成为可能（之前限制为 20 层）\n",
    "- 赢得 ImageNet 2015（3.57% top-5 误差）\n",
    "- 成为标准架构模式\n",
    "- 启发了变体：DenseNet、ResNeXt 等\n",
    "\n",
    "### 数学洞察：\n",
    "损失 L 对较早层的梯度：\n",
    "```\n",
    "∂L/∂x = ∂L/∂y * (∂F/∂x + ∂x/∂x) = ∂L/∂y * (∂F/∂x + I)\n",
    "```\n",
    "`+ I` 项确保梯度始终流动！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
