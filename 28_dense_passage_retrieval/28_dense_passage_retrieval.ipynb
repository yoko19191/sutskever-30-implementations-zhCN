{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文28：开放域问答的密集通道检索\n",
    "## Vladimir Karpukhin, Barlas Oğuz, Sewon Min, et al., Meta AI (2020)\n",
    "\n",
    "### 密集通道检索 (DPR)\n",
    "\n",
    "学习问题和段落的密集嵌入。通过嵌入空间中的相似性检索。击败 BM25！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 双编码器架构\n",
    "\n",
    "```\n",
    "问题 → 编码器_Q → q (密集向量)\n",
    "段落 → 编码器_P → p (密集向量)\n",
    "\n",
    "相似度: sim(q, p) = q · p  (点积)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextEncoder:\n",
    "    \"\"\"Simplified text encoder (in practice: use BERT)\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        \n",
    "        # Simple RNN weights\n",
    "        self.W_xh = np.random.randn(hidden_dim, embedding_dim) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.b_h = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_out = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "    \n",
    "    def encode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Encode sequence of token IDs to dense vector\n",
    "        Returns: dense embedding (hidden_dim,)\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        # Process tokens\n",
    "        for token_id in token_ids:\n",
    "            # Lookup embedding\n",
    "            x = self.embeddings[token_id].reshape(-1, 1)\n",
    "            \n",
    "            # RNN step\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "        \n",
    "        # Final representation (CLS-like)\n",
    "        output = np.dot(self.W_out, h).flatten()\n",
    "        \n",
    "        # L2 normalize for cosine similarity\n",
    "        output = output / (np.linalg.norm(output) + 1e-8)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create encoders\n",
    "vocab_size = 1000\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "\n",
    "question_encoder = SimpleTextEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "passage_encoder = SimpleTextEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Test\n",
    "test_tokens = [10, 25, 37, 42]\n",
    "q_emb = question_encoder.encode(test_tokens)\n",
    "p_emb = passage_encoder.encode(test_tokens)\n",
    "\n",
    "print(f\"Question embedding shape: {q_emb.shape}\")\n",
    "print(f\"Passage embedding shape: {p_emb.shape}\")\n",
    "print(f\"Similarity (dot product): {np.dot(q_emb, p_emb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合成 QA 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"Simple word tokenizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        words = text.lower().split()\n",
    "        token_ids = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in self.word_to_id:\n",
    "                self.word_to_id[word] = self.next_id\n",
    "                self.id_to_word[self.next_id] = word\n",
    "                self.next_id += 1\n",
    "            token_ids.append(self.word_to_id[word])\n",
    "        \n",
    "        return token_ids\n",
    "\n",
    "# Create synthetic dataset\n",
    "passages = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower in Paris, France.\",\n",
    "    \"The Great Wall of China is a series of fortifications in northern China.\",\n",
    "    \"The Statue of Liberty is a colossal neoclassical sculpture in New York.\",\n",
    "    \"The Colosseum is an oval amphitheatre in the centre of Rome, Italy.\",\n",
    "    \"The Taj Mahal is an ivory-white marble mausoleum in Agra, India.\",\n",
    "    \"Mount Everest is Earth's highest mountain above sea level.\",\n",
    "    \"The Amazon River is the largest river by discharge volume of water.\",\n",
    "    \"The Sahara is a desert on the African continent.\",\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    (\"What is the Eiffel Tower?\", 0),  # (question, relevant_passage_idx)\n",
    "    (\"Where is the Great Wall located?\", 1),\n",
    "    (\"What is the tallest mountain?\", 5),\n",
    "    (\"Where is the Statue of Liberty?\", 2),\n",
    "    (\"What is the largest river?\", 6),\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "passage_tokens = [tokenizer.tokenize(p) for p in passages]\n",
    "question_tokens = [(tokenizer.tokenize(q), idx) for q, idx in questions]\n",
    "\n",
    "print(\"Sample passage:\")\n",
    "print(f\"Text: {passages[0]}\")\n",
    "print(f\"Tokens: {passage_tokens[0][:10]}...\")\n",
    "print(f\"\\nVocabulary size: {tokenizer.next_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码语料库和问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize encoders with correct vocab size\n",
    "vocab_size = tokenizer.next_id\n",
    "question_encoder = SimpleTextEncoder(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "passage_encoder = SimpleTextEncoder(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "\n",
    "# Encode all passages\n",
    "passage_embeddings = []\n",
    "for tokens in passage_tokens:\n",
    "    emb = passage_encoder.encode(tokens)\n",
    "    passage_embeddings.append(emb)\n",
    "passage_embeddings = np.array(passage_embeddings)\n",
    "\n",
    "# Encode questions\n",
    "question_embeddings = []\n",
    "for tokens, _ in question_tokens:\n",
    "    emb = question_encoder.encode(tokens)\n",
    "    question_embeddings.append(emb)\n",
    "question_embeddings = np.array(question_embeddings)\n",
    "\n",
    "print(f\"Passage embeddings: {passage_embeddings.shape}\")\n",
    "print(f\"Question embeddings: {question_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过最大内积搜索 (MIPS) 进行密集检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(query_embedding, passage_embeddings, k=3):\n",
    "    \"\"\"\n",
    "    Retrieve top-k passages for query\n",
    "    Uses dot product similarity (MIPS)\n",
    "    \"\"\"\n",
    "    # Compute similarities\n",
    "    similarities = np.dot(passage_embeddings, query_embedding)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "    top_k_scores = similarities[top_k_indices]\n",
    "    \n",
    "    return top_k_indices, top_k_scores\n",
    "\n",
    "# Test retrieval\n",
    "print(\"\\nDense Retrieval Results:\\n\" + \"=\"*80)\n",
    "for i, (q_tokens, correct_idx) in enumerate(question_tokens):\n",
    "    question_text = questions[i][0]\n",
    "    q_emb = question_embeddings[i]\n",
    "    \n",
    "    # Retrieve\n",
    "    top_indices, top_scores = retrieve_top_k(q_emb, passage_embeddings, k=3)\n",
    "    \n",
    "    print(f\"\\nQ: {question_text}\")\n",
    "    print(f\"Correct passage: #{correct_idx}\")\n",
    "    print(f\"\\nRetrieved (top-3):\")\n",
    "    for rank, (idx, score) in enumerate(zip(top_indices, top_scores), 1):\n",
    "        is_correct = \"✓\" if idx == correct_idx else \"✗\"\n",
    "        print(f\"  {rank}. [{is_correct}] (score={score:.3f}) {passages[idx][:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"(Encoders are untrained, so results are random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用批内负样本进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Numerical stability\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def contrastive_loss(query_emb, positive_emb, negative_embs):\n",
    "    \"\"\"\n",
    "    Contrastive loss (InfoNCE)\n",
    "    \n",
    "    L = -log( exp(q·p+) / (exp(q·p+) + Σ exp(q·p-)) )\n",
    "    \"\"\"\n",
    "    # Positive score\n",
    "    pos_score = np.dot(query_emb, positive_emb)\n",
    "    \n",
    "    # Negative scores\n",
    "    neg_scores = [np.dot(query_emb, neg_emb) for neg_emb in negative_embs]\n",
    "    \n",
    "    # All scores\n",
    "    all_scores = np.array([pos_score] + neg_scores)\n",
    "    \n",
    "    # Softmax\n",
    "    probs = softmax(all_scores)\n",
    "    \n",
    "    # Negative log likelihood (positive should be first)\n",
    "    loss = -np.log(probs[0] + 1e-8)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Simulate training batch\n",
    "batch_size = 3\n",
    "batch_questions = question_embeddings[:batch_size]\n",
    "batch_passages = passage_embeddings[:batch_size]\n",
    "\n",
    "# In-batch negatives: for each question, other passages in batch are negatives\n",
    "total_loss = 0\n",
    "print(\"\\nIn-Batch Negative Training:\\n\" + \"=\"*80)\n",
    "for i in range(batch_size):\n",
    "    q_emb = batch_questions[i]\n",
    "    pos_emb = batch_passages[i]  # Correct passage\n",
    "    \n",
    "    # Negatives: all other passages in batch\n",
    "    neg_embs = [batch_passages[j] for j in range(batch_size) if j != i]\n",
    "    \n",
    "    loss = contrastive_loss(q_emb, pos_emb, neg_embs)\n",
    "    total_loss += loss\n",
    "    \n",
    "    print(f\"Question {i}: loss = {loss:.4f}\")\n",
    "\n",
    "avg_loss = total_loss / batch_size\n",
    "print(f\"\\nAverage batch loss: {avg_loss:.4f}\")\n",
    "print(\"\\nIn-batch negatives: efficient hard negative mining!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化嵌入空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2D projection (PCA-like)\n",
    "def project_2d(embeddings):\n",
    "    \"\"\"Project high-dim embeddings to 2D (simplified PCA)\"\"\"\n",
    "    # Mean center\n",
    "    mean = np.mean(embeddings, axis=0)\n",
    "    centered = embeddings - mean\n",
    "    \n",
    "    # Take first 2 principal components (simplified)\n",
    "    U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
    "    projected = U[:, :2] * S[:2]\n",
    "    \n",
    "    return projected\n",
    "\n",
    "# Project to 2D\n",
    "all_embeddings = np.vstack([passage_embeddings, question_embeddings])\n",
    "projected = project_2d(all_embeddings)\n",
    "\n",
    "passage_2d = projected[:len(passage_embeddings)]\n",
    "question_2d = projected[len(passage_embeddings):]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot passages\n",
    "plt.scatter(passage_2d[:, 0], passage_2d[:, 1], s=200, c='lightblue', \n",
    "           edgecolors='black', linewidths=2, marker='s', label='Passages', zorder=2)\n",
    "\n",
    "# Annotate passages\n",
    "for i, (x, y) in enumerate(passage_2d):\n",
    "    plt.text(x, y-0.15, f'P{i}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot questions\n",
    "plt.scatter(question_2d[:, 0], question_2d[:, 1], s=200, c='lightcoral', \n",
    "           edgecolors='black', linewidths=2, marker='o', label='Questions', zorder=3)\n",
    "\n",
    "# Annotate questions\n",
    "for i, (x, y) in enumerate(question_2d):\n",
    "    plt.text(x, y+0.15, f'Q{i}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Draw connections (question to correct passage)\n",
    "for i, (q_tokens, correct_idx) in enumerate(question_tokens):\n",
    "    q_pos = question_2d[i]\n",
    "    p_pos = passage_2d[correct_idx]\n",
    "    plt.plot([q_pos[0], p_pos[0]], [q_pos[1], p_pos[1]], \n",
    "            'g--', alpha=0.5, linewidth=2, label='Correct' if i == 0 else '')\n",
    "\n",
    "plt.xlabel('Dimension 1', fontsize=12)\n",
    "plt.ylabel('Dimension 2', fontsize=12)\n",
    "plt.title('Dense Retrieval Embedding Space (2D Projection)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nIdeal: Questions close to their relevant passages!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与 BM25（稀疏检索）对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBM25:\n",
    "    \"\"\"Simplified BM25 scoring\"\"\"\n",
    "    def __init__(self, passages, k1=1.5, b=0.75):\n",
    "        self.passages = passages\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        self.doc_freqs = {}\n",
    "        self.avg_doc_len = 0\n",
    "        \n",
    "        all_words = []\n",
    "        for passage in passages:\n",
    "            words = set(passage.lower().split())\n",
    "            all_words.extend(passage.lower().split())\n",
    "            for word in words:\n",
    "                self.doc_freqs[word] = self.doc_freqs.get(word, 0) + 1\n",
    "        \n",
    "        self.avg_doc_len = len(all_words) / len(passages)\n",
    "        self.N = len(passages)\n",
    "    \n",
    "    def score(self, query, passage_idx):\n",
    "        \"\"\"BM25 score for query and passage\"\"\"\n",
    "        query_words = query.lower().split()\n",
    "        passage = self.passages[passage_idx]\n",
    "        passage_words = passage.lower().split()\n",
    "        passage_len = len(passage_words)\n",
    "        \n",
    "        # Count term frequencies\n",
    "        tf = Counter(passage_words)\n",
    "        \n",
    "        score = 0\n",
    "        for word in query_words:\n",
    "            if word not in tf:\n",
    "                continue\n",
    "            \n",
    "            # IDF\n",
    "            df = self.doc_freqs.get(word, 0)\n",
    "            idf = np.log((self.N - df + 0.5) / (df + 0.5) + 1)\n",
    "            \n",
    "            # TF component\n",
    "            freq = tf[word]\n",
    "            norm = 1 - self.b + self.b * (passage_len / self.avg_doc_len)\n",
    "            tf_component = (freq * (self.k1 + 1)) / (freq + self.k1 * norm)\n",
    "            \n",
    "            score += idf * tf_component\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def retrieve(self, query, k=3):\n",
    "        \"\"\"Retrieve top-k passages for query\"\"\"\n",
    "        scores = [self.score(query, i) for i in range(len(self.passages))]\n",
    "        top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "        top_k_scores = [scores[i] for i in top_k_indices]\n",
    "        return top_k_indices, top_k_scores\n",
    "\n",
    "# Create BM25 retriever\n",
    "bm25 = SimpleBM25(passages)\n",
    "\n",
    "# Compare BM25 vs Dense\n",
    "print(\"\\nBM25 vs Dense Retrieval Comparison:\\n\" + \"=\"*80)\n",
    "for i, (question_text, correct_idx) in enumerate(questions):\n",
    "    print(f\"\\nQ: {question_text}\")\n",
    "    print(f\"Correct: #{correct_idx}\")\n",
    "    \n",
    "    # BM25\n",
    "    bm25_indices, bm25_scores = bm25.retrieve(question_text, k=3)\n",
    "    print(f\"\\nBM25 Top-3:\")\n",
    "    for rank, (idx, score) in enumerate(zip(bm25_indices, bm25_scores), 1):\n",
    "        is_correct = \"✓\" if idx == correct_idx else \"✗\"\n",
    "        print(f\"  {rank}. [{is_correct}] (score={score:.3f}) #{idx}\")\n",
    "    \n",
    "    # Dense\n",
    "    q_emb = question_embeddings[i]\n",
    "    dense_indices, dense_scores = retrieve_top_k(q_emb, passage_embeddings, k=3)\n",
    "    print(f\"\\nDense Top-3:\")\n",
    "    for rank, (idx, score) in enumerate(zip(dense_indices, dense_scores), 1):\n",
    "        is_correct = \"✓\" if idx == correct_idx else \"✗\"\n",
    "        print(f\"  {rank}. [{is_correct}] (score={score:.3f}) #{idx}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BM25: Lexical matching (sparse)\")\n",
    "print(\"Dense: Semantic matching (dense embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检索指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, correct_indices, k_values=[1, 3, 5]):\n",
    "    \"\"\"\n",
    "    Compute retrieval metrics:\n",
    "    - Recall@k: % of queries where correct passage is in top-k\n",
    "    - MRR (Mean Reciprocal Rank): average 1/rank of correct passage\n",
    "    \"\"\"\n",
    "    n_queries = len(predictions)\n",
    "    \n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for pred, correct_idx in zip(predictions, correct_indices):\n",
    "        # Find rank of correct passage\n",
    "        if correct_idx in pred:\n",
    "            rank = list(pred).index(correct_idx) + 1\n",
    "            reciprocal_ranks.append(1.0 / rank)\n",
    "            \n",
    "            # Update recall@k\n",
    "            for k in k_values:\n",
    "                if rank <= k:\n",
    "                    recalls[k] += 1\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    # Compute averages\n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    recalls = {k: v / n_queries for k, v in recalls.items()}\n",
    "    \n",
    "    return recalls, mrr\n",
    "\n",
    "# Evaluate both methods\n",
    "bm25_predictions = []\n",
    "dense_predictions = []\n",
    "correct_indices = []\n",
    "\n",
    "for i, (question_text, correct_idx) in enumerate(questions):\n",
    "    # BM25\n",
    "    bm25_top, _ = bm25.retrieve(question_text, k=5)\n",
    "    bm25_predictions.append(bm25_top)\n",
    "    \n",
    "    # Dense\n",
    "    q_emb = question_embeddings[i]\n",
    "    dense_top, _ = retrieve_top_k(q_emb, passage_embeddings, k=5)\n",
    "    dense_predictions.append(dense_top)\n",
    "    \n",
    "    correct_indices.append(correct_idx)\n",
    "\n",
    "# Compute metrics\n",
    "bm25_recalls, bm25_mrr = compute_metrics(bm25_predictions, correct_indices)\n",
    "dense_recalls, dense_mrr = compute_metrics(dense_predictions, correct_indices)\n",
    "\n",
    "# Display\n",
    "print(\"\\nRetrieval Metrics:\\n\" + \"=\"*60)\n",
    "print(f\"{'Metric':<15} {'BM25':<15} {'Dense':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for k in [1, 3, 5]:\n",
    "    print(f\"Recall@{k:<10} {bm25_recalls[k]:<15.2%} {dense_recalls[k]:<15.2%}\")\n",
    "print(f\"MRR{'':<12} {bm25_mrr:<15.3f} {dense_mrr:<15.3f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n(Models are untrained - results are random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键要点\n",
    "\n",
    "### 密集通道检索 (DPR) 架构：\n",
    "\n",
    "**双编码器**：\n",
    "```\n",
    "问题: q → BERT_Q → E_Q(q) = q_emb\n",
    "段落:  p → BERT_P → E_P(p) = p_emb\n",
    "\n",
    "相似度: sim(q, p) = q_emb · p_emb\n",
    "```\n",
    "\n",
    "### 训练目标：\n",
    "\n",
    "**对比损失 (InfoNCE)**：\n",
    "$$\n",
    "L(q_i, p_i^+, p_i^{-1}, ..., p_i^{-n}) = -\\log \\frac{e^{\\text{sim}(q_i, p_i^+)}}{e^{\\text{sim}(q_i, p_i^+)} + \\sum_j e^{\\text{sim}(q_i, p_i^{-j})}}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $p_i^+$: 正（相关）段落\n",
    "- $p_i^{-j}$: 负（不相关）段落\n",
    "\n",
    "### 批内负样本：\n",
    "\n",
    "高效的负样本挖掘：\n",
    "```\n",
    "批次: [(q1, p1+), (q2, p2+), ..., (qB, pB+)]\n",
    "\n",
    "对于 q1：\n",
    "  正样本: p1+\n",
    "  负样本: p2+, p3+, ..., pB+ (来自批次中的其他示例)\n",
    "```\n",
    "\n",
    "**优势**：\n",
    "- 不需要额外的段落\n",
    "- 梯度流经所有示例\n",
    "- 可扩展到大批次大小\n",
    "\n",
    "### 困难负样本挖掘：\n",
    "\n",
    "1. **BM25 负样本**：不是相关但 BM25 分数最高的结果\n",
    "2. **随机负样本**：来自语料库的随机段落\n",
    "3. **批内负样本**：批次中的其他正样本\n",
    "\n",
    "**最佳**：结合全部三种！\n",
    "\n",
    "### 推理（检索）：\n",
    "\n",
    "**离线**：\n",
    "1. 编码所有段落：$P = \\{E_P(p_1), ..., E_P(p_N)\\}$\n",
    "2. 构建 MIPS 索引（例如 FAISS）\n",
    "\n",
    "**在线**（查询时）：\n",
    "1. 编码查询：$q_{emb} = E_Q(q)$\n",
    "2. 搜索索引：top-k 通过 $\\arg\\max_p \\, q_{emb} \\cdot p_{emb}$\n",
    "\n",
    "### DPR vs BM25：\n",
    "\n",
    "| 方面 | BM25 | DPR |\n",
    "|--------|------|-----|\n",
    "| 匹配 | 词法（精确词） | 语义（含义） |\n",
    "| 训练 | 无（启发式） | 从数据学习 |\n",
    "| 鲁棒性 | 对措辞敏感 | 处理改写 |\n",
    "| 速度 | 快（稀疏） | 使用 MIPS 索引时很快 |\n",
    "| 内存 | 低 | 高（密集向量） |\n",
    "\n",
    "### 结果（来自论文）：\n",
    "\n",
    "**Natural Questions**：\n",
    "- BM25: 59.1% Top-20 准确率\n",
    "- DPR: 78.4% Top-20 准确率\n",
    "\n",
    "**WebQuestions**：\n",
    "- BM25: 55.0%\n",
    "- DPR: 75.0%\n",
    "\n",
    "**TREC**：\n",
    "- BM25: 70.9%\n",
    "- DPR: 79.4%\n",
    "\n",
    "### 实现细节：\n",
    "\n",
    "1. **编码器**：BERT-base（110M 参数）\n",
    "2. **嵌入维度**：768（BERT 隐藏层大小）\n",
    "3. **批次大小**：128（大批次用于批内负样本）\n",
    "4. **困难负样本**：每个正样本 1 个 BM25 + 1 个随机\n",
    "5. **训练**：在 59k QA 对上训练约 40 个 epoch\n",
    "\n",
    "### 优势：\n",
    "\n",
    "- ✅ **语义匹配**：理解含义，不仅仅是词\n",
    "- ✅ **端到端**：从问题-段落对学习\n",
    "- ✅ **处理改写**：\"最高峰\" = \"最高峰\"\n",
    "- ✅ **可扩展**：使用 FAISS 进行数十亿段落的 MIPS\n",
    "- ✅ **优于 BM25**：+15-20% 绝对准确率提升\n",
    "\n",
    "### 局限性：\n",
    "\n",
    "- ❌ **需要训练数据**：需要 QA 对\n",
    "- ❌ **内存**：所有段落的密集向量\n",
    "- ❌ **索引更新**：语料库更改时需要重新编码\n",
    "- ❌ **可能遗漏精确匹配**：BM25 对罕见实体更好\n",
    "\n",
    "### 最佳实践：\n",
    "\n",
    "1. **混合检索**：结合 BM25 + DPR\n",
    "2. **大批次**：更多批内负样本\n",
    "3. **困难负样本**：使用 BM25 top 结果\n",
    "4. **微调**：领域特定数据改进结果\n",
    "5. **FAISS**：用于大规模快速 MIPS\n",
    "\n",
    "### 现代扩展：\n",
    "\n",
    "- **ColBERT**：晚期交互用于更好排序\n",
    "- **ANCE**：近似最近邻负样本\n",
    "- **RocketQA**：跨批次负样本\n",
    "- **Contriever**：无监督密集检索\n",
    "- **Dense X Retrieval**：多向量表示\n",
    "\n",
    "### 应用：\n",
    "\n",
    "- 开放域 QA（例如 Google 搜索）\n",
    "- RAG（检索增强生成）\n",
    "- 文档搜索\n",
    "- 语义搜索\n",
    "- 知识库补全"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
