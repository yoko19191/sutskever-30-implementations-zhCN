{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文21：Deep Speech 2 - 端到端语音识别\n",
    "## Dario Amodei et al., Baidu Research (2015)\n",
    "\n",
    "### CTC损失：连接时序分类\n",
    "\n",
    "CTC使得无需帧级对齐即可训练序列模型。这对语音识别至关重要！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对齐问题\n",
    "\n",
    "语音：\"hello\" → 音频帧：[h][h][e][e][l][l][l][o][o]\n",
    "\n",
    "问题：我们不知道哪些帧对应哪些字母！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC introduces blank symbol (ε) to handle alignment\n",
    "# Vocabulary: [a, b, c, ..., z, space, blank]\n",
    "\n",
    "vocab = list('abcdefghijklmnopqrstuvwxyz ') + ['ε']  # ε is blank\n",
    "char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "blank_idx = len(vocab) - 1\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Blank index: {blank_idx}\")\n",
    "print(f\"Sample chars: {vocab[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC对齐规则\n",
    "\n",
    "**折叠规则**：移除空白符号和重复字符\n",
    "- `[h][ε][e][l][l][o]` → \"hello\"\n",
    "- `[h][h][e][ε][l][o]` → \"helo\" \n",
    "- `[h][ε][h][e][l][o]` → \"hhelo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_ctc(sequence, blank_idx):\n",
    "    \"\"\"\n",
    "    Collapse CTC sequence to target string\n",
    "    1. Remove blanks\n",
    "    2. Merge repeated characters\n",
    "    \"\"\"\n",
    "    # Remove blanks\n",
    "    no_blanks = [s for s in sequence if s != blank_idx]\n",
    "    \n",
    "    # Merge repeats\n",
    "    if len(no_blanks) == 0:\n",
    "        return []\n",
    "    \n",
    "    collapsed = [no_blanks[0]]\n",
    "    for s in no_blanks[1:]:\n",
    "        if s != collapsed[-1]:\n",
    "            collapsed.append(s)\n",
    "    \n",
    "    return collapsed\n",
    "\n",
    "# Test collapse\n",
    "examples = [\n",
    "    [char_to_idx['h'], blank_idx, char_to_idx['e'], char_to_idx['l'], char_to_idx['l'], char_to_idx['o']],\n",
    "    [char_to_idx['h'], char_to_idx['h'], char_to_idx['e'], blank_idx, char_to_idx['l'], char_to_idx['o']],\n",
    "    [blank_idx, char_to_idx['h'], blank_idx, char_to_idx['i'], blank_idx],\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    original = ''.join([idx_to_char[i] for i in ex])\n",
    "    collapsed = collapse_ctc(ex, blank_idx)\n",
    "    result = ''.join([idx_to_char[i] for i in collapsed])\n",
    "    print(f\"{original:20s} → {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成合成音频特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_features(text, frames_per_char=3, feature_dim=20):\n",
    "    \"\"\"\n",
    "    Simulate audio features (e.g., MFCCs)\n",
    "    In reality: extract from raw audio\n",
    "    \"\"\"\n",
    "    # Convert text to indices\n",
    "    char_indices = [char_to_idx[c] for c in text]\n",
    "    \n",
    "    # Generate features for each character (repeated frames)\n",
    "    features = []\n",
    "    for char_idx in char_indices:\n",
    "        # Create feature vector for this character\n",
    "        char_feature = np.random.randn(feature_dim) + char_idx * 0.1\n",
    "        \n",
    "        # Repeat for multiple frames (simulate speaking duration)\n",
    "        num_frames = np.random.randint(frames_per_char - 1, frames_per_char + 2)\n",
    "        for _ in range(num_frames):\n",
    "            # Add noise\n",
    "            features.append(char_feature + np.random.randn(feature_dim) * 0.3)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Generate sample\n",
    "text = \"hello\"\n",
    "features = generate_audio_features(text)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"Audio features: {features.shape} (frames × features)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(features.T, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Feature Value')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.ylabel('Feature Dimension')\n",
    "plt.title(f'Synthetic Audio Features for \"{text}\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单RNN声学模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticModel:\n",
    "    \"\"\"RNN that outputs character probabilities per frame\"\"\"\n",
    "    def __init__(self, feature_dim, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # RNN weights\n",
    "        self.W_xh = np.random.randn(hidden_size, feature_dim) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Output layer\n",
    "        self.W_out = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.b_out = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        features: (num_frames, feature_dim)\n",
    "        Returns: (num_frames, vocab_size) - log probabilities\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(len(features)):\n",
    "            x = features[t:t+1].T  # (feature_dim, 1)\n",
    "            \n",
    "            # RNN update\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            \n",
    "            # Output (logits)\n",
    "            logits = np.dot(self.W_out, h) + self.b_out\n",
    "            \n",
    "            # Log softmax\n",
    "            log_probs = logits - np.log(np.sum(np.exp(logits)))\n",
    "            outputs.append(log_probs.flatten())\n",
    "        \n",
    "        return np.array(outputs)  # (num_frames, vocab_size)\n",
    "\n",
    "# Create model\n",
    "feature_dim = 20\n",
    "hidden_size = 32\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = AcousticModel(feature_dim, hidden_size, vocab_size)\n",
    "\n",
    "# Test forward pass\n",
    "log_probs = model.forward(features)\n",
    "print(f\"\\nAcoustic model output: {log_probs.shape}\")\n",
    "print(f\"Each frame has probability distribution over {vocab_size} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC前向算法（简化版）\n",
    "\n",
    "计算给定帧级预测的目标序列概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_naive(log_probs, target, blank_idx):\n",
    "    \"\"\"\n",
    "    Simplified CTC loss computation\n",
    "    \n",
    "    log_probs: (T, vocab_size) - log probabilities per frame\n",
    "    target: list of character indices (without blanks)\n",
    "    blank_idx: index of blank symbol\n",
    "    \n",
    "    This is a simplified version - full CTC uses dynamic programming\n",
    "    \"\"\"\n",
    "    T = len(log_probs)\n",
    "    U = len(target)\n",
    "    \n",
    "    # Insert blanks between characters: a → ε a ε b → ε a ε b ε\n",
    "    extended_target = [blank_idx]\n",
    "    for t in target:\n",
    "        extended_target.extend([t, blank_idx])\n",
    "    S = len(extended_target)\n",
    "    \n",
    "    # Forward algorithm with dynamic programming\n",
    "    # alpha[t, s] = prob of being at position s at time t\n",
    "    log_alpha = np.ones((T, S)) * -np.inf\n",
    "    \n",
    "    # Initialize\n",
    "    log_alpha[0, 0] = log_probs[0, extended_target[0]]\n",
    "    if S > 1:\n",
    "        log_alpha[0, 1] = log_probs[0, extended_target[1]]\n",
    "    \n",
    "    # Forward pass\n",
    "    for t in range(1, T):\n",
    "        for s in range(S):\n",
    "            label = extended_target[s]\n",
    "            \n",
    "            # Option 1: stay at same label (or blank)\n",
    "            candidates = [log_alpha[t-1, s]]\n",
    "            \n",
    "            # Option 2: transition from previous label\n",
    "            if s > 0:\n",
    "                candidates.append(log_alpha[t-1, s-1])\n",
    "            \n",
    "            # Option 3: skip blank (if current is not blank and different from prev)\n",
    "            if s > 1 and label != blank_idx and extended_target[s-2] != label:\n",
    "                candidates.append(log_alpha[t-1, s-2])\n",
    "            \n",
    "            # Log-sum-exp for numerical stability\n",
    "            log_alpha[t, s] = np.logaddexp.reduce(candidates) + log_probs[t, label]\n",
    "    \n",
    "    # Final probability: sum over last two positions (with/without final blank)\n",
    "    log_prob = np.logaddexp(log_alpha[T-1, S-1], log_alpha[T-1, S-2] if S > 1 else -np.inf)\n",
    "    \n",
    "    # CTC loss is negative log probability\n",
    "    return -log_prob, log_alpha\n",
    "\n",
    "# Test CTC loss\n",
    "target = [char_to_idx[c] for c in \"hi\"]\n",
    "loss, alpha = ctc_loss_naive(log_probs, target, blank_idx)\n",
    "\n",
    "print(f\"\\nTarget: 'hi'\")\n",
    "print(f\"CTC Loss: {loss:.4f}\")\n",
    "print(f\"Log probability: {-loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化CTC路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forward probabilities (alpha)\n",
    "target_str = \"hi\"\n",
    "target_indices = [char_to_idx[c] for c in target_str]\n",
    "\n",
    "# Recompute with smaller example\n",
    "small_features = generate_audio_features(target_str, frames_per_char=2)\n",
    "small_log_probs = model.forward(small_features)\n",
    "loss, alpha = ctc_loss_naive(small_log_probs, target_indices, blank_idx)\n",
    "\n",
    "# Create extended target for visualization\n",
    "extended = [blank_idx]\n",
    "for t in target_indices:\n",
    "    extended.extend([t, blank_idx])\n",
    "extended_labels = [idx_to_char[i] for i in extended]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(alpha.T, cmap='hot', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Log Probability')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.ylabel('CTC State')\n",
    "plt.title(f'CTC Forward Algorithm for \"{target_str}\"')\n",
    "plt.yticks(range(len(extended_labels)), extended_labels)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBrighter cells = higher probability paths\")\n",
    "print(\"CTC explores all valid alignments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贪婪CTC解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(log_probs, blank_idx):\n",
    "    \"\"\"\n",
    "    Greedy decoding: pick most likely character at each frame\n",
    "    Then collapse using CTC rules\n",
    "    \"\"\"\n",
    "    # Get most likely character per frame\n",
    "    predictions = np.argmax(log_probs, axis=1)\n",
    "    \n",
    "    # Collapse\n",
    "    decoded = collapse_ctc(predictions.tolist(), blank_idx)\n",
    "    \n",
    "    return decoded, predictions\n",
    "\n",
    "# Test decoding\n",
    "test_text = \"hello\"\n",
    "test_features = generate_audio_features(test_text)\n",
    "test_log_probs = model.forward(test_features)\n",
    "\n",
    "decoded, raw_predictions = greedy_decode(test_log_probs, blank_idx)\n",
    "\n",
    "print(f\"True text: '{test_text}'\")\n",
    "print(f\"\\nFrame-by-frame predictions:\")\n",
    "print(''.join([idx_to_char[i] for i in raw_predictions]))\n",
    "print(f\"\\nAfter CTC collapse:\")\n",
    "print(''.join([idx_to_char[i] for i in decoded]))\n",
    "print(f\"\\n(Model is untrained, so prediction is random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化预测与真值对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability distribution over time\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot log probabilities\n",
    "ax1.imshow(test_log_probs.T, cmap='viridis', aspect='auto')\n",
    "ax1.set_ylabel('Character')\n",
    "ax1.set_xlabel('Time Frame')\n",
    "ax1.set_title('Log Probabilities per Frame (darker = higher prob)')\n",
    "ax1.set_yticks(range(0, vocab_size, 5))\n",
    "ax1.set_yticklabels([vocab[i] for i in range(0, vocab_size, 5)])\n",
    "\n",
    "# Plot predictions\n",
    "ax2.plot(raw_predictions, 'o-', markersize=6)\n",
    "ax2.set_xlabel('Time Frame')\n",
    "ax2.set_ylabel('Predicted Character Index')\n",
    "ax2.set_title('Greedy Predictions')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心要点\n",
    "\n",
    "### CTC问题：\n",
    "- **未知对齐**：不知道哪些音频帧对应哪些字符\n",
    "- **可变长度**：音频帧数多于输出字符数\n",
    "- **无分割**：不知道词/字符从哪里开始/结束\n",
    "\n",
    "### CTC解决方案：\n",
    "1. **空白符号（ε）**：允许重复和静音\n",
    "2. **所有对齐**：对所有有效路径的概率求和\n",
    "3. **端到端**：无需帧级标签即可训练\n",
    "\n",
    "### CTC规则：\n",
    "```\n",
    "1. 插入空白：\"cat\" → \"ε c ε a ε t ε\"\n",
    "2. 任何折叠到目标的路径都有效\n",
    "3. 对所有有效路径的概率求和\n",
    "```\n",
    "\n",
    "### 前向算法：\n",
    "- 在时间和标签位置上进行动态规划\n",
    "- α[t, s] = 在时间t处于位置s的概率\n",
    "- 三种转换：保持、前移、跳过空白\n",
    "\n",
    "### 损失：\n",
    "$$\\mathcal{L}_{CTC} = -\\log P(y|x) = -\\log \\sum_{\\pi \\in \\mathcal{B}^{-1}(y)} P(\\pi|x)$$\n",
    "\n",
    "其中 $\\mathcal{B}^{-1}(y)$ 是所有折叠到y的对齐\n",
    "\n",
    "### 解码：\n",
    "1. **贪婪**：每帧选择最佳字符，然后折叠\n",
    "2. **束搜索**：保留top-k假设\n",
    "3. **前缀束搜索**：更适合CTC（生产环境使用）\n",
    "\n",
    "### Deep Speech 2架构：\n",
    "```\n",
    "音频 → 特征（MFCC/频谱图）\n",
    "  ↓\n",
    "卷积层（捕获局部模式）\n",
    "  ↓\n",
    "RNN层（双向GRU/LSTM）\n",
    "  ↓\n",
    "全连接层\n",
    "  ↓\n",
    "Softmax（字符概率）\n",
    "  ↓\n",
    "CTC损失\n",
    "```\n",
    "\n",
    "### 优势：\n",
    "- ✅ 无需对齐\n",
    "- ✅ 端到端可训练\n",
    "- ✅ 处理可变长度\n",
    "- ✅ 适用于任何序列任务\n",
    "\n",
    "### 局限：\n",
    "- ❌ 独立性假设（每帧独立）\n",
    "- ❌ 不能很好地建模输出依赖\n",
    "- ❌ 只支持单调对齐\n",
    "\n",
    "### 现代替代方案：\n",
    "- **基于注意力**：带注意力的Seq2seq（Listen, Attend, Spell）\n",
    "- **Transducer**：RNN-T结合了CTC+注意力\n",
    "- **Transformers**：Wav2Vec 2.0、Whisper\n",
    "\n",
    "### 应用：\n",
    "- 语音识别\n",
    "- 手写识别\n",
    "- OCR\n",
    "- 关键词检测\n",
    "- 任何有未知对齐的任务！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
