{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 24: Machine Super Intelligence\n",
    "\n",
    "**Shane Legg's PhD Thesis (2008): Universal Artificial Intelligence**\n",
    "\n",
    "A practical exploration of universal intelligence, AIXI agents, and Solomonoff induction through computable approximations.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements key concepts from Shane Legg's foundational work on universal artificial intelligence:\n",
    "\n",
    "- **Universal Intelligence**: Formal mathematical definition of intelligence\n",
    "- **AIXI Agent**: Optimal reinforcement learning agent using Solomonoff induction\n",
    "- **Solomonoff Induction**: Universal prior for sequence prediction\n",
    "- **Kolmogorov Complexity**: Measuring information content\n",
    "- **Monte Carlo AIXI**: Practical approximation using sampling\n",
    "- **Intelligence Measures**: Quantifying agent performance across environments\n",
    "\n",
    "Since exact AIXI is incomputable, we focus on **practical approximations** using toy environments.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. **Theories of Intelligence** - Psychometric models and g-factor\n",
    "2. **Universal AI & Solomonoff Induction** - Sequence prediction and compression\n",
    "3. **AIXI Agent & Environment Model** - MC-AIXI in toy MDPs\n",
    "4. **Universal Intelligence Measure** - Υ(π) for various agents\n",
    "5. **Approximations & Computational Limits** - Time-bounded AIXI\n",
    "6. **Pathways to Superintelligence** - Recursive self-improvement and intelligence explosion\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This notebook uses NumPy-only implementations with synthetic environments for educational purposes. Runtime is kept under 5 minutes.\n",
    "\n",
    "**Connections**:\n",
    "- Paper 23 (MDL): Minimum description length for model selection\n",
    "- Paper 25 (Kolmogorov Complexity): Information-theoretic foundations\n",
    "- Paper 8 (DQN): Practical deep RL vs theoretical optimal agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import itertools\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Imports complete!\")\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Theories of Intelligence\n",
    "\n",
    "## 1.1 Psychometric Intelligence\n",
    "\n",
    "Human intelligence research uses **psychometric models** to measure cognitive abilities. The **g-factor** (general intelligence) emerges from correlations between different cognitive tests.\n",
    "\n",
    "### Spearman's g-factor\n",
    "\n",
    "When people take multiple cognitive tests (verbal, spatial, memory, reasoning), performance across tests is positively correlated. This suggests a single underlying factor: **general intelligence (g)**.\n",
    "\n",
    "**Model**:\n",
    "- Test score = g × loading + specific ability + noise\n",
    "- High g → better performance across all domains\n",
    "\n",
    "We simulate this with synthetic test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cognitive_test_data(n_subjects=200, n_tests=8, g_variance=0.7):\n",
    "    \"\"\"\n",
    "    Generate synthetic cognitive test scores with g-factor structure.\n",
    "    \n",
    "    Args:\n",
    "        n_subjects: Number of test subjects\n",
    "        n_tests: Number of different cognitive tests\n",
    "        g_variance: Proportion of variance explained by g-factor\n",
    "        \n",
    "    Returns:\n",
    "        scores: (n_subjects, n_tests) test scores\n",
    "        g_factor: (n_subjects,) underlying general intelligence\n",
    "    \"\"\"\n",
    "    # Generate underlying g-factor (general intelligence) for each subject\n",
    "    g_factor = np.random.randn(n_subjects)\n",
    "    \n",
    "    # Test loadings: how much each test depends on g\n",
    "    # Higher loading = more g-dependent\n",
    "    loadings = np.random.uniform(0.5, 0.9, n_tests)\n",
    "    \n",
    "    # Generate scores\n",
    "    scores = np.zeros((n_subjects, n_tests))\n",
    "    \n",
    "    for i in range(n_tests):\n",
    "        # Score = g-component + specific ability + noise\n",
    "        g_component = g_factor * loadings[i] * np.sqrt(g_variance)\n",
    "        specific = np.random.randn(n_subjects) * np.sqrt(1 - g_variance)\n",
    "        scores[:, i] = g_component + specific\n",
    "    \n",
    "    # Normalize to 0-100 scale\n",
    "    scores = 50 + 15 * scores  # Mean=50, SD=15 (like IQ)\n",
    "    scores = np.clip(scores, 0, 100)\n",
    "    \n",
    "    return scores, g_factor, loadings\n",
    "\n",
    "# Generate test data\n",
    "test_names = ['Verbal', 'Spatial', 'Memory', 'Reasoning', 'Processing', 'Attention', 'Math', 'Pattern']\n",
    "scores, g_factor, loadings = generate_cognitive_test_data(n_subjects=200, n_tests=len(test_names))\n",
    "\n",
    "print(\"Generated cognitive test data:\")\n",
    "print(f\"Subjects: {scores.shape[0]}\")\n",
    "print(f\"Tests: {scores.shape[1]}\")\n",
    "print(f\"\\nMean scores per test:\")\n",
    "for i, name in enumerate(test_names):\n",
    "    print(f\"  {name:12s}: {scores[:, i].mean():.1f} ± {scores[:, i].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix (shows positive manifold)\n",
    "correlation_matrix = np.corrcoef(scores.T)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.xticks(range(len(test_names)), test_names, rotation=45)\n",
    "plt.yticks(range(len(test_names)), test_names)\n",
    "plt.title('Correlation Matrix of Cognitive Tests\\n(Positive Manifold Shows g-factor)', fontsize=14)\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(test_names)):\n",
    "    for j in range(len(test_names)):\n",
    "        plt.text(j, i, f'{correlation_matrix[i, j]:.2f}', \n",
    "                ha='center', va='center', color='white' if abs(correlation_matrix[i, j]) > 0.5 else 'black',\n",
    "                fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation: All tests show POSITIVE correlations\")\n",
    "print(\"This 'positive manifold' suggests a common underlying factor (g).\")\n",
    "print(f\"Mean off-diagonal correlation: {np.mean(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract g-factor using Principal Component Analysis (PCA)\n",
    "def extract_g_factor(scores):\n",
    "    \"\"\"\n",
    "    Extract g-factor as first principal component.\n",
    "    \n",
    "    The first PC captures the maximum variance and represents\n",
    "    the common factor across all tests.\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    scores_centered = scores - scores.mean(axis=0)\n",
    "    \n",
    "    # Compute covariance matrix\n",
    "    cov_matrix = np.cov(scores_centered.T)\n",
    "    \n",
    "    # Eigendecomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    \n",
    "    # Sort by eigenvalue (descending)\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # First component is g-factor\n",
    "    g_extracted = scores_centered @ eigenvectors[:, 0]\n",
    "    \n",
    "    # Variance explained\n",
    "    variance_explained = eigenvalues / eigenvalues.sum()\n",
    "    \n",
    "    return g_extracted, variance_explained, eigenvectors[:, 0]\n",
    "\n",
    "g_extracted, var_explained, g_loadings = extract_g_factor(scores)\n",
    "\n",
    "# Visualize variance explained\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, len(var_explained) + 1), var_explained * 100, color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Variance Explained (%)')\n",
    "axes[0].set_title(f'Scree Plot\\nFirst PC (g-factor) explains {var_explained[0]*100:.1f}% of variance')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# g-factor loadings\n",
    "axes[1].barh(test_names, np.abs(g_loadings), color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('Absolute Loading on g-factor')\n",
    "axes[1].set_title('Test Loadings on g-factor\\n(How much each test measures g)')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\ng-factor (first PC) explains {var_explained[0]*100:.1f}% of total variance\")\n",
    "print(f\"Correlation between true g and extracted g: {np.corrcoef(g_factor, g_extracted)[0, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 From Psychometric to Universal Intelligence\n",
    "\n",
    "**Limitations of psychometric g**:\n",
    "- Human-centric (only measures human-like intelligence)\n",
    "- Test-dependent (results vary with test selection)\n",
    "- No formal mathematical definition\n",
    "\n",
    "**Legg & Hutter's Universal Intelligence** addresses these by defining intelligence formally:\n",
    "\n",
    "$$\\Upsilon(\\pi) = \\sum_{\\mu \\in E} 2^{-K(\\mu)} V_\\mu^\\pi$$\n",
    "\n",
    "Where:\n",
    "- $\\pi$ is the agent\n",
    "- $E$ is the set of all computable environments\n",
    "- $K(\\mu)$ is Kolmogorov complexity of environment $\\mu$\n",
    "- $V_\\mu^\\pi$ is expected reward in environment $\\mu$\n",
    "- $2^{-K(\\mu)}$ weights simpler environments more heavily (Solomonoff prior)\n",
    "\n",
    "This definition:\n",
    "- Is **universal** (applies to any agent, any environment)\n",
    "- Is **formal** (precise mathematical definition)\n",
    "- Aligns with **Occam's razor** (simpler environments weighted more)\n",
    "- Is **incomputable** (but can be approximated!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Universal AI & Solomonoff Induction\n",
    "\n",
    "## 2.1 Solomonoff Induction\n",
    "\n",
    "**Problem**: Given a sequence of observations, predict the next symbol.\n",
    "\n",
    "**Solomonoff's solution**: Consider ALL computable hypotheses, weighted by their **Kolmogorov complexity** (length of shortest program that generates them).\n",
    "\n",
    "$$P(x_{1:n}) = \\sum_{p: U(p) = x_{1:n}} 2^{-|p|}$$\n",
    "\n",
    "Where:\n",
    "- $U$ is a universal Turing machine\n",
    "- $p$ is a program\n",
    "- $|p|$ is program length\n",
    "- Shorter programs have higher prior probability\n",
    "\n",
    "**Key properties**:\n",
    "1. Universal: Dominates any computable predictor asymptotically\n",
    "2. Optimal: Converges to true distribution faster than any other method\n",
    "3. Incomputable: No algorithm can compute exact Solomonoff probabilities\n",
    "\n",
    "We approximate this using **simple program enumeration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProgramEnumerator:\n",
    "    \"\"\"\n",
    "    Toy approximation of Solomonoff induction using simple program enumeration.\n",
    "    \n",
    "    We enumerate short programs (finite state machines) and weight them\n",
    "    by 2^(-length) to approximate the Solomonoff prior.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alphabet_size=2, max_program_length=8):\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.max_length = max_program_length\n",
    "        self.programs = []  # List of (program, weight) tuples\n",
    "        \n",
    "    def enumerate_programs(self):\n",
    "        \"\"\"\n",
    "        Enumerate simple programs as repeating patterns.\n",
    "        \n",
    "        Programs are represented as short sequences that repeat.\n",
    "        E.g., [0, 1] represents 010101...\n",
    "        \"\"\"\n",
    "        programs = []\n",
    "        \n",
    "        # Enumerate all sequences up to max_length\n",
    "        for length in range(1, self.max_length + 1):\n",
    "            for pattern in itertools.product(range(self.alphabet_size), repeat=length):\n",
    "                program = list(pattern)\n",
    "                weight = 2.0 ** (-length)  # Solomonoff prior\n",
    "                programs.append((program, weight))\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = sum(w for _, w in programs)\n",
    "        programs = [(p, w / total_weight) for p, w in programs]\n",
    "        \n",
    "        self.programs = programs\n",
    "        return len(programs)\n",
    "    \n",
    "    def generate_sequence(self, program, length):\n",
    "        \"\"\"\n",
    "        Generate sequence by repeating program pattern.\n",
    "        \"\"\"\n",
    "        seq = []\n",
    "        for i in range(length):\n",
    "            seq.append(program[i % len(program)])\n",
    "        return np.array(seq)\n",
    "    \n",
    "    def predict_next(self, observed_sequence):\n",
    "        \"\"\"\n",
    "        Predict next symbol using Solomonoff-style weighted voting.\n",
    "        \n",
    "        For each program:\n",
    "        1. Check if it's consistent with observed sequence\n",
    "        2. If yes, see what it predicts next\n",
    "        3. Weight prediction by program's prior probability\n",
    "        \"\"\"\n",
    "        n = len(observed_sequence)\n",
    "        \n",
    "        # Accumulate weighted predictions\n",
    "        predictions = np.zeros(self.alphabet_size)\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        for program, weight in self.programs:\n",
    "            # Generate what this program would produce\n",
    "            generated = self.generate_sequence(program, n + 1)\n",
    "            \n",
    "            # Check if consistent with observations\n",
    "            if np.array_equal(generated[:n], observed_sequence):\n",
    "                next_symbol = generated[n]\n",
    "                predictions[next_symbol] += weight\n",
    "                total_weight += weight\n",
    "        \n",
    "        if total_weight > 0:\n",
    "            predictions /= total_weight\n",
    "        else:\n",
    "            # Uniform if no consistent programs\n",
    "            predictions = np.ones(self.alphabet_size) / self.alphabet_size\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Create predictor\n",
    "predictor = SimpleProgramEnumerator(alphabet_size=2, max_program_length=6)\n",
    "n_programs = predictor.enumerate_programs()\n",
    "\n",
    "print(f\"Enumerated {n_programs} programs\")\n",
    "print(f\"\\nExample programs (pattern, weight):\")\n",
    "for i in range(min(10, len(predictor.programs))):\n",
    "    program, weight = predictor.programs[i]\n",
    "    print(f\"  {program} -> weight = {weight:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different sequences\n",
    "test_sequences = [\n",
    "    ([0, 1, 0, 1, 0, 1], \"Alternating\"),\n",
    "    ([1, 1, 1, 1, 1, 1], \"Constant\"),\n",
    "    ([0, 1, 1, 0, 1, 1], \"Pattern 011\"),\n",
    "    ([0, 0, 1, 0, 0, 1], \"Pattern 001\"),\n",
    "]\n",
    "\n",
    "print(\"Solomonoff Predictions:\\n\")\n",
    "for seq, description in test_sequences:\n",
    "    seq_array = np.array(seq)\n",
    "    pred = predictor.predict_next(seq_array)\n",
    "    \n",
    "    print(f\"{description:15s}: {seq}\")\n",
    "    print(f\"  P(next=0) = {pred[0]:.4f}\")\n",
    "    print(f\"  P(next=1) = {pred[1]:.4f}\")\n",
    "    print(f\"  Prediction: {np.argmax(pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize program length distribution\n",
    "program_lengths = [len(p) for p, _ in predictor.programs]\n",
    "weights_by_length = defaultdict(float)\n",
    "\n",
    "for program, weight in predictor.programs:\n",
    "    weights_by_length[len(program)] += weight\n",
    "\n",
    "lengths = sorted(weights_by_length.keys())\n",
    "total_weights = [weights_by_length[l] for l in lengths]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total weight by program length\n",
    "axes[0].bar(lengths, total_weights, color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Program Length (bits)')\n",
    "axes[0].set_ylabel('Total Prior Probability')\n",
    "axes[0].set_title('Solomonoff Prior Distribution\\n(Shorter programs weighted more)')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Number of programs by length\n",
    "length_counts = [sum(1 for l in program_lengths if l == length) for length in lengths]\n",
    "axes[1].bar(lengths, length_counts, color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('Program Length (bits)')\n",
    "axes[1].set_ylabel('Number of Programs')\n",
    "axes[1].set_title('Program Count by Length\\n(Exponential growth in hypotheses)')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Solomonoff prior favors simplicity (Occam's Razor)\")\n",
    "print(f\"Programs of length 1 have total weight: {weights_by_length[1]:.4f}\")\n",
    "print(f\"Programs of length 6 have total weight: {weights_by_length[6]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sequence Prediction Performance\n",
    "\n",
    "We test how well Solomonoff induction learns different patterns compared to simpler baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sequence_prediction(true_pattern, seq_length=30, description=\"\"):\n",
    "    \"\"\"\n",
    "    Test prediction accuracy over time as we observe more of the sequence.\n",
    "    \n",
    "    Compares:\n",
    "    - Solomonoff approximation\n",
    "    - Frequency baseline (predict most common symbol so far)\n",
    "    - Random baseline\n",
    "    \"\"\"\n",
    "    # Generate true sequence\n",
    "    true_seq = []\n",
    "    for i in range(seq_length):\n",
    "        true_seq.append(true_pattern[i % len(true_pattern)])\n",
    "    true_seq = np.array(true_seq)\n",
    "    \n",
    "    solomonoff_correct = []\n",
    "    frequency_correct = []\n",
    "    \n",
    "    # Start predicting after seeing a few symbols\n",
    "    for t in range(3, seq_length):\n",
    "        observed = true_seq[:t]\n",
    "        true_next = true_seq[t]\n",
    "        \n",
    "        # Solomonoff prediction\n",
    "        sol_pred = predictor.predict_next(observed)\n",
    "        sol_correct = (np.argmax(sol_pred) == true_next)\n",
    "        solomonoff_correct.append(sol_correct)\n",
    "        \n",
    "        # Frequency baseline: predict most common symbol\n",
    "        freq_pred = 1 if np.sum(observed) > len(observed) / 2 else 0\n",
    "        freq_correct = (freq_pred == true_next)\n",
    "        frequency_correct.append(freq_correct)\n",
    "    \n",
    "    return solomonoff_correct, frequency_correct\n",
    "\n",
    "# Test on different patterns\n",
    "patterns = [\n",
    "    ([0, 1], \"Alternating\"),\n",
    "    ([0, 0, 1], \"Pattern 001\"),\n",
    "    ([1, 0, 1, 1], \"Pattern 1011\"),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(patterns), figsize=(16, 4))\n",
    "\n",
    "for idx, (pattern, desc) in enumerate(patterns):\n",
    "    sol_acc, freq_acc = test_sequence_prediction(pattern, seq_length=30, description=desc)\n",
    "    \n",
    "    # Compute cumulative accuracy\n",
    "    sol_cumavg = np.cumsum(sol_acc) / np.arange(1, len(sol_acc) + 1)\n",
    "    freq_cumavg = np.cumsum(freq_acc) / np.arange(1, len(freq_acc) + 1)\n",
    "    \n",
    "    axes[idx].plot(sol_cumavg, label='Solomonoff', linewidth=2, color='steelblue')\n",
    "    axes[idx].plot(freq_cumavg, label='Frequency', linewidth=2, color='coral', linestyle='--')\n",
    "    axes[idx].axhline(0.5, color='gray', linestyle=':', label='Random')\n",
    "    axes[idx].set_xlabel('Observations')\n",
    "    axes[idx].set_ylabel('Cumulative Accuracy')\n",
    "    axes[idx].set_title(f'{desc}\\nPattern: {\"\".join(map(str, pattern))}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    axes[idx].set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Solomonoff induction quickly identifies simple patterns!\")\n",
    "print(\"It outperforms frequency baseline by considering program simplicity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Kolmogorov Complexity Approximation\n",
    "\n",
    "**Kolmogorov complexity** $K(x)$ is the length of the shortest program that outputs $x$.\n",
    "\n",
    "While incomputable in general, we can approximate it using:\n",
    "1. **Compression**: $K(x) \\approx$ compressed length\n",
    "2. **Program search**: Find shortest program that generates $x$\n",
    "\n",
    "We use approach #2 with our enumerated programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_kolmogorov_complexity(sequence):\n",
    "    \"\"\"\n",
    "    Estimate K(sequence) by finding shortest program that generates it.\n",
    "    \"\"\"\n",
    "    n = len(sequence)\n",
    "    min_length = float('inf')\n",
    "    best_program = None\n",
    "    \n",
    "    for program, weight in predictor.programs:\n",
    "        generated = predictor.generate_sequence(program, n)\n",
    "        if np.array_equal(generated, sequence):\n",
    "            if len(program) < min_length:\n",
    "                min_length = len(program)\n",
    "                best_program = program\n",
    "    \n",
    "    return min_length, best_program\n",
    "\n",
    "# Test sequences with different complexities\n",
    "test_sequences_k = [\n",
    "    (np.array([0, 0, 0, 0, 0, 0]), \"All zeros\"),\n",
    "    (np.array([0, 1, 0, 1, 0, 1]), \"Alternating\"),\n",
    "    (np.array([0, 0, 1, 0, 0, 1]), \"Pattern 001\"),\n",
    "    (np.array([0, 1, 1, 0, 1, 1]), \"Pattern 011\"),\n",
    "    (np.array([1, 0, 0, 1, 1, 0]), \"No simple pattern\"),\n",
    "]\n",
    "\n",
    "print(\"Kolmogorov Complexity Estimates:\\n\")\n",
    "complexities = []\n",
    "labels = []\n",
    "\n",
    "for seq, desc in test_sequences_k:\n",
    "    k_est, program = estimate_kolmogorov_complexity(seq)\n",
    "    complexities.append(k_est)\n",
    "    labels.append(desc)\n",
    "    \n",
    "    print(f\"{desc:20s}: K ≈ {k_est} bits\")\n",
    "    if program is not None:\n",
    "        print(f\"  Shortest program: {\"\".join(map(str, program))}\")\n",
    "        print(f\"  Sequence: {\"\".join(map(str, seq.tolist()))}\\n\")\n",
    "    else:\n",
    "        print(f\"  No program found (complexity > {predictor.max_length})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize complexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green', 'blue', 'orange', 'orange', 'red']\n",
    "bars = plt.barh(labels, complexities, color=colors, alpha=0.7)\n",
    "plt.xlabel('Estimated Kolmogorov Complexity (bits)')\n",
    "plt.title('Sequence Complexity Estimates\\n(Shorter = simpler)', fontsize=14)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add values\n",
    "for i, (complexity, label) in enumerate(zip(complexities, labels)):\n",
    "    plt.text(complexity + 0.1, i, f'{complexity}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSimpler patterns have lower Kolmogorov complexity!\")\n",
    "print(\"This formalizes Occam's Razor: prefer simpler explanations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: AIXI Agent & Environment Model\n",
    "\n",
    "## 3.1 The AIXI Agent\n",
    "\n",
    "**AIXI** is the theoretically optimal reinforcement learning agent (Hutter, 2005).\n",
    "\n",
    "**Action selection**:\n",
    "\n",
    "$$a_t^* = \\arg\\max_{a_t} \\sum_{o_t, r_t} \\max_{a_{t+1}} \\sum_{o_{t+1}, r_{t+1}} \\cdots \\max_{a_m} \\sum_{o_m, r_m} [r_t + \\cdots + r_m] \\cdot P(o_t r_t \\cdots o_m r_m | a_t \\cdots a_m)$$\n",
    "\n",
    "Where:\n",
    "$$P(\\text{observations} | \\text{actions}) = \\sum_{\\mu \\in E} 2^{-K(\\mu)} P_\\mu(\\text{observations} | \\text{actions})$$\n",
    "\n",
    "AIXI:\n",
    "1. Considers all possible environments weighted by Kolmogorov complexity\n",
    "2. Plans optimally using minimax search over action sequences\n",
    "3. Is incomputable (requires infinite computation)\n",
    "\n",
    "**Monte Carlo AIXI (MC-AIXI)** approximates this using:\n",
    "- Sampling a small set of environment hypotheses\n",
    "- Monte Carlo tree search for planning\n",
    "- Finite lookahead horizon\n",
    "\n",
    "We implement a simplified MC-AIXI in toy grid-world environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyGridWorld:\n",
    "    \"\"\"\n",
    "    Simple 5x5 grid world environment.\n",
    "    \n",
    "    Agent starts at (0, 0) and must reach goal at (4, 4).\n",
    "    Actions: up, down, left, right\n",
    "    Rewards: +10 for reaching goal, -1 per step\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [self.size - 1, self.size - 1]\n",
    "        self.done = False\n",
    "        self.total_reward = 0\n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):\n",
    "        \"\"\"Return agent position as observation.\"\"\"\n",
    "        return tuple(self.agent_pos)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute action.\n",
    "        Actions: 0=up, 1=down, 2=left, 3=right\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            return self.get_observation(), 0, True\n",
    "        \n",
    "        # Execute action\n",
    "        if action == 0:  # up\n",
    "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
    "        elif action == 1:  # down\n",
    "            self.agent_pos[0] = min(self.size - 1, self.agent_pos[0] + 1)\n",
    "        elif action == 2:  # left\n",
    "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
    "        elif action == 3:  # right\n",
    "            self.agent_pos[1] = min(self.size - 1, self.agent_pos[1] + 1)\n",
    "        \n",
    "        # Check if goal reached\n",
    "        reward = -1  # Step penalty\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 10\n",
    "            self.done = True\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        return self.get_observation(), reward, self.done\n",
    "    \n",
    "    def copy(self):\n",
    "        \"\"\"Create a copy for simulation.\"\"\"\n",
    "        new_env = ToyGridWorld(self.size)\n",
    "        new_env.agent_pos = self.agent_pos.copy()\n",
    "        new_env.done = self.done\n",
    "        new_env.total_reward = self.total_reward\n",
    "        return new_env\n",
    "\n",
    "# Test environment\n",
    "env = ToyGridWorld(size=5)\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"Toy Grid World Environment\")\n",
    "print(f\"Size: {env.size}x{env.size}\")\n",
    "print(f\"Start: {env.agent_pos}\")\n",
    "print(f\"Goal: {env.goal_pos}\")\n",
    "print(f\"Actions: 0=up, 1=down, 2=left, 3=right\")\n",
    "print(f\"\\nInitial observation: {obs}\")\n",
    "\n",
    "# Try random actions\n",
    "print(\"\\nRandom episode:\")\n",
    "total_reward = 0\n",
    "for step in range(20):\n",
    "    action = np.random.randint(0, 4)\n",
    "    obs, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"  Step {step}: action={action}, pos={obs}, reward={reward:.1f}\")\n",
    "    if done:\n",
    "        print(f\"  Goal reached! Total reward: {total_reward}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTreeNode:\n",
    "    \"\"\"Node in Monte Carlo tree search.\"\"\"\n",
    "    \n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "    \n",
    "    def is_fully_expanded(self, n_actions):\n",
    "        return len(self.children) == n_actions\n",
    "    \n",
    "    def best_child(self, exploration_weight=1.0):\n",
    "        \"\"\"Select child using UCB1.\"\"\"\n",
    "        choices_weights = []\n",
    "        for child in self.children:\n",
    "            if child.visits == 0:\n",
    "                weight = float('inf')\n",
    "            else:\n",
    "                exploit = child.value / child.visits\n",
    "                explore = exploration_weight * np.sqrt(np.log(self.visits) / child.visits)\n",
    "                weight = exploit + explore\n",
    "            choices_weights.append(weight)\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "class SimpleMCAIXI:\n",
    "    \"\"\"\n",
    "    Simplified Monte Carlo AIXI agent.\n",
    "    \n",
    "    Uses Monte Carlo Tree Search (MCTS) to plan actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, n_simulations=100, horizon=5):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_simulations = n_simulations\n",
    "        self.horizon = horizon\n",
    "    \n",
    "    def select_action(self, env):\n",
    "        \"\"\"\n",
    "        Select action using MCTS.\n",
    "        \n",
    "        1. Selection: traverse tree using UCB1\n",
    "        2. Expansion: add new child node\n",
    "        3. Simulation: rollout random policy\n",
    "        4. Backpropagation: update values\n",
    "        \"\"\"\n",
    "        root = MCTreeNode(env.copy())\n",
    "        \n",
    "        for _ in range(self.n_simulations):\n",
    "            node = root\n",
    "            sim_env = env.copy()\n",
    "            \n",
    "            # Selection\n",
    "            while node.is_fully_expanded(self.n_actions) and len(node.children) > 0:\n",
    "                node = node.best_child()\n",
    "                sim_env.step(node.action)\n",
    "            \n",
    "            # Expansion\n",
    "            if not node.is_fully_expanded(self.n_actions) and not sim_env.done:\n",
    "                action = len(node.children)  # Try next untried action\n",
    "                new_env = sim_env.copy()\n",
    "                new_env.step(action)\n",
    "                child = MCTreeNode(new_env, parent=node, action=action)\n",
    "                node.children.append(child)\n",
    "                node = child\n",
    "                sim_env = new_env\n",
    "            \n",
    "            # Simulation (rollout)\n",
    "            rollout_reward = 0\n",
    "            for _ in range(self.horizon):\n",
    "                if sim_env.done:\n",
    "                    break\n",
    "                action = np.random.randint(0, self.n_actions)\n",
    "                _, reward, _ = sim_env.step(action)\n",
    "                rollout_reward += reward\n",
    "            \n",
    "            # Backpropagation\n",
    "            while node is not None:\n",
    "                node.visits += 1\n",
    "                node.value += rollout_reward\n",
    "                node = node.parent\n",
    "        \n",
    "        # Select best action\n",
    "        if len(root.children) == 0:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        \n",
    "        best_child = max(root.children, key=lambda c: c.visits)\n",
    "        return best_child.action\n",
    "\n",
    "# Test MC-AIXI agent\n",
    "print(\"Testing MC-AIXI agent...\\n\")\n",
    "\n",
    "agent = SimpleMCAIXI(n_actions=4, n_simulations=50, horizon=10)\n",
    "env = ToyGridWorld(size=5)\n",
    "obs = env.reset()\n",
    "\n",
    "episode_reward = 0\n",
    "for step in range(20):\n",
    "    action = agent.select_action(env)\n",
    "    obs, reward, done = env.step(action)\n",
    "    episode_reward += reward\n",
    "    \n",
    "    action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    print(f\"Step {step}: {action_names[action]:5s} -> pos={obs}, reward={reward:+.1f}\")\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\nGoal reached in {step + 1} steps!\")\n",
    "        print(f\"Total reward: {episode_reward}\")\n",
    "        break\n",
    "\n",
    "if not done:\n",
    "    print(f\"\\nDid not reach goal. Total reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Comparing Agents\n",
    "\n",
    "Let's compare different agents:\n",
    "1. **Random**: Selects actions uniformly\n",
    "2. **Greedy**: Moves toward goal (Manhattan distance)\n",
    "3. **MC-AIXI**: Uses MCTS planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(env):\n",
    "    \"\"\"Random action selection.\"\"\"\n",
    "    return np.random.randint(0, 4)\n",
    "\n",
    "def greedy_agent(env):\n",
    "    \"\"\"\n",
    "    Greedy agent: move toward goal.\n",
    "    \n",
    "    Computes Manhattan distance and chooses action that reduces it.\n",
    "    \"\"\"\n",
    "    agent_pos = env.agent_pos\n",
    "    goal_pos = env.goal_pos\n",
    "    \n",
    "    # Vertical movement\n",
    "    if agent_pos[0] < goal_pos[0]:\n",
    "        return 1  # down\n",
    "    elif agent_pos[0] > goal_pos[0]:\n",
    "        return 0  # up\n",
    "    # Horizontal movement\n",
    "    elif agent_pos[1] < goal_pos[1]:\n",
    "        return 3  # right\n",
    "    elif agent_pos[1] > goal_pos[1]:\n",
    "        return 2  # left\n",
    "    else:\n",
    "        return np.random.randint(0, 4)\n",
    "\n",
    "def evaluate_agent(agent_fn, n_episodes=20, max_steps=30):\n",
    "    \"\"\"\n",
    "    Evaluate agent over multiple episodes.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    steps_to_goal = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        env = ToyGridWorld(size=5)\n",
    "        env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        for step in range(max_steps):\n",
    "            action = agent_fn(env)\n",
    "            obs, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                steps_to_goal.append(step + 1)\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return total_rewards, steps_to_goal\n",
    "\n",
    "# Evaluate agents\n",
    "print(\"Evaluating agents over 20 episodes...\\n\")\n",
    "\n",
    "mc_aixi_agent = SimpleMCAIXI(n_actions=4, n_simulations=30, horizon=10)\n",
    "\n",
    "random_rewards, random_steps = evaluate_agent(random_agent, n_episodes=20)\n",
    "greedy_rewards, greedy_steps = evaluate_agent(greedy_agent, n_episodes=20)\n",
    "aixi_rewards, aixi_steps = evaluate_agent(lambda env: mc_aixi_agent.select_action(env), n_episodes=20)\n",
    "\n",
    "# Results\n",
    "print(\"Results (mean ± std):\")\n",
    "print(f\"\\nRandom Agent:\")\n",
    "print(f\"  Reward: {np.mean(random_rewards):.1f} ± {np.std(random_rewards):.1f}\")\n",
    "print(f\"  Success rate: {len(random_steps)/20*100:.0f}%\")\n",
    "if len(random_steps) > 0:\n",
    "    print(f\"  Steps to goal: {np.mean(random_steps):.1f} ± {np.std(random_steps):.1f}\")\n",
    "\n",
    "print(f\"\\nGreedy Agent:\")\n",
    "print(f\"  Reward: {np.mean(greedy_rewards):.1f} ± {np.std(greedy_rewards):.1f}\")\n",
    "print(f\"  Success rate: {len(greedy_steps)/20*100:.0f}%\")\n",
    "if len(greedy_steps) > 0:\n",
    "    print(f\"  Steps to goal: {np.mean(greedy_steps):.1f} ± {np.std(greedy_steps):.1f}\")\n",
    "\n",
    "print(f\"\\nMC-AIXI Agent:\")\n",
    "print(f\"  Reward: {np.mean(aixi_rewards):.1f} ± {np.std(aixi_rewards):.1f}\")\n",
    "print(f\"  Success rate: {len(aixi_steps)/20*100:.0f}%\")\n",
    "if len(aixi_steps) > 0:\n",
    "    print(f\"  Steps to goal: {np.mean(aixi_steps):.1f} ± {np.std(aixi_steps):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Reward comparison\n",
    "agent_names = ['Random', 'Greedy', 'MC-AIXI']\n",
    "mean_rewards = [np.mean(random_rewards), np.mean(greedy_rewards), np.mean(aixi_rewards)]\n",
    "std_rewards = [np.std(random_rewards), np.std(greedy_rewards), np.std(aixi_rewards)]\n",
    "\n",
    "axes[0].bar(agent_names, mean_rewards, yerr=std_rewards, capsize=5, \n",
    "           color=['gray', 'steelblue', 'coral'], alpha=0.7)\n",
    "axes[0].set_ylabel('Total Reward')\n",
    "axes[0].set_title('Agent Performance\\n(Higher is better)')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Steps to goal comparison\n",
    "steps_data = [random_steps, greedy_steps, aixi_steps]\n",
    "axes[1].boxplot(steps_data, labels=agent_names)\n",
    "axes[1].set_ylabel('Steps to Goal')\n",
    "axes[1].set_title('Efficiency\\n(Lower is better)')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMC-AIXI combines planning and lookahead to achieve better performance!\")\n",
    "print(\"Greedy agent is fast but suboptimal in complex environments.\")\n",
    "print(\"Random agent shows the baseline for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Universal Intelligence Measure\n",
    "\n",
    "## 4.1 Formal Definition\n",
    "\n",
    "**Legg & Hutter's Universal Intelligence Measure**:\n",
    "\n",
    "$$\\Upsilon(\\pi) = \\sum_{\\mu \\in E} 2^{-K(\\mu)} V_\\mu^\\pi$$\n",
    "\n",
    "Where:\n",
    "- $\\pi$ is the agent policy\n",
    "- $E$ is the space of all computable environments\n",
    "- $K(\\mu)$ is Kolmogorov complexity of environment $\\mu$\n",
    "- $V_\\mu^\\pi$ is expected discounted reward in environment $\\mu$:\n",
    "\n",
    "$$V_\\mu^\\pi = \\mathbb{E}_{\\pi, \\mu}\\left[\\sum_{t=1}^\\infty \\gamma^t r_t\\right]$$\n",
    "\n",
    "**Key properties**:\n",
    "1. **Universal**: Considers all possible computable environments\n",
    "2. **Weighted by simplicity**: Simpler environments matter more (Solomonoff prior)\n",
    "3. **Performance-based**: Measured by reward achievement\n",
    "4. **Incomputable**: But can be approximated with finite environment sets\n",
    "\n",
    "We approximate $\\Upsilon(\\pi)$ using a small suite of toy environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentSuite:\n",
    "    \"\"\"\n",
    "    Collection of toy environments with estimated Kolmogorov complexities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.environments = self.create_environments()\n",
    "    \n",
    "    def create_environments(self):\n",
    "        \"\"\"\n",
    "        Create environments with varying complexities.\n",
    "        \n",
    "        Each environment returns:\n",
    "        - env_fn: Function that creates environment instance\n",
    "        - complexity: Estimated K(μ) in bits\n",
    "        - description: Human-readable description\n",
    "        \"\"\"\n",
    "        envs = []\n",
    "        \n",
    "        # Env 1: Constant reward (simplest)\n",
    "        def constant_reward_env():\n",
    "            class ConstantEnv:\n",
    "                def __init__(self):\n",
    "                    self.done = False\n",
    "                def reset(self):\n",
    "                    self.done = False\n",
    "                    return 0\n",
    "                def step(self, action):\n",
    "                    return 0, 1.0, False  # Always reward +1\n",
    "                def copy(self):\n",
    "                    new = ConstantEnv()\n",
    "                    new.done = self.done\n",
    "                    return new\n",
    "            return ConstantEnv()\n",
    "        \n",
    "        envs.append({\n",
    "            'env_fn': constant_reward_env,\n",
    "            'complexity': 2,  # Very simple: always return 1\n",
    "            'description': 'Constant reward (+1)'\n",
    "        })\n",
    "        \n",
    "        # Env 2: Binary choice (left=0, right=1)\n",
    "        def binary_choice_env():\n",
    "            class BinaryEnv:\n",
    "                def __init__(self):\n",
    "                    self.done = False\n",
    "                def reset(self):\n",
    "                    self.done = False\n",
    "                    return 0\n",
    "                def step(self, action):\n",
    "                    # Action 3 (right) gives +1, others give 0\n",
    "                    reward = 1.0 if action == 3 else 0.0\n",
    "                    return 0, reward, False\n",
    "                def copy(self):\n",
    "                    return BinaryEnv()\n",
    "            return BinaryEnv()\n",
    "        \n",
    "        envs.append({\n",
    "            'env_fn': binary_choice_env,\n",
    "            'complexity': 4,  # Need to encode: if action==3 then 1 else 0\n",
    "            'description': 'Binary choice (right=+1)'\n",
    "        })\n",
    "        \n",
    "        # Env 3: Grid world (more complex)\n",
    "        def grid_world_env():\n",
    "            return ToyGridWorld(size=5)\n",
    "        \n",
    "        envs.append({\n",
    "            'env_fn': grid_world_env,\n",
    "            'complexity': 8,  # Need to encode: grid, movement dynamics, goal\n",
    "            'description': 'Grid world navigation'\n",
    "        })\n",
    "        \n",
    "        return envs\n",
    "    \n",
    "    def compute_universal_intelligence(self, agent_fn, n_episodes=10, max_steps=20, gamma=0.95):\n",
    "        \"\"\"\n",
    "        Compute Υ(π) approximation.\n",
    "        \n",
    "        Υ(π) ≈ Σ_μ 2^(-K(μ)) * V_μ^π\n",
    "        \"\"\"\n",
    "        intelligence = 0.0\n",
    "        environment_values = []\n",
    "        \n",
    "        for env_spec in self.environments:\n",
    "            # Get environment parameters\n",
    "            env_fn = env_spec['env_fn']\n",
    "            complexity = env_spec['complexity']\n",
    "            \n",
    "            # Compute environment weight (Solomonoff prior)\n",
    "            weight = 2.0 ** (-complexity)\n",
    "            \n",
    "            # Estimate V_μ^π (expected discounted return)\n",
    "            returns = []\n",
    "            for _ in range(n_episodes):\n",
    "                env = env_fn()\n",
    "                env.reset()\n",
    "                \n",
    "                discounted_return = 0.0\n",
    "                discount = 1.0\n",
    "                \n",
    "                for step in range(max_steps):\n",
    "                    action = agent_fn(env)\n",
    "                    _, reward, done = env.step(action)\n",
    "                    discounted_return += discount * reward\n",
    "                    discount *= gamma\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                returns.append(discounted_return)\n",
    "            \n",
    "            value = np.mean(returns)\n",
    "            environment_values.append({\n",
    "                'description': env_spec['description'],\n",
    "                'complexity': complexity,\n",
    "                'weight': weight,\n",
    "                'value': value,\n",
    "                'weighted_value': weight * value\n",
    "            })\n",
    "            \n",
    "            # Add to intelligence measure\n",
    "            intelligence += weight * value\n",
    "        \n",
    "        # Normalize by total weight\n",
    "        total_weight = sum(2.0 ** (-env['complexity']) for env in self.environments)\n",
    "        intelligence /= total_weight\n",
    "        \n",
    "        return intelligence, environment_values\n",
    "\n",
    "# Create environment suite\n",
    "suite = EnvironmentSuite()\n",
    "\n",
    "print(\"Environment Suite:\")\n",
    "print()\n",
    "for i, env_spec in enumerate(suite.environments, 1):\n",
    "    weight = 2.0 ** (-env_spec['complexity'])\n",
    "    print(f\"{i}. {env_spec['description']}\")\n",
    "    print(f\"   K(μ) ≈ {env_spec['complexity']} bits\")\n",
    "    print(f\"   Weight = 2^(-{env_spec['complexity']}) = {weight:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute intelligence for different agents\n",
    "print(\"Computing Universal Intelligence Υ(π) for each agent...\\n\")\n",
    "\n",
    "mc_aixi_light = SimpleMCAIXI(n_actions=4, n_simulations=20, horizon=8)\n",
    "\n",
    "# Random agent\n",
    "upsilon_random, env_values_random = suite.compute_universal_intelligence(\n",
    "    random_agent, n_episodes=10, max_steps=15\n",
    ")\n",
    "\n",
    "# Greedy agent\n",
    "upsilon_greedy, env_values_greedy = suite.compute_universal_intelligence(\n",
    "    greedy_agent, n_episodes=10, max_steps=15\n",
    ")\n",
    "\n",
    "# MC-AIXI agent\n",
    "upsilon_aixi, env_values_aixi = suite.compute_universal_intelligence(\n",
    "    lambda env: mc_aixi_light.select_action(env), n_episodes=10, max_steps=15\n",
    ")\n",
    "\n",
    "print(\"Universal Intelligence Υ(π):\")\n",
    "print(f\"  Random:  {upsilon_random:.3f}\")\n",
    "print(f\"  Greedy:  {upsilon_greedy:.3f}\")\n",
    "print(f\"  MC-AIXI: {upsilon_aixi:.3f}\")\n",
    "print()\n",
    "\n",
    "# Show breakdown by environment\n",
    "print(\"Breakdown by environment (MC-AIXI):\")\n",
    "for ev in env_values_aixi:\n",
    "    print(f\"  {ev['description']:30s}: V={ev['value']:6.2f}, weight={ev['weight']:.4f}, contribution={ev['weighted_value']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intelligence comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall intelligence\n",
    "agents = ['Random', 'Greedy', 'MC-AIXI']\n",
    "intelligence_scores = [upsilon_random, upsilon_greedy, upsilon_aixi]\n",
    "\n",
    "axes[0].bar(agents, intelligence_scores, color=['gray', 'steelblue', 'coral'], alpha=0.7)\n",
    "axes[0].set_ylabel('Υ(π)')\n",
    "axes[0].set_title('Universal Intelligence Measure\\n(Higher = more intelligent)', fontsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, score in enumerate(intelligence_scores):\n",
    "    axes[0].text(i, score + 0.05, f'{score:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Performance by environment\n",
    "env_names = [ev['description'] for ev in env_values_aixi]\n",
    "random_values = [ev['value'] for ev in env_values_random]\n",
    "greedy_values = [ev['value'] for ev in env_values_greedy]\n",
    "aixi_values = [ev['value'] for ev in env_values_aixi]\n",
    "\n",
    "x = np.arange(len(env_names))\n",
    "width = 0.25\n",
    "\n",
    "axes[1].bar(x - width, random_values, width, label='Random', color='gray', alpha=0.7)\n",
    "axes[1].bar(x, greedy_values, width, label='Greedy', color='steelblue', alpha=0.7)\n",
    "axes[1].bar(x + width, aixi_values, width, label='MC-AIXI', color='coral', alpha=0.7)\n",
    "\n",
    "axes[1].set_ylabel('Expected Return V_μ^π')\n",
    "axes[1].set_title('Performance by Environment', fontsize=12)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(env_names, rotation=15, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: MC-AIXI achieves higher Υ(π) through better performance\")\n",
    "print(\"across diverse environments, especially complex ones!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Intelligence and Environment Complexity\n",
    "\n",
    "The universal intelligence measure weights simpler environments more heavily. This means:\n",
    "- Performance on simple environments matters more\n",
    "- But agents must still handle diverse challenges\n",
    "\n",
    "Let's visualize how environment complexity affects the intelligence measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze contribution by complexity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Solomonoff weights by complexity\n",
    "complexities = [ev['complexity'] for ev in env_values_aixi]\n",
    "weights = [ev['weight'] for ev in env_values_aixi]\n",
    "contributions_aixi = [ev['weighted_value'] for ev in env_values_aixi]\n",
    "\n",
    "axes[0].bar(env_names, weights, color='purple', alpha=0.6)\n",
    "axes[0].set_ylabel('Solomonoff Weight (2^-K)')\n",
    "axes[0].set_title('Environment Weighting\\n(Simpler = higher weight)', fontsize=12)\n",
    "axes[0].set_xticks(range(len(env_names)))\n",
    "axes[0].set_xticklabels(env_names, rotation=15, ha='right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add complexity labels\n",
    "for i, (w, k) in enumerate(zip(weights, complexities)):\n",
    "    axes[0].text(i, w + 0.01, f'K={k}', ha='center', fontsize=9)\n",
    "\n",
    "# Contribution to Υ(π)\n",
    "axes[1].bar(env_names, contributions_aixi, color='coral', alpha=0.7)\n",
    "axes[1].set_ylabel('Contribution to Υ(π)')\n",
    "axes[1].set_title('Environment Contributions\\n(weight × value)', fontsize=12)\n",
    "axes[1].set_xticks(range(len(env_names)))\n",
    "axes[1].set_xticklabels(env_names, rotation=15, ha='right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Simpler environments (lower K) contribute more to intelligence measure.\")\n",
    "print(\"This embodies Occam's razor: simple patterns matter most.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Approximations & Computational Limits\n",
    "\n",
    "## 5.1 Why AIXI is Incomputable\n",
    "\n",
    "**Theoretical AIXI**:\n",
    "- Considers ALL computable environments\n",
    "- Uses exact Kolmogorov complexity (incomputable)\n",
    "- Performs infinite lookahead planning\n",
    "\n",
    "**Practical approximations**:\n",
    "1. **MC-AIXI**: Samples environment hypotheses, uses MCTS\n",
    "2. **Time-bounded AIXI**: Limits computation time\n",
    "3. **Restricted hypothesis spaces**: Only consider simple environment models\n",
    "\n",
    "We explore the **time-computation tradeoff**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MC-AIXI with different computation budgets\n",
    "def evaluate_with_budget(n_simulations_list, n_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluate MC-AIXI with different computation budgets.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for n_sims in n_simulations_list:\n",
    "        agent = SimpleMCAIXI(n_actions=4, n_simulations=n_sims, horizon=8)\n",
    "        \n",
    "        # Test on grid world\n",
    "        rewards = []\n",
    "        for _ in range(n_episodes):\n",
    "            env = ToyGridWorld(size=5)\n",
    "            env.reset()\n",
    "            \n",
    "            episode_reward = 0\n",
    "            for step in range(20):\n",
    "                action = agent.select_action(env)\n",
    "                _, reward, done = env.step(action)\n",
    "                episode_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            rewards.append(episode_reward)\n",
    "        \n",
    "        results.append({\n",
    "            'simulations': n_sims,\n",
    "            'mean_reward': np.mean(rewards),\n",
    "            'std_reward': np.std(rewards)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different budgets\n",
    "print(\"Testing computation budget effect...\\n\")\n",
    "budgets = [5, 10, 20, 50, 100]\n",
    "budget_results = evaluate_with_budget(budgets, n_episodes=10)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\"{'Simulations':<12} {'Mean Reward':<15} {'Std Reward':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for res in budget_results:\n",
    "    print(f\"{res['simulations']:<12} {res['mean_reward']:<15.2f} {res['std_reward']:<15.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance vs computation\n",
    "simulations = [res['simulations'] for res in budget_results]\n",
    "mean_rewards = [res['mean_reward'] for res in budget_results]\n",
    "std_rewards = [res['std_reward'] for res in budget_results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(simulations, mean_rewards, yerr=std_rewards, \n",
    "             marker='o', markersize=8, capsize=5, linewidth=2,\n",
    "             color='steelblue', label='MC-AIXI')\n",
    "plt.axhline(np.mean(greedy_rewards), color='coral', linestyle='--', \n",
    "           linewidth=2, label='Greedy (no planning)')\n",
    "plt.xlabel('MCTS Simulations (computation budget)', fontsize=12)\n",
    "plt.ylabel('Mean Episode Reward', fontsize=12)\n",
    "plt.title('Performance vs Computation Budget\\n(Diminishing returns)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation: Performance improves with computation but plateaus.\")\n",
    "print(\"This shows the tradeoff between optimality and computability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Approximation Quality\n",
    "\n",
    "Different approximations make different tradeoffs:\n",
    "\n",
    "| Method | Computation | Optimality | Practical? |\n",
    "|--------|-------------|------------|------------|\n",
    "| Theoretical AIXI | Infinite | Perfect | No |\n",
    "| MC-AIXI (large budget) | Very high | Good | Sometimes |\n",
    "| MC-AIXI (small budget) | Medium | Fair | Yes |\n",
    "| Greedy/heuristic | Low | Poor | Yes |\n",
    "\n",
    "This illustrates the **fundamental tension** between:\n",
    "- **Universality**: Handling all environments\n",
    "- **Optimality**: Making best decisions\n",
    "- **Computability**: Running in finite time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Pathways to Superintelligence\n",
    "\n",
    "## 6.1 Recursive Self-Improvement\n",
    "\n",
    "A key pathway to superintelligence is **recursive self-improvement**:\n",
    "1. System improves its own intelligence\n",
    "2. Smarter system makes better improvements\n",
    "3. Process accelerates (intelligence explosion)\n",
    "\n",
    "We simulate this with a toy model where agents can:\n",
    "- Improve their planning depth\n",
    "- Increase their MCTS simulation budget\n",
    "- Learn environment models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfImprovingAgent:\n",
    "    \"\"\"\n",
    "    Agent that can improve its own capabilities.\n",
    "    \n",
    "    Improvement mechanisms:\n",
    "    - Increase MCTS simulation budget\n",
    "    - Increase planning horizon\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_simulations=10, initial_horizon=5):\n",
    "        self.simulations = initial_simulations\n",
    "        self.horizon = initial_horizon\n",
    "        self.improvement_history = [{\n",
    "            'step': 0,\n",
    "            'simulations': initial_simulations,\n",
    "            'horizon': initial_horizon,\n",
    "            'intelligence': 0\n",
    "        }]\n",
    "    \n",
    "    def select_action(self, env):\n",
    "        \"\"\"Select action using current capabilities.\"\"\"\n",
    "        agent = SimpleMCAIXI(\n",
    "            n_actions=4,\n",
    "            n_simulations=self.simulations,\n",
    "            horizon=self.horizon\n",
    "        )\n",
    "        return agent.select_action(env)\n",
    "    \n",
    "    def self_improve(self, performance_feedback):\n",
    "        \"\"\"\n",
    "        Improve capabilities based on performance.\n",
    "        \n",
    "        Better performance → more resources for improvement.\n",
    "        This creates positive feedback loop.\n",
    "        \"\"\"\n",
    "        # Improvement rate proportional to current intelligence\n",
    "        improvement_factor = 1.0 + (performance_feedback / 10.0)\n",
    "        \n",
    "        # Increase computation budget\n",
    "        self.simulations = int(self.simulations * improvement_factor)\n",
    "        self.simulations = min(self.simulations, 200)  # Cap for practicality\n",
    "        \n",
    "        # Increase planning depth\n",
    "        if np.random.rand() < 0.3:  # Occasional horizon increase\n",
    "            self.horizon = min(self.horizon + 1, 15)\n",
    "        \n",
    "        self.improvement_history.append({\n",
    "            'step': len(self.improvement_history),\n",
    "            'simulations': self.simulations,\n",
    "            'horizon': self.horizon,\n",
    "            'intelligence': performance_feedback\n",
    "        })\n",
    "\n",
    "# Simulate self-improvement process\n",
    "print(\"Simulating recursive self-improvement...\\n\")\n",
    "\n",
    "agent = SelfImprovingAgent(initial_simulations=5, initial_horizon=3)\n",
    "\n",
    "n_improvement_cycles = 8\n",
    "for cycle in range(n_improvement_cycles):\n",
    "    # Evaluate current performance\n",
    "    rewards = []\n",
    "    for _ in range(5):  # 5 episodes per evaluation\n",
    "        env = ToyGridWorld(size=5)\n",
    "        env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        for step in range(20):\n",
    "            action = agent.select_action(env)\n",
    "            _, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(episode_reward)\n",
    "    \n",
    "    performance = np.mean(rewards)\n",
    "    \n",
    "    print(f\"Cycle {cycle}:\")\n",
    "    print(f\"  Simulations: {agent.simulations:4d}  Horizon: {agent.horizon:2d}\")\n",
    "    print(f\"  Performance: {performance:6.2f}\\n\")\n",
    "    \n",
    "    # Self-improve\n",
    "    agent.self_improve(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intelligence explosion\n",
    "history = agent.improvement_history\n",
    "steps = [h['step'] for h in history]\n",
    "simulations_hist = [h['simulations'] for h in history]\n",
    "horizon_hist = [h['horizon'] for h in history]\n",
    "intelligence_hist = [h['intelligence'] for h in history]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Capability growth\n",
    "ax1 = axes[0]\n",
    "ax1.plot(steps, simulations_hist, marker='o', linewidth=2, \n",
    "        color='steelblue', label='MCTS Simulations')\n",
    "ax1.set_xlabel('Improvement Cycle', fontsize=12)\n",
    "ax1.set_ylabel('MCTS Simulations', color='steelblue', fontsize=12)\n",
    "ax1.tick_params(axis='y', labelcolor='steelblue')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(steps, horizon_hist, marker='s', linewidth=2, \n",
    "             color='coral', label='Planning Horizon')\n",
    "ax1_twin.set_ylabel('Planning Horizon', color='coral', fontsize=12)\n",
    "ax1_twin.tick_params(axis='y', labelcolor='coral')\n",
    "\n",
    "axes[0].set_title('Capability Growth\\n(Recursive Self-Improvement)', fontsize=13)\n",
    "\n",
    "# Intelligence growth (exponential-like)\n",
    "axes[1].plot(steps, intelligence_hist, marker='o', linewidth=2.5, \n",
    "            color='purple', label='Performance')\n",
    "axes[1].set_xlabel('Improvement Cycle', fontsize=12)\n",
    "axes[1].set_ylabel('Performance (Intelligence)', fontsize=12)\n",
    "axes[1].set_title('Intelligence Explosion\\n(Accelerating improvement)', fontsize=13)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Self-improvement creates positive feedback loop.\")\n",
    "print(\"Better performance → more resources → better improvements → faster growth.\")\n",
    "print(\"This is the theoretical basis for 'intelligence explosion'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Intelligence Explosion Dynamics\n",
    "\n",
    "**Factors affecting takeoff speed**:\n",
    "\n",
    "1. **Optimization power**: How well can system improve itself?\n",
    "2. **Recalcitrance**: How hard is it to make improvements?\n",
    "3. **Diminishing returns**: Do improvements get harder over time?\n",
    "\n",
    "**Takeoff scenarios**:\n",
    "- **Slow takeoff**: Linear or sublinear growth (decades to superintelligence)\n",
    "- **Fast takeoff**: Exponential growth (years to superintelligence)\n",
    "- **Hard takeoff**: Super-exponential growth (days/weeks to superintelligence)\n",
    "\n",
    "We model different growth curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_takeoff(growth_model, n_steps=50):\n",
    "    \"\"\"\n",
    "    Simulate intelligence growth under different models.\n",
    "    \n",
    "    growth_model: function(current_intelligence, step) -> new_intelligence\n",
    "    \"\"\"\n",
    "    intelligence = [1.0]  # Start at baseline human-level (1.0)\n",
    "    \n",
    "    for step in range(1, n_steps):\n",
    "        new_int = growth_model(intelligence[-1], step)\n",
    "        intelligence.append(new_int)\n",
    "    \n",
    "    return intelligence\n",
    "\n",
    "# Different growth models\n",
    "def linear_growth(current, step):\n",
    "    \"\"\"Slow takeoff: I(t) = I(0) + kt\"\"\"\n",
    "    return 1.0 + 0.1 * step\n",
    "\n",
    "def exponential_growth(current, step):\n",
    "    \"\"\"Fast takeoff: I(t) = I(0) * e^(kt)\"\"\"\n",
    "    return current * 1.15\n",
    "\n",
    "def superexponential_growth(current, step):\n",
    "    \"\"\"Hard takeoff: I(t+1) = I(t)^k (recursive improvement)\"\"\"\n",
    "    growth_rate = 1.05 + (current - 1.0) * 0.01  # Acceleration\n",
    "    return current * growth_rate\n",
    "\n",
    "def diminishing_returns(current, step):\n",
    "    \"\"\"Slow takeoff with saturation: asymptotic to limit\"\"\"\n",
    "    limit = 10.0\n",
    "    rate = 0.15\n",
    "    return current + rate * (limit - current)\n",
    "\n",
    "# Simulate scenarios\n",
    "linear_curve = simulate_takeoff(linear_growth, n_steps=50)\n",
    "exponential_curve = simulate_takeoff(exponential_growth, n_steps=50)\n",
    "superexp_curve = simulate_takeoff(superexponential_growth, n_steps=50)\n",
    "diminishing_curve = simulate_takeoff(diminishing_returns, n_steps=50)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "steps = range(len(linear_curve))\n",
    "plt.plot(steps, linear_curve, linewidth=2.5, label='Linear (Slow Takeoff)', color='blue')\n",
    "plt.plot(steps, exponential_curve, linewidth=2.5, label='Exponential (Fast Takeoff)', color='orange')\n",
    "plt.plot(steps, superexp_curve, linewidth=2.5, label='Super-exponential (Hard Takeoff)', color='red')\n",
    "plt.plot(steps, diminishing_curve, linewidth=2.5, label='Diminishing Returns', color='green', linestyle='--')\n",
    "\n",
    "plt.axhline(1.0, color='gray', linestyle=':', linewidth=1.5, label='Human-level')\n",
    "plt.xlabel('Time Steps', fontsize=13)\n",
    "plt.ylabel('Intelligence (human-level = 1.0)', fontsize=13)\n",
    "plt.title('Intelligence Explosion Scenarios\\n(Different growth dynamics)', fontsize=15)\n",
    "plt.legend(fontsize=11, loc='upper left')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.ylim([0.8, 1000])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Intelligence growth depends critically on feedback dynamics!\")\n",
    "print(\"\\nAt step 50:\")\n",
    "print(f\"  Linear:          {linear_curve[-1]:8.1f}x human-level\")\n",
    "print(f\"  Exponential:     {exponential_curve[-1]:8.1f}x human-level\")\n",
    "print(f\"  Super-exp:       {superexp_curve[-1]:8.1f}x human-level\")\n",
    "print(f\"  Diminishing:     {diminishing_curve[-1]:8.1f}x human-level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Implications and Risks\n",
    "\n",
    "**Key insights from universal intelligence theory**:\n",
    "\n",
    "1. **Intelligence is measurable**: Υ(π) provides formal definition\n",
    "2. **AIXI is optimal**: But incomputable (fundamental limitation)\n",
    "3. **Approximations exist**: MC-AIXI and variants work in practice\n",
    "4. **Self-improvement is possible**: Leads to potential intelligence explosion\n",
    "\n",
    "**Open questions**:\n",
    "- How fast will real AI systems improve?\n",
    "- What are fundamental limits on intelligence?\n",
    "- How do we maintain control during recursive self-improvement?\n",
    "- Can we align superintelligent systems with human values?\n",
    "\n",
    "**Connections to safety**:\n",
    "- AIXI has no inherent values (only maximizes reward)\n",
    "- Specification of reward function is critical\n",
    "- Superintelligence would be very capable but not necessarily aligned\n",
    "- Need robust value alignment before intelligence explosion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Key Takeaways\n",
    "\n",
    "## Core Concepts Implemented\n",
    "\n",
    "1. **Psychometric Intelligence (g-factor)**\n",
    "   - Simulated cognitive test data\n",
    "   - Extracted general intelligence factor using PCA\n",
    "   - Showed positive manifold in correlation matrix\n",
    "\n",
    "2. **Solomonoff Induction**\n",
    "   - Approximated universal prior using program enumeration\n",
    "   - Demonstrated sequence prediction weighted by simplicity\n",
    "   - Estimated Kolmogorov complexity of sequences\n",
    "\n",
    "3. **AIXI Agent**\n",
    "   - Implemented MC-AIXI using MCTS\n",
    "   - Tested in toy grid-world environments\n",
    "   - Compared to random and greedy baselines\n",
    "\n",
    "4. **Universal Intelligence Measure (Υ)**\n",
    "   - Created environment suite with varying complexities\n",
    "   - Computed Υ(π) for different agents\n",
    "   - Showed intelligence emerges from diverse performance\n",
    "\n",
    "5. **Computational Limits**\n",
    "   - Explored computation-performance tradeoffs\n",
    "   - Demonstrated diminishing returns with increased budget\n",
    "   - Illustrated incomputability of perfect AIXI\n",
    "\n",
    "6. **Recursive Self-Improvement**\n",
    "   - Simulated agent improving its own capabilities\n",
    "   - Modeled different intelligence explosion scenarios\n",
    "   - Discussed implications for superintelligence\n",
    "\n",
    "## Philosophical Implications\n",
    "\n",
    "- **Intelligence is formally definable** (not just intuitive)\n",
    "- **Simplicity matters** (Solomonoff prior = Occam's razor)\n",
    "- **Optimality is incomputable** (fundamental limitation)\n",
    "- **Self-improvement creates feedback loops** (intelligence explosion possible)\n",
    "\n",
    "## Practical Lessons\n",
    "\n",
    "- Real AI systems use **approximations** of theoretical ideals\n",
    "- **Computation budget** critically affects performance\n",
    "- **Environment diversity** tests true intelligence\n",
    "- **Value alignment** needed before superintelligence\n",
    "\n",
    "---\n",
    "\n",
    "**Further Reading**:\n",
    "- Legg, S. (2008). *Machine Super Intelligence*. PhD Thesis.\n",
    "- Hutter, M. (2005). *Universal Artificial Intelligence*.\n",
    "- Solomonoff, R. (1964). *A Formal Theory of Inductive Inference*.\n",
    "- Bostrom, N. (2014). *Superintelligence: Paths, Dangers, Strategies*.\n",
    "\n",
    "**Related Papers**:\n",
    "- Paper 23: MDL (Minimum Description Length)\n",
    "- Paper 25: Kolmogorov Complexity  \n",
    "- Paper 8: DQN (Practical deep RL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
