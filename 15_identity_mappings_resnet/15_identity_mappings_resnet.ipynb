{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 论文15：深度残差网络中的恒等映射\n## Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2016)\n\n### 预激活 ResNet\n\n改进的残差块，具有更好的梯度流动。核心洞察：将激活移到卷积之前！"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 原始 ResNet 块\n\n```\nx → Conv → BN → ReLU → Conv → BN → (+) → ReLU → output\n    ↓                                  ↑\n    └──────────── identity ────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def batch_norm_1d(x, gamma=1.0, beta=0.0, eps=1e-5):\n",
    "    \"\"\"Simplified batch normalization for 1D\"\"\"\n",
    "    mean = np.mean(x)\n",
    "    var = np.var(x)\n",
    "    x_normalized = (x - mean) / np.sqrt(var + eps)\n",
    "    return gamma * x_normalized + beta\n",
    "\n",
    "class OriginalResidualBlock:\n",
    "    \"\"\"Original ResNet block (post-activation)\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        # Two layers\n",
    "        self.W1 = np.random.randn(dim, dim) * 0.01\n",
    "        self.W2 = np.random.randn(dim, dim) * 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Original: x → Conv → BN → ReLU → Conv → BN → (+x) → ReLU\n",
    "        \"\"\"\n",
    "        # First conv-bn-relu\n",
    "        out = np.dot(self.W1, x)\n",
    "        out = batch_norm_1d(out)\n",
    "        out = relu(out)\n",
    "        \n",
    "        # Second conv-bn\n",
    "        out = np.dot(self.W2, out)\n",
    "        out = batch_norm_1d(out)\n",
    "        \n",
    "        # Add identity (residual connection)\n",
    "        out = out + x\n",
    "        \n",
    "        # Final ReLU (post-activation)\n",
    "        out = relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test\n",
    "original_block = OriginalResidualBlock(dim=8)\n",
    "x = np.random.randn(8)\n",
    "output_original = original_block.forward(x)\n",
    "\n",
    "print(f\"Input: {x[:4]}...\")\n",
    "print(f\"Original ResNet output: {output_original[:4]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 预激活 ResNet 块\n\n```\nx → BN → ReLU → Conv → BN → ReLU → Conv → (+) → output\n    ↓                                       ↑\n    └──────────── identity ─────────────────┘\n```\n\n**关键区别**: 激活在卷积之前，干净的恒等路径！"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActivationResidualBlock:\n",
    "    \"\"\"Pre-activation ResNet block (improved)\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.W1 = np.random.randn(dim, dim) * 0.01\n",
    "        self.W2 = np.random.randn(dim, dim) * 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pre-activation: x → BN → ReLU → Conv → BN → ReLU → Conv → (+x)\n",
    "        \"\"\"\n",
    "        # First bn-relu-conv\n",
    "        out = batch_norm_1d(x)\n",
    "        out = relu(out)\n",
    "        out = np.dot(self.W1, out)\n",
    "        \n",
    "        # Second bn-relu-conv\n",
    "        out = batch_norm_1d(out)\n",
    "        out = relu(out)\n",
    "        out = np.dot(self.W2, out)\n",
    "        \n",
    "        # Add identity (NO activation after!)\n",
    "        out = out + x\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test\n",
    "preact_block = PreActivationResidualBlock(dim=8)\n",
    "output_preact = preact_block.forward(x)\n",
    "\n",
    "print(f\"\\nPre-activation ResNet output: {output_preact[:4]}...\")\n",
    "print(f\"\\nKey difference: Clean identity path (no ReLU after addition)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 梯度流动分析\n\n为什么预激活更好："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_flow(block_type, num_layers=10, input_dim=8):\n",
    "    \"\"\"\n",
    "    Simulate gradient flow through stacked residual blocks\n",
    "    \"\"\"\n",
    "    x = np.random.randn(input_dim)\n",
    "    \n",
    "    # Create blocks\n",
    "    if block_type == 'original':\n",
    "        blocks = [OriginalResidualBlock(input_dim) for _ in range(num_layers)]\n",
    "    else:\n",
    "        blocks = [PreActivationResidualBlock(input_dim) for _ in range(num_layers)]\n",
    "    \n",
    "    # Forward pass\n",
    "    activations = [x]\n",
    "    current = x\n",
    "    for block in blocks:\n",
    "        current = block.forward(current)\n",
    "        activations.append(current.copy())\n",
    "    \n",
    "    # Simulate backward pass (simplified gradient flow)\n",
    "    grad = np.ones(input_dim)  # Gradient from loss\n",
    "    gradients = [grad]\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        # For residual blocks: gradient splits into identity + residual path\n",
    "        # Pre-activation has cleaner gradient flow\n",
    "        \n",
    "        if block_type == 'original':\n",
    "            # Post-activation: gradient affected by ReLU derivative\n",
    "            # Simplified: some gradient is killed by ReLU\n",
    "            grad_through_residual = grad * np.random.uniform(0.5, 1.0, input_dim)\n",
    "            grad = grad + grad_through_residual  # Identity + residual\n",
    "        else:\n",
    "            # Pre-activation: clean identity path\n",
    "            grad_through_residual = grad * np.random.uniform(0.7, 1.0, input_dim)\n",
    "            grad = grad + grad_through_residual  # Better gradient flow\n",
    "        \n",
    "        gradients.append(grad.copy())\n",
    "    \n",
    "    return activations, gradients\n",
    "\n",
    "# Compare gradient flow\n",
    "_, grad_original = compute_gradient_flow('original', num_layers=20)\n",
    "_, grad_preact = compute_gradient_flow('preact', num_layers=20)\n",
    "\n",
    "# Compute gradient magnitudes\n",
    "grad_mag_original = [np.linalg.norm(g) for g in grad_original]\n",
    "grad_mag_preact = [np.linalg.norm(g) for g in grad_preact]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(grad_mag_original, 'o-', label='Original ResNet (post-activation)', linewidth=2)\n",
    "plt.plot(grad_mag_preact, 's-', label='Pre-activation ResNet', linewidth=2)\n",
    "plt.xlabel('Layer (from output to input)', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Gradient Flow Comparison', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original ResNet gradient at input: {grad_mag_original[-1]:.2f}\")\n",
    "print(f\"Pre-activation gradient at input: {grad_mag_preact[-1]:.2f}\")\n",
    "print(f\"\\nPre-activation maintains stronger gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 不同的激活位置\n\n论文分析了各种放置选项："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different architectures\n",
    "architectures = [\n",
    "    {\n",
    "        'name': 'Original',\n",
    "        'structure': 'x → Conv → BN → ReLU → Conv → BN → (+x) → ReLU',\n",
    "        'identity': 'Blocked by ReLU',\n",
    "        'score': '★★★☆☆'\n",
    "    },\n",
    "    {\n",
    "        'name': 'BN after addition',\n",
    "        'structure': 'x → Conv → BN → ReLU → Conv → BN → (+x) → BN → ReLU',\n",
    "        'identity': 'Blocked by BN & ReLU',\n",
    "        'score': '★★☆☆☆'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ReLU before addition',\n",
    "        'structure': 'x → BN → ReLU → Conv → BN → ReLU → Conv → ReLU → (+x)',\n",
    "        'identity': 'Blocked by ReLU',\n",
    "        'score': '★★☆☆☆'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Full pre-activation',\n",
    "        'structure': 'x → BN → ReLU → Conv → BN → ReLU → Conv → (+x)',\n",
    "        'identity': 'CLEAN! ✓',\n",
    "        'score': '★★★★★'\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESIDUAL BLOCK ARCHITECTURES COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, arch in enumerate(architectures, 1):\n",
    "    print(f\"{i}. {arch['name']:20s} {arch['score']}\")\n",
    "    print(f\"   Structure: {arch['structure']}\")\n",
    "    print(f\"   Identity path: {arch['identity']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WINNER: Full pre-activation (BN → ReLU → Conv)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 深度网络比较"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResNet:\n",
    "    \"\"\"Stack of residual blocks\"\"\"\n",
    "    def __init__(self, dim, num_blocks, block_type='preact'):\n",
    "        self.blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            if block_type == 'preact':\n",
    "                self.blocks.append(PreActivationResidualBlock(dim))\n",
    "            else:\n",
    "                self.blocks.append(OriginalResidualBlock(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "            activations.append(x.copy())\n",
    "        return x, activations\n",
    "\n",
    "# Compare deep networks\n",
    "depth = 50\n",
    "dim = 16\n",
    "x_input = np.random.randn(dim)\n",
    "\n",
    "net_original = DeepResNet(dim, depth, 'original')\n",
    "net_preact = DeepResNet(dim, depth, 'preact')\n",
    "\n",
    "out_original, acts_original = net_original.forward(x_input)\n",
    "out_preact, acts_preact = net_preact.forward(x_input)\n",
    "\n",
    "# Compute activation statistics\n",
    "norms_original = [np.linalg.norm(a) for a in acts_original]\n",
    "norms_preact = [np.linalg.norm(a) for a in acts_preact]\n",
    "\n",
    "# Plot activation norms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Activation magnitudes\n",
    "ax1.plot(norms_original, label='Original ResNet', linewidth=2)\n",
    "ax1.plot(norms_preact, label='Pre-activation ResNet', linewidth=2)\n",
    "ax1.set_xlabel('Layer', fontsize=12)\n",
    "ax1.set_ylabel('Activation Magnitude', fontsize=12)\n",
    "ax1.set_title(f'Activation Flow (Depth={depth})', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Activation heatmaps\n",
    "acts_matrix_original = np.array(acts_original).T\n",
    "acts_matrix_preact = np.array(acts_preact).T\n",
    "\n",
    "im = ax2.imshow(acts_matrix_preact - acts_matrix_original, cmap='RdBu', aspect='auto')\n",
    "ax2.set_xlabel('Layer', fontsize=12)\n",
    "ax2.set_ylabel('Feature Dimension', fontsize=12)\n",
    "ax2.set_title('Difference (Pre-act - Original)', fontsize=14)\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOriginal ResNet final norm: {norms_original[-1]:.4f}\")\n",
    "print(f\"Pre-activation final norm: {norms_preact[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 恒等映射分析"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_identity_mapping(block, num_tests=100):\n",
    "    \"\"\"\n",
    "    Test how well the block can learn identity mapping\n",
    "    (When residual path learns zero, output should equal input)\n",
    "    \"\"\"\n",
    "    # Zero out weights (residual path learns nothing)\n",
    "    block.W1 = np.zeros_like(block.W1)\n",
    "    block.W2 = np.zeros_like(block.W2)\n",
    "    \n",
    "    errors = []\n",
    "    for _ in range(num_tests):\n",
    "        x = np.random.randn(block.dim)\n",
    "        y = block.forward(x)\n",
    "        error = np.linalg.norm(y - x)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return np.mean(errors), np.std(errors)\n",
    "\n",
    "# Test both block types\n",
    "original_test = OriginalResidualBlock(dim=8)\n",
    "preact_test = PreActivationResidualBlock(dim=8)\n",
    "\n",
    "mean_err_original, std_err_original = test_identity_mapping(original_test)\n",
    "mean_err_preact, std_err_preact = test_identity_mapping(preact_test)\n",
    "\n",
    "print(\"\\nIdentity Mapping Test (residual path = 0):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original ResNet error: {mean_err_original:.6f} ± {std_err_original:.6f}\")\n",
    "print(f\"Pre-activation error:  {mean_err_preact:.6f} ± {std_err_preact:.6f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPre-activation has {'BETTER' if mean_err_preact < mean_err_original else 'WORSE'} identity mapping!\")\n",
    "print(\"(Lower error = cleaner identity path)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 架构比较可视化"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "def draw_block(ax, title, is_preact=False):\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 12)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Identity path (left)\n",
    "    ax.plot([1, 1], [1, 11], 'b-', linewidth=4, label='Identity path')\n",
    "    ax.arrow(1, 10.5, 0, -0.3, head_width=0.3, head_length=0.2, fc='blue', ec='blue')\n",
    "    \n",
    "    # Residual path (right)\n",
    "    y_pos = 11\n",
    "    \n",
    "    if is_preact:\n",
    "        # Pre-activation: BN → ReLU → Conv → BN → ReLU → Conv\n",
    "        operations = ['BN', 'ReLU', 'Conv', 'BN', 'ReLU', 'Conv']\n",
    "        colors = ['lightgreen', 'lightyellow', 'lightblue', 'lightgreen', 'lightyellow', 'lightblue']\n",
    "    else:\n",
    "        # Original: Conv → BN → ReLU → Conv → BN\n",
    "        operations = ['Conv', 'BN', 'ReLU', 'Conv', 'BN', 'ReLU*']\n",
    "        colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightblue', 'lightgreen', 'lightcoral']\n",
    "    \n",
    "    for i, (op, color) in enumerate(zip(operations, colors)):\n",
    "        y = y_pos - i * 1.5\n",
    "        \n",
    "        # Draw box\n",
    "        width = 2\n",
    "        height = 1\n",
    "        ax.add_patch(plt.Rectangle((6-width/2, y-height/2), width, height, \n",
    "                                   fill=True, color=color, ec='black', linewidth=2))\n",
    "        ax.text(6, y, op, ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Draw arrow to next\n",
    "        if i < len(operations) - 1:\n",
    "            ax.arrow(6, y-height/2-0.1, 0, -0.3, head_width=0.2, head_length=0.1, \n",
    "                    fc='black', ec='black', linewidth=1.5)\n",
    "    \n",
    "    # Addition\n",
    "    add_y = y_pos - len(operations) * 1.5\n",
    "    ax.plot([1, 6], [add_y, add_y], 'k-', linewidth=2)\n",
    "    ax.scatter([3.5], [add_y], s=500, c='white', edgecolors='black', linewidths=3, zorder=5)\n",
    "    ax.text(3.5, add_y, '+', ha='center', va='center', fontsize=20, fontweight='bold', zorder=6)\n",
    "    \n",
    "    # Output arrow\n",
    "    ax.arrow(3.5, add_y-0.3, 0, -0.5, head_width=0.3, head_length=0.2, \n",
    "            fc='green', ec='green', linewidth=3)\n",
    "    ax.text(3.5, add_y-1.2, 'Output', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Input\n",
    "    ax.text(1, 11.5, 'Input', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(6, 11.5, 'Input', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Annotations\n",
    "    if not is_preact:\n",
    "        ax.text(8.5, add_y, 'ReLU* blocks\\nidentity!', fontsize=10, color='red', \n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    else:\n",
    "        ax.text(8.5, add_y, 'Clean\\nidentity!', fontsize=10, color='green',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "draw_block(axes[0], 'Original ResNet (Post-activation)', is_preact=False)\n",
    "draw_block(axes[1], 'Pre-activation ResNet (Improved)', is_preact=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 关键要点\n\n### 恒等映射问题:\n\n在原始 ResNet 中:\n```\ny = ReLU(F(x) + x)\n```\n加法后的 ReLU **阻断了**恒等路径！\n\n### 预激活解决方案:\n\n```\ny = F'(x) + x\n```\n其中 F'(x) = Conv(ReLU(BN(Conv(ReLU(BN(x))))))\n\n**干净的恒等路径** → 更好的梯度流动！\n\n### 关键变化:\n\n1. **将 BN 移到 Conv 之前**: `x → BN → ReLU → Conv`\n2. **移除最后的 ReLU**: 加法后没有激活\n3. **结果**: 恒等路径真正是恒等的\n\n### 梯度流动:\n\n**原始版本**:\n```\n∂L/∂x = ∂L/∂y · (∂F/∂x + I) · ∂ReLU/∂y\n```\nReLU 导数会阻断梯度！\n\n**预激活版本**:\n```\n∂L/∂x = ∂L/∂y · (∂F'/∂x + I)\n```\n通过恒等的干净梯度流动！\n\n### 优势:\n\n- ✅ **更好的梯度流动**: 恒等路径上无阻断\n- ✅ **更容易优化**: 可以训练更深的网络（1000+ 层）\n- ✅ **更好的准确率**: 小但一致的改进\n- ✅ **正则化**: Conv 前的 BN 起正则化作用\n\n### 比较:\n\n| 架构 | 恒等路径 | 梯度流动 | 性能 |\n|--------------|---------------|---------------|-------------|\n| 原始 ResNet | 被 ReLU 阻断 | 良好 | ★★★★☆ |\n| 预激活 | **干净** | **更好** | ★★★★★ |\n\n### 实现技巧:\n\n1. 对于非常深的网络（>50 层）使用预激活\n2. 对于浅层网络保持原始 ResNet（向后兼容）\n3. 第一层可以保持后激活（还没有恒等）\n4. 最后一层需要后激活以产生最终输出\n\n### 结果:\n\n- CIFAR-10: 成功训练了 1001 层的网络！\n- ImageNet: 相比原始 ResNet 有一致的改进\n- 使 1000+ 层网络的训练成为可能\n\n### 为什么重要:\n\n这篇论文表明**架构细节很重要**。小的变化（移动 BN/ReLU）可以对可训练性和性能产生重大影响。这是深度学习研究中迭代改进的一个典型例子。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}