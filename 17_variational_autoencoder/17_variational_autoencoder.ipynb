{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 论文17：变分有损自编码器\n## Xi Chen, Diederik P. Kingma, et al. (2016)\n\n### VAE：具有学习潜在空间的生成模型\n\n将深度学习与变分推理结合用于生成建模。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 变分自编码器 (VAE) 基础\n\nVAE学习：\n- **编码器**：q(z|x) - 近似后验\n- **解码器**：p(x|z) - 生成模型\n\n**损失**：ELBO = 重构损失 + KL散度"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "class VAE:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: x -> h -> (mu, log_var)\n",
    "        self.W_enc_h = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "        self.b_enc_h = np.zeros(hidden_dim)\n",
    "        \n",
    "        self.W_mu = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
    "        self.b_mu = np.zeros(latent_dim)\n",
    "        \n",
    "        self.W_logvar = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
    "        self.b_logvar = np.zeros(latent_dim)\n",
    "        \n",
    "        # Decoder: z -> h -> x_recon\n",
    "        self.W_dec_h = np.random.randn(latent_dim, hidden_dim) * 0.1\n",
    "        self.b_dec_h = np.zeros(hidden_dim)\n",
    "        \n",
    "        self.W_recon = np.random.randn(hidden_dim, input_dim) * 0.1\n",
    "        self.b_recon = np.zeros(input_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to latent distribution parameters\n",
    "        \n",
    "        Returns: mu, log_var of q(z|x)\n",
    "        \"\"\"\n",
    "        h = relu(np.dot(x, self.W_enc_h) + self.b_enc_h)\n",
    "        mu = np.dot(h, self.W_mu) + self.b_mu\n",
    "        log_var = np.dot(h, self.W_logvar) + self.b_logvar\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + sigma * epsilon\n",
    "        where epsilon ~ N(0, I)\n",
    "        \"\"\"\n",
    "        std = np.exp(0.5 * log_var)\n",
    "        epsilon = np.random.randn(*mu.shape)\n",
    "        z = mu + std * epsilon\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent code to reconstruction\n",
    "        \n",
    "        Returns: reconstructed x\n",
    "        \"\"\"\n",
    "        h = relu(np.dot(z, self.W_dec_h) + self.b_dec_h)\n",
    "        x_recon = sigmoid(np.dot(h, self.W_recon) + self.b_recon)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        mu, log_var = self.encode(x)\n",
    "        \n",
    "        # Sample latent\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # Decode\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        return x_recon, mu, log_var, z\n",
    "    \n",
    "    def loss(self, x, x_recon, mu, log_var):\n",
    "        \"\"\"\n",
    "        VAE loss = Reconstruction Loss + KL Divergence\n",
    "        \"\"\"\n",
    "        # Reconstruction loss (binary cross-entropy)\n",
    "        recon_loss = -np.sum(\n",
    "            x * np.log(x_recon + 1e-8) + \n",
    "            (1 - x) * np.log(1 - x_recon + 1e-8)\n",
    "        )\n",
    "        \n",
    "        # KL divergence: KL(q(z|x) || p(z))\n",
    "        # where p(z) = N(0, I)\n",
    "        # KL = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        kl_loss = -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var))\n",
    "        \n",
    "        return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "\n",
    "# Create VAE\n",
    "input_dim = 16  # e.g., 4x4 image flattened\n",
    "hidden_dim = 32\n",
    "latent_dim = 2  # 2D for visualization\n",
    "\n",
    "vae = VAE(input_dim, hidden_dim, latent_dim)\n",
    "print(f\"VAE created:\")\n",
    "print(f\"  Input: {input_dim}\")\n",
    "print(f\"  Hidden: {hidden_dim}\")\n",
    "print(f\"  Latent: {latent_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 生成合成数据 (Generate Synthetic Data)\n\n简单的4x4模式用于演示"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patterns(num_samples=100):\n",
    "    \"\"\"\n",
    "    Generate simple 4x4 binary patterns\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        pattern = np.zeros((4, 4))\n",
    "        \n",
    "        if i % 4 == 0:\n",
    "            # Horizontal line\n",
    "            pattern[1:2, :] = 1\n",
    "        elif i % 4 == 1:\n",
    "            # Vertical line\n",
    "            pattern[:, 2:3] = 1\n",
    "        elif i % 4 == 2:\n",
    "            # Diagonal\n",
    "            np.fill_diagonal(pattern, 1)\n",
    "        else:\n",
    "            # Corner square\n",
    "            pattern[:2, :2] = 1\n",
    "        \n",
    "        # Add small noise\n",
    "        noise = np.random.randn(4, 4) * 0.05\n",
    "        pattern = np.clip(pattern + noise, 0, 1)\n",
    "        \n",
    "        data.append(pattern.flatten())\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "# Generate training data\n",
    "X_train = generate_patterns(200)\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i].reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Pattern {i}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Training Data Samples')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(X_train)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 测试前向传播和损失 (Test Forward Pass and Loss)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single example\n",
    "x = X_train[0:1]\n",
    "x_recon, mu, log_var, z = vae.forward(x)\n",
    "\n",
    "total_loss, recon_loss, kl_loss = vae.loss(x, x_recon, mu, log_var)\n",
    "\n",
    "print(f\"Forward pass:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Latent mu: {mu}\")\n",
    "print(f\"  Latent log_var: {log_var}\")\n",
    "print(f\"  Latent z: {z}\")\n",
    "print(f\"  Reconstruction shape: {x_recon.shape}\")\n",
    "print(f\"\\nLosses:\")\n",
    "print(f\"  Total: {total_loss:.4f}\")\n",
    "print(f\"  Reconstruction: {recon_loss:.4f}\")\n",
    "print(f\"  KL Divergence: {kl_loss:.4f}\")\n",
    "\n",
    "# Visualize reconstruction\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax1.imshow(x.reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(x_recon.reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "ax2.set_title('Reconstruction (Untrained)')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 可视化潜在空间 (Visualize Latent Space)\n\n由于latent_dim=2，我们可以可视化学习到的表示"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all training data\n",
    "latent_codes = []\n",
    "pattern_types = []\n",
    "\n",
    "for i, x in enumerate(X_train):\n",
    "    mu, log_var = vae.encode(x.reshape(1, -1))\n",
    "    latent_codes.append(mu[0])\n",
    "    pattern_types.append(i % 4)\n",
    "\n",
    "latent_codes = np.array(latent_codes)\n",
    "pattern_types = np.array(pattern_types)\n",
    "\n",
    "# Plot latent space\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    latent_codes[:, 0], \n",
    "    latent_codes[:, 1], \n",
    "    c=pattern_types, \n",
    "    cmap='tab10', \n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "plt.colorbar(scatter, label='Pattern Type')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('Latent Space (Untrained VAE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Latent space visualization shows distribution of encoded patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 从先验采样并生成 (Sample from Prior and Generate)\n\n采样 z ~ N(0, I) 并解码以生成新样本"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from standard normal prior\n",
    "num_samples = 8\n",
    "z_samples = np.random.randn(num_samples, latent_dim)\n",
    "\n",
    "# Generate samples\n",
    "generated = []\n",
    "for z in z_samples:\n",
    "    x_gen = vae.decode(z.reshape(1, -1))\n",
    "    generated.append(x_gen[0])\n",
    "\n",
    "# Visualize generated samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(generated[i].reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'z={z_samples[i][:2]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Generated Samples from Prior p(z) = N(0, I)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 潜在空间中的插值 (Interpolation in Latent Space)\n\n在潜在空间中的两点之间平滑插值"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode two different patterns\n",
    "x1 = X_train[0:1]  # Pattern type 0\n",
    "x2 = X_train[1:2]  # Pattern type 1\n",
    "\n",
    "mu1, _ = vae.encode(x1)\n",
    "mu2, _ = vae.encode(x2)\n",
    "\n",
    "# Interpolate\n",
    "num_steps = 8\n",
    "interpolated = []\n",
    "\n",
    "for alpha in np.linspace(0, 1, num_steps):\n",
    "    z_interp = (1 - alpha) * mu1 + alpha * mu2\n",
    "    x_interp = vae.decode(z_interp)\n",
    "    interpolated.append(x_interp[0])\n",
    "\n",
    "# Visualize interpolation\n",
    "fig, axes = plt.subplots(1, num_steps, figsize=(16, 2))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(interpolated[i].reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'α={i/(num_steps-1):.2f}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Latent Space Interpolation', fontsize=14, y=1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Smooth transitions show continuity in latent space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 重参数化技巧可视化 (Reparameterization Trick Visualization)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show multiple samples from same distribution\n",
    "x = X_train[0:1]\n",
    "mu, log_var = vae.encode(x)\n",
    "\n",
    "# Sample multiple times\n",
    "num_samples = 100\n",
    "z_samples = []\n",
    "for _ in range(num_samples):\n",
    "    z = vae.reparameterize(mu, log_var)\n",
    "    z_samples.append(z[0])\n",
    "\n",
    "z_samples = np.array(z_samples)\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1], alpha=0.3, s=20)\n",
    "plt.scatter(mu[0, 0], mu[0, 1], color='red', s=200, marker='*', label='μ', zorder=5)\n",
    "\n",
    "# Draw ellipse for 2 standard deviations\n",
    "std = np.exp(0.5 * log_var[0])\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "ellipse_x = mu[0, 0] + 2 * std[0] * np.cos(theta)\n",
    "ellipse_y = mu[0, 1] + 2 * std[1] * np.sin(theta)\n",
    "plt.plot(ellipse_x, ellipse_y, 'r--', label='2σ boundary', linewidth=2)\n",
    "\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('Reparameterization Trick: z = μ + σ ⊙ ε, where ε ~ N(0,I)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"μ = {mu[0]}\")\n",
    "print(f\"σ = {std}\")\n",
    "print(f\"Sample mean: {z_samples.mean(axis=0)}\")\n",
    "print(f\"Sample std: {z_samples.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 关键要点 (Key Takeaways)\n\n### VAE架构 (VAE Architecture)：\n1. **编码器**：q_φ(z|x) - 将输入映射到潜在分布\n2. **重参数化**：z = μ + σ ⊙ ε（使反向传播成为可能）\n3. **解码器**：p_θ(x|z) - 从潜在代码生成输出\n\n### 损失函数 (ELBO)：\n```\nL = E[log p(x|z)] - KL(q(z|x) || p(z))\n  = 重构损失 - KL散度\n```\n\n### KL散度 (KL Divergence)：\n- 正则化潜在空间使其接近先验 p(z) = N(0, I)\n- 防止过拟合\n- 确保平滑的潜在空间\n\n### 重参数化技巧 (Reparameterization Trick)：\n- 使采样可微分\n- z = μ(x) + σ(x) ⊙ ε，其中 ε ~ N(0, I)\n- 梯度通过 μ 和 σ 流动\n\n### 特性 (Properties)：\n- **生成性**：可以采样新数据\n- **连续潜在空间**：平滑插值\n- **概率性**：对不确定性建模\n- **解耦表示**：（使用 β-VAE 等）\n\n### 应用 (Applications)：\n- 图像生成\n- 降维\n- 半监督学习\n- 异常检测\n- 数据增强\n\n### 变体 (Variants)：\n- **β-VAE**：加权的KL用于解耦\n- **条件VAE**：条件化生成\n- **层次VAE**：多个潜在层级\n- **VQ-VAE**：离散潜在变量"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}