{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文2：循环神经网络的不合理有效性\n",
    "\n",
    "## Andrej Karpathy\n",
    "\n",
    "### 字符级语言模型与原始RNN\n",
    "\n",
    "实现一个学习生成文本的字符级RNN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成合成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带有模式的简单合成文本\n",
    "data = \"\"\"\n",
    "hello world\n",
    "hello deep learning\n",
    "deep neural networks\n",
    "neural networks learn patterns\n",
    "patterns in data\n",
    "data drives learning\n",
    "learning from examples\n",
    "examples help networks\n",
    "networks process information\n",
    "information is everywhere\n",
    "everywhere you look data\n",
    "\"\"\" * 10  # 重复以获得更多训练数据\n",
    "\n",
    "# 构建词汇表\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"数据长度: {len(data)} 个字符\")\n",
    "print(f\"词汇表大小: {vocab_size}\")\n",
    "print(f\"词汇表: {repr(''.join(chars))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始RNN实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "        \"\"\"\n",
    "        inputs: 整数列表（字符索引）\n",
    "        hprev: 初始隐藏状态\n",
    "        \"\"\"\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        loss = 0\n",
    "        \n",
    "        # 前向传播\n",
    "        for t, char_idx in enumerate(inputs):\n",
    "            # 独热编码输入\n",
    "            xs[t] = np.zeros((self.vocab_size, 1))\n",
    "            xs[t][char_idx] = 1\n",
    "            \n",
    "            # 隐藏状态: h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)\n",
    "            hs[t] = np.tanh(\n",
    "                np.dot(self.Wxh, xs[t]) + \n",
    "                np.dot(self.Whh, hs[t-1]) + \n",
    "                self.bh\n",
    "            )\n",
    "            \n",
    "            # 输出: y_t = W_hy * h_t + b_y\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            \n",
    "            # Softmax概率\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "            \n",
    "        return xs, hs, ys, ps\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"交叉熵损失\"\"\"\n",
    "        loss = 0\n",
    "        for t, target_idx in enumerate(targets):\n",
    "            loss += -np.log(ps[t][target_idx, 0])\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        \"\"\"通过时间反向传播\"\"\"\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        \n",
    "        # 反向传播\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # 输出梯度\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            \n",
    "            # 输出层梯度\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            \n",
    "            # 隐藏层梯度\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            dhraw = (1 - hs[t] ** 2) * dh  # tanh导数\n",
    "            \n",
    "            # 权重梯度\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            \n",
    "            # 下一个时间步的梯度\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "        \n",
    "        # 裁剪梯度以防止梯度爆炸\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "    \n",
    "    def sample(self, h, seed_ix, n):\n",
    "        \"\"\"\n",
    "        从模型采样字符序列\n",
    "        h: 初始隐藏状态\n",
    "        seed_ix: 种子字符索引\n",
    "        n: 要生成的字符数量\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "        indices = []\n",
    "        \n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            \n",
    "            # 从分布中采样\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            \n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            indices.append(ix)\n",
    "        \n",
    "        return indices\n",
    "\n",
    "# 初始化模型\n",
    "hidden_size = 64\n",
    "rnn = VanillaRNN(vocab_size, hidden_size)\n",
    "print(f\"\\n模型已初始化，包含 {hidden_size} 个隐藏单元\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, data, char_to_ix, ix_to_char, num_iterations=2000, seq_length=25):\n",
    "    \"\"\"训练RNN\"\"\"\n",
    "    n = 0  # 数据中的位置\n",
    "    p = 0  # 数据指针\n",
    "    \n",
    "    # Adagrad的内存变量\n",
    "    mWxh = np.zeros_like(rnn.Wxh)\n",
    "    mWhh = np.zeros_like(rnn.Whh)\n",
    "    mWhy = np.zeros_like(rnn.Why)\n",
    "    mbh = np.zeros_like(rnn.bh)\n",
    "    mby = np.zeros_like(rnn.by)\n",
    "    \n",
    "    smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "    losses = []\n",
    "    \n",
    "    hprev = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    for n in range(num_iterations):\n",
    "        # 准备输入和目标\n",
    "        if p + seq_length + 1 >= len(data) or n == 0:\n",
    "            hprev = np.zeros((hidden_size, 1))\n",
    "            p = 0\n",
    "        \n",
    "        inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "        targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "        \n",
    "        # 前向传播\n",
    "        xs, hs, ys, ps = rnn.forward(inputs, hprev)\n",
    "        loss = rnn.loss(ps, targets)\n",
    "        \n",
    "        # 反向传播\n",
    "        dWxh, dWhh, dWhy, dbh, dby = rnn.backward(xs, hs, ps, targets)\n",
    "        \n",
    "        # Adagrad参数更新\n",
    "        learning_rate = 0.1\n",
    "        for param, dparam, mem in zip(\n",
    "            [rnn.Wxh, rnn.Whh, rnn.Why, rnn.bh, rnn.by],\n",
    "            [dWxh, dWhh, dWhy, dbh, dby],\n",
    "            [mWxh, mWhh, mWhy, mbh, mby]\n",
    "        ):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "        \n",
    "        # 跟踪损失\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "        losses.append(smooth_loss)\n",
    "        \n",
    "        # 从模型采样\n",
    "        if n % 200 == 0:\n",
    "            sample_ix = rnn.sample(hprev, inputs[0], 100)\n",
    "            txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "            print(f\"\\n--- 迭代 {n}, 损失: {smooth_loss:.4f} ---\")\n",
    "            print(txt)\n",
    "        \n",
    "        # 移动数据指针\n",
    "        p += seq_length\n",
    "        hprev = hs[len(inputs) - 1]\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 训练模型\n",
    "print(\"正在训练RNN...\\n\")\n",
    "losses = train_rnn(rnn, data, char_to_ix, ix_to_char, num_iterations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化训练进度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('平滑损失')\n",
    "plt.title('RNN训练损失（字符级语言模型）')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从训练好的模型生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用不同的种子生成样本\n",
    "h = np.zeros((hidden_size, 1))\n",
    "\n",
    "print(\"生成的样本:\\n\")\n",
    "for i in range(5):\n",
    "    seed_char = np.random.choice(chars)\n",
    "    seed_ix = char_to_ix[seed_char]\n",
    "    sample_ix = rnn.sample(h, seed_ix, 150)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print(f\"样本 {i+1} (种子: '{seed_char}'):\")\n",
    "    print(txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化隐藏状态激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过序列的前向传播来可视化激活\n",
    "test_text = \"hello deep learning\"\n",
    "test_inputs = [char_to_ix[ch] for ch in test_text]\n",
    "hprev = np.zeros((hidden_size, 1))\n",
    "\n",
    "xs, hs, ys, ps = rnn.forward(test_inputs, hprev)\n",
    "\n",
    "# 提取隐藏状态\n",
    "hidden_states = np.array([hs[t].flatten() for t in range(len(test_inputs))])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(hidden_states.T, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='激活值')\n",
    "plt.xlabel('时间步（字符位置）')\n",
    "plt.ylabel('隐藏单元')\n",
    "plt.title('RNN隐藏状态激活')\n",
    "plt.xticks(range(len(test_text)), list(test_text))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n可视化显示了RNN处理 '{test_text}' 时隐藏状态如何演化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键要点\n",
    "\n",
    "1. **字符级建模**：RNN可以学习逐字符生成文本\n",
    "2. **循环连接**：隐藏状态在时间步之间传递信息\n",
    "3. **通过时间反向传播**：梯度在序列中反向流动\n",
    "4. **梯度裁剪**：对于防止梯度爆炸至关重要\n",
    "5. **采样**：采样中的温度控制影响多样性\n",
    "\n",
    "### 不合理有效性：\n",
    "- 简单的RNN架构可以学习复杂的模式\n",
    "- 无需显式的特征工程\n",
    "- 自动学习层次化表示\n",
    "- 泛化到未见过的字符组合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
