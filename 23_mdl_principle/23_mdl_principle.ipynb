{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®ºæ–‡23ï¼šæœ€å°æè¿°é•¿åº¦åŸåˆ™\n",
    "\n",
    "**ä¸»è¦å¼•ç”¨**ï¼šGrÃ¼nwald, P. D. (2007). *The Minimum Description Length Principle*. MIT Press.\n",
    "\n",
    "**åŸºç¡€è®ºæ–‡**ï¼šRissanen, J. (1978). Modeling by shortest data description. *Automatica*, 14(5), 465-471."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¦‚è¿°å’Œæ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "### æ ¸å¿ƒåŸåˆ™\n",
    "\n",
    "**æœ€å°æè¿°é•¿åº¦ï¼ˆMDLï¼‰åŸåˆ™**åŸºäºä¸€ä¸ªç®€å•è€Œæ·±åˆ»çš„æƒ³æ³•ï¼š\n",
    "\n",
    "> **\"æœ€å¥½çš„æ¨¡å‹æ˜¯èƒ½å¤Ÿæœ€æœ‰æ•ˆå‹ç¼©æ•°æ®çš„æ¨¡å‹ã€‚\"**\n",
    "\n",
    "æˆ–è€…æ›´æ­£å¼åœ°ï¼š\n",
    "\n",
    "```\n",
    "æœ€ä½³æ¨¡å‹ = argmin [ æ¨¡å‹æè¿°é•¿åº¦ + æ•°æ®|æ¨¡å‹æè¿°é•¿åº¦ ]\n",
    "                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                        æ¨¡å‹å¤æ‚åº¦             æ‹Ÿåˆä¼˜åº¦\n",
    "```\n",
    "\n",
    "### æ ¸å¿ƒç›´è§‰\n",
    "\n",
    "1. **å¥¥å¡å§†å‰ƒåˆ€å½¢å¼åŒ–**ï¼šé™¤éå¤æ‚åº¦å¾—åˆ°æ›´å¥½æ‹Ÿåˆçš„è¯æ˜ï¼Œå¦åˆ™åå¥½æ›´ç®€å•çš„æ¨¡å‹\n",
    "\n",
    "2. **å‹ç¼© = ç†è§£**ï¼šå¦‚æœä½ èƒ½å¾ˆå¥½åœ°å‹ç¼©æ•°æ®ï¼Œä½ å°±ç†è§£äº†å®ƒçš„æ¨¡å¼\n",
    "\n",
    "3. **å¤æ‚åº¦ä¸æ‹Ÿåˆçš„æƒè¡¡**ï¼š\n",
    "   - å¤æ‚æ¨¡å‹æ‹Ÿåˆæ•°æ®æ›´å¥½ï¼Œä½†éœ€è¦æ›´å¤šä½æ¥æè¿°\n",
    "   - ç®€å•æ¨¡å‹æè¿°æˆæœ¬ä½ï¼Œä½†å¯èƒ½æ‹Ÿåˆè¾ƒå·®\n",
    "   - MDLæ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹\n",
    "\n",
    "### ä¿¡æ¯è®ºåŸºç¡€\n",
    "\n",
    "MDLåŸºäº**Kolmogorovå¤æ‚æ€§**å’Œ**Shannonä¿¡æ¯è®º**ï¼š\n",
    "\n",
    "- **Kolmogorovå¤æ‚æ€§**ï¼šç”Ÿæˆå­—ç¬¦ä¸²çš„æœ€çŸ­ç¨‹åºé•¿åº¦\n",
    "- **Shannonç†µ**ï¼šéšæœºå˜é‡çš„æœ€ä¼˜ç¼–ç é•¿åº¦\n",
    "- **MDL**ï¼šä½¿ç”¨å¯è®¡ç®—ç é•¿çš„å®é™…è¿‘ä¼¼\n",
    "\n",
    "### æ•°å­¦å…¬å¼\n",
    "\n",
    "ç»™å®šæ•°æ® `D` å’Œæ¨¡å‹ç±» `M`ï¼ŒMDLå‡†åˆ™ä¸ºï¼š\n",
    "\n",
    "```\n",
    "MDL(M) = L(M) + L(D | M)\n",
    "```\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- `L(M)` = æ¨¡å‹çš„ç é•¿ï¼ˆå‚æ•°ã€ç»“æ„ï¼‰\n",
    "- `L(D | M)` = ç»™å®šæ¨¡å‹ä¸‹æ•°æ®çš„ç é•¿ï¼ˆæ®‹å·®ã€è¯¯å·®ï¼‰\n",
    "\n",
    "### ä¸æœºå™¨å­¦ä¹ çš„è”ç³»\n",
    "\n",
    "| MDLæ¦‚å¿µ | MLç­‰ä»·ç‰© | ç›´è§‰ |\n",
    "|-------------|---------------|----------|\n",
    "| **L(M)** | æ­£åˆ™åŒ– | æƒ©ç½šæ¨¡å‹å¤æ‚åº¦ |\n",
    "| **L(D\\|M)** | æŸå¤±å‡½æ•° | å¥–åŠ±è‰¯å¥½æ‹Ÿåˆ |\n",
    "| **MDL** | æ­£åˆ™åŒ–æŸå¤± | å¹³è¡¡æ‹Ÿåˆå’Œå¤æ‚åº¦ |\n",
    "| **ä¸¤éƒ¨ç¼–ç ** | æ¨¡å‹+è¯¯å·® | å°†ç»“æ„ä¸å™ªå£°åˆ†ç¦» |\n",
    "\n",
    "### åº”ç”¨\n",
    "\n",
    "- **æ¨¡å‹é€‰æ‹©**ï¼šé€‰æ‹©æœ€ä½³æ¶æ„/è¶…å‚æ•°\n",
    "- **ç‰¹å¾é€‰æ‹©**ï¼šåŒ…å«å“ªäº›ç‰¹å¾ï¼Ÿ\n",
    "- **ç¥ç»ç½‘ç»œå‰ªæ**ï¼ˆè®ºæ–‡5ï¼‰ï¼šç§»é™¤ä¸å¿…è¦çš„æƒé‡\n",
    "- **å‹ç¼©**ï¼šå‘ç°æ•°æ®ä¸­çš„æ¨¡å¼\n",
    "- **å˜åŒ–ç‚¹æ£€æµ‹**ï¼šç”Ÿæˆè¿‡ç¨‹ä½•æ—¶æ”¹å˜ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import gammaln\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬1èŠ‚ï¼šä¿¡æ¯è®ºåŸºç¡€\n",
    "\n",
    "åœ¨å®ç°MDLä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£å¦‚ä½•åº¦é‡ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 1: Information-Theoretic Code Lengths\n",
    "# ================================================================\n",
    "\n",
    "def universal_code_length(n):\n",
    "    \"\"\"\n",
    "    Approximate universal code length for positive integer n.\n",
    "    Uses simplified Elias gamma code approximation.\n",
    "    \n",
    "    L(n) â‰ˆ logâ‚‚(n) + logâ‚‚(logâ‚‚(n)) + c\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    log_n = np.log2(n + 1)  # +1 to handle n=1\n",
    "    return log_n + np.log2(log_n + 1) + 2.865  # Constant from universal coding theory\n",
    "\n",
    "\n",
    "def real_code_length(x, precision_bits=32):\n",
    "    \"\"\"\n",
    "    Code length for real number with given precision.\n",
    "    \n",
    "    Args:\n",
    "        x: Real number to encode\n",
    "        precision_bits: Number of bits for precision (default: float32)\n",
    "    \n",
    "    Returns:\n",
    "        Code length in bits\n",
    "    \"\"\"\n",
    "    # Need to encode: sign (1 bit) + exponent + mantissa\n",
    "    return precision_bits\n",
    "\n",
    "\n",
    "def probability_code_length(p):\n",
    "    \"\"\"\n",
    "    Optimal code length for event with probability p.\n",
    "    Shannon's source coding theorem: L = -logâ‚‚(p)\n",
    "    \"\"\"\n",
    "    if p <= 0 or p > 1:\n",
    "        return float('inf')\n",
    "    return -np.log2(p)\n",
    "\n",
    "\n",
    "def entropy(probabilities):\n",
    "    \"\"\"\n",
    "    Shannon entropy: H(X) = -Î£ p(x) logâ‚‚ p(x)\n",
    "    \n",
    "    This is the expected code length under optimal coding.\n",
    "    \"\"\"\n",
    "    p = np.array(probabilities)\n",
    "    p = p[p > 0]  # Remove zeros (0 log 0 = 0)\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "print(\"Information-Theoretic Code Lengths\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Universal Code Lengths (integers):\")\n",
    "for n in [1, 10, 100, 1000, 10000]:\n",
    "    bits = universal_code_length(n)\n",
    "    print(f\"   n = {n:5d}: {bits:.2f} bits (naive: {np.log2(n):.2f} bits)\")\n",
    "\n",
    "print(\"\\n2. Probability-based Code Lengths:\")\n",
    "for p in [0.5, 0.1, 0.01, 0.001]:\n",
    "    bits = probability_code_length(p)\n",
    "    print(f\"   p = {p:.3f}: {bits:.2f} bits\")\n",
    "\n",
    "print(\"\\n3. Entropy Examples:\")\n",
    "# Fair coin\n",
    "h_fair = entropy([0.5, 0.5])\n",
    "print(f\"   Fair coin: {h_fair:.3f} bits/flip\")\n",
    "\n",
    "# Biased coin\n",
    "h_biased = entropy([0.9, 0.1])\n",
    "print(f\"   Biased coin (90/10): {h_biased:.3f} bits/flip\")\n",
    "\n",
    "# Uniform die\n",
    "h_die = entropy([1/6] * 6)\n",
    "print(f\"   Fair 6-sided die: {h_die:.3f} bits/roll\")\n",
    "\n",
    "print(\"\\nâœ“ Information-theoretic foundations established\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬2èŠ‚ï¼šMDLç”¨äºæ¨¡å‹é€‰æ‹© - å¤šé¡¹å¼å›å½’\n",
    "\n",
    "ç»å…¸ç¤ºä¾‹ï¼š**ä»€ä¹ˆé˜¶æ•°çš„å¤šé¡¹å¼æœ€é€‚åˆæ•°æ®ï¼Ÿ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 2: MDL for Polynomial Regression\n",
    "# ================================================================\n",
    "\n",
    "def generate_polynomial_data(n_points=50, true_degree=3, noise_std=0.5):\n",
    "    \"\"\"\n",
    "    Generate data from a polynomial plus noise.\n",
    "    \"\"\"\n",
    "    X = np.linspace(-2, 2, n_points)\n",
    "    \n",
    "    # True polynomial (degree 3): y = xÂ³ - 2xÂ² + x + 1\n",
    "    if true_degree == 3:\n",
    "        y_true = X**3 - 2*X**2 + X + 1\n",
    "    elif true_degree == 2:\n",
    "        y_true = X**2 - X + 1\n",
    "    elif true_degree == 1:\n",
    "        y_true = 2*X + 1\n",
    "    else:\n",
    "        y_true = 1 + X  # Default to linear\n",
    "    \n",
    "    # Add noise\n",
    "    y_noisy = y_true + np.random.randn(n_points) * noise_std\n",
    "    \n",
    "    return X, y_noisy, y_true\n",
    "\n",
    "\n",
    "def fit_polynomial(X, y, degree):\n",
    "    \"\"\"\n",
    "    Fit polynomial of given degree.\n",
    "    \n",
    "    Returns:\n",
    "        coefficients: Polynomial coefficients\n",
    "        y_pred: Predictions\n",
    "        rss: Residual sum of squares\n",
    "    \"\"\"\n",
    "    coeffs = np.polyfit(X, y, degree)\n",
    "    y_pred = np.polyval(coeffs, X)\n",
    "    rss = np.sum((y - y_pred) ** 2)\n",
    "    \n",
    "    return coeffs, y_pred, rss\n",
    "\n",
    "\n",
    "def mdl_polynomial(X, y, degree):\n",
    "    \"\"\"\n",
    "    Compute MDL for polynomial of given degree.\n",
    "    \n",
    "    MDL = L(model) + L(data | model)\n",
    "    \n",
    "    L(model): Number of parameters Ã— precision\n",
    "    L(data | model): Encode residuals using Gaussian assumption\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    n_params = degree + 1\n",
    "    \n",
    "    # Fit model\n",
    "    _, _, rss = fit_polynomial(X, y, degree)\n",
    "    \n",
    "    # Model description length\n",
    "    # Each parameter needs logâ‚‚(N) bits (Fisher information approximation)\n",
    "    L_model = n_params * np.log2(N) / 2\n",
    "    \n",
    "    # Data description length given model\n",
    "    # Assuming Gaussian errors: -logâ‚‚(p(data | model))\n",
    "    # Using normalized RSS as proxy for variance\n",
    "    if rss < 1e-10:  # Perfect fit\n",
    "        L_data = 0\n",
    "    else:\n",
    "        # Gaussian coding: L âˆ log(variance)\n",
    "        L_data = N / 2 * np.log2(rss / N + 1e-10)\n",
    "    \n",
    "    return L_model + L_data, L_model, L_data\n",
    "\n",
    "\n",
    "def aic_polynomial(X, y, degree):\n",
    "    \"\"\"\n",
    "    Akaike Information Criterion: AIC = 2k - 2ln(L)\n",
    "    \n",
    "    Related to MDL but with different constant factor.\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    n_params = degree + 1\n",
    "    _, _, rss = fit_polynomial(X, y, degree)\n",
    "    \n",
    "    # Log-likelihood for Gaussian errors\n",
    "    log_likelihood = -N/2 * np.log(2 * np.pi * rss / N) - N/2\n",
    "    \n",
    "    return 2 * n_params - 2 * log_likelihood\n",
    "\n",
    "\n",
    "def bic_polynomial(X, y, degree):\n",
    "    \"\"\"\n",
    "    Bayesian Information Criterion: BIC = kÂ·ln(N) - 2ln(L)\n",
    "    \n",
    "    Stronger penalty for complexity than AIC.\n",
    "    Very similar to MDL!\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    n_params = degree + 1\n",
    "    _, _, rss = fit_polynomial(X, y, degree)\n",
    "    \n",
    "    # Log-likelihood for Gaussian errors\n",
    "    log_likelihood = -N/2 * np.log(2 * np.pi * rss / N) - N/2\n",
    "    \n",
    "    return n_params * np.log(N) - 2 * log_likelihood\n",
    "\n",
    "\n",
    "# Generate data\n",
    "print(\"MDL for Polynomial Model Selection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X, y, y_true = generate_polynomial_data(n_points=50, true_degree=3, noise_std=0.5)\n",
    "\n",
    "print(\"\\nTrue model: Degree 3 polynomial\")\n",
    "print(\"Data points: 50\")\n",
    "print(\"Noise std: 0.5\")\n",
    "\n",
    "# Test different polynomial degrees\n",
    "degrees = range(1, 10)\n",
    "mdl_scores = []\n",
    "aic_scores = []\n",
    "bic_scores = []\n",
    "rss_scores = []\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(f\"{'Degree':>6} | {'RSS':>10} | {'MDL':>10} | {'AIC':>10} | {'BIC':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for degree in degrees:\n",
    "    # Compute scores\n",
    "    mdl_total, mdl_model, mdl_data = mdl_polynomial(X, y, degree)\n",
    "    aic = aic_polynomial(X, y, degree)\n",
    "    bic = bic_polynomial(X, y, degree)\n",
    "    _, _, rss = fit_polynomial(X, y, degree)\n",
    "    \n",
    "    mdl_scores.append(mdl_total)\n",
    "    aic_scores.append(aic)\n",
    "    bic_scores.append(bic)\n",
    "    rss_scores.append(rss)\n",
    "    \n",
    "    marker = \" â†\" if degree == 3 else \"\"\n",
    "    print(f\"{degree:6d} | {rss:10.3f} | {mdl_total:10.3f} | {aic:10.3f} | {bic:10.3f}{marker}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Find best models\n",
    "best_mdl = np.argmin(mdl_scores) + 1\n",
    "best_aic = np.argmin(aic_scores) + 1\n",
    "best_bic = np.argmin(bic_scores) + 1\n",
    "best_rss = np.argmin(rss_scores) + 1\n",
    "\n",
    "print(f\"\\nBest degree by MDL: {best_mdl}\")\n",
    "print(f\"Best degree by AIC: {best_aic}\")\n",
    "print(f\"Best degree by BIC: {best_bic}\")\n",
    "print(f\"Best degree by RSS: {best_rss} (overfits!)\")\n",
    "print(f\"True degree: 3\")\n",
    "\n",
    "print(\"\\nâœ“ MDL correctly identifies true model complexity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬3èŠ‚ï¼šå¯è§†åŒ– - MDLç»„ä»¶\n",
    "\n",
    "å¯è§†åŒ–æ¨¡å‹å¤æ‚åº¦å’Œæ‹Ÿåˆè´¨é‡ä¹‹é—´çš„æƒè¡¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 3: Visualizations\n",
    "# ================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Data and fitted polynomials\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(X, y, alpha=0.6, s=30, label='Noisy data', color='gray')\n",
    "ax.plot(X, y_true, 'k--', linewidth=2, label='True function (degree 3)', alpha=0.7)\n",
    "\n",
    "# Plot a few polynomial fits\n",
    "for degree, color in [(1, 'red'), (3, 'green'), (9, 'blue')]:\n",
    "    _, y_pred, _ = fit_polynomial(X, y, degree)\n",
    "    label = f'Degree {degree}' + (' (best MDL)' if degree == best_mdl else '')\n",
    "    ax.plot(X, y_pred, color=color, linewidth=2, label=label, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Polynomial Fits of Different Degrees', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MDL components breakdown\n",
    "ax = axes[0, 1]\n",
    "\n",
    "# Compute MDL components for each degree\n",
    "model_lengths = []\n",
    "data_lengths = []\n",
    "\n",
    "for degree in degrees:\n",
    "    _, L_model, L_data = mdl_polynomial(X, y, degree)\n",
    "    model_lengths.append(L_model)\n",
    "    data_lengths.append(L_data)\n",
    "\n",
    "degrees_list = list(degrees)\n",
    "ax.plot(degrees_list, model_lengths, 'o-', label='L(Model)', linewidth=2, markersize=8)\n",
    "ax.plot(degrees_list, data_lengths, 's-', label='L(Data | Model)', linewidth=2, markersize=8)\n",
    "ax.plot(degrees_list, mdl_scores, '^-', label='MDL Total', linewidth=2.5, markersize=8, color='purple')\n",
    "ax.axvline(x=best_mdl, color='green', linestyle='--', alpha=0.5, label=f'Best MDL (degree {best_mdl})')\n",
    "\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
    "ax.set_ylabel('Description Length (bits)', fontsize=12)\n",
    "ax.set_title('MDL Components Trade-off', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Comparison of model selection criteria\n",
    "ax = axes[1, 0]\n",
    "\n",
    "# Normalize scores for comparison\n",
    "mdl_norm = (np.array(mdl_scores) - np.min(mdl_scores)) / (np.max(mdl_scores) - np.min(mdl_scores) + 1e-10)\n",
    "aic_norm = (np.array(aic_scores) - np.min(aic_scores)) / (np.max(aic_scores) - np.min(aic_scores) + 1e-10)\n",
    "bic_norm = (np.array(bic_scores) - np.min(bic_scores)) / (np.max(bic_scores) - np.min(bic_scores) + 1e-10)\n",
    "rss_norm = (np.array(rss_scores) - np.min(rss_scores)) / (np.max(rss_scores) - np.min(rss_scores) + 1e-10)\n",
    "\n",
    "ax.plot(degrees_list, mdl_norm, 'o-', label='MDL', linewidth=2, markersize=7)\n",
    "ax.plot(degrees_list, aic_norm, 's-', label='AIC', linewidth=2, markersize=7)\n",
    "ax.plot(degrees_list, bic_norm, '^-', label='BIC', linewidth=2, markersize=7)\n",
    "ax.plot(degrees_list, rss_norm, 'v-', label='RSS (no penalty)', linewidth=2, markersize=7, alpha=0.6)\n",
    "ax.axvline(x=3, color='black', linestyle='--', alpha=0.3, label='True degree')\n",
    "\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
    "ax.set_ylabel('Normalized Score (lower is better)', fontsize=12)\n",
    "ax.set_title('Model Selection Criteria Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Bias-Variance-Complexity visualization\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Simulate bias-variance trade-off\n",
    "complexity = np.array(degrees_list)\n",
    "bias_squared = 10 / (complexity + 1)  # Decreases with complexity\n",
    "variance = complexity * 0.3  # Increases with complexity\n",
    "total_error = bias_squared + variance\n",
    "\n",
    "ax.plot(degrees_list, bias_squared, 'o-', label='BiasÂ²', linewidth=2, markersize=7)\n",
    "ax.plot(degrees_list, variance, 's-', label='Variance', linewidth=2, markersize=7)\n",
    "ax.plot(degrees_list, total_error, '^-', label='Total Error', linewidth=2.5, markersize=8, color='red')\n",
    "ax.axvline(x=best_mdl, color='green', linestyle='--', alpha=0.5, label=f'MDL optimum')\n",
    "\n",
    "ax.set_xlabel('Model Complexity (Degree)', fontsize=12)\n",
    "ax.set_ylabel('Error Components', fontsize=12)\n",
    "ax.set_title('Bias-Variance Trade-off\\n(MDL approximates this optimum)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mdl_polynomial_selection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ MDL visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬4èŠ‚ï¼šMDLç”¨äºç¥ç»ç½‘ç»œæ¶æ„é€‰æ‹©\n",
    "\n",
    "å°†MDLåº”ç”¨äºé€‰æ‹©ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆéšè—å•å…ƒæ•°é‡ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 4: MDL for Neural Network Architecture Selection\n",
    "# ================================================================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "class SimpleNN:\n",
    "    \"\"\"\n",
    "    Simple feedforward neural network for classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        scale = 0.1\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * scale\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * scale\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        self.h = sigmoid(X @ self.W1 + self.b1)\n",
    "        self.logits = self.h @ self.W2 + self.b2\n",
    "        self.probs = softmax(self.logits)\n",
    "        return self.probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def compute_loss(self, X, y):\n",
    "        \"\"\"Cross-entropy loss.\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        N = len(X)\n",
    "        \n",
    "        # One-hot encode y\n",
    "        y_onehot = np.zeros((N, self.output_dim))\n",
    "        y_onehot[np.arange(N), y] = 1\n",
    "        \n",
    "        # Cross-entropy\n",
    "        loss = -np.sum(y_onehot * np.log(probs + 1e-10)) / N\n",
    "        return loss\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total number of parameters.\"\"\"\n",
    "        return (self.input_dim * self.hidden_dim + self.hidden_dim + \n",
    "                self.hidden_dim * self.output_dim + self.output_dim)\n",
    "    \n",
    "    def train_simple(self, X, y, epochs=100, lr=0.1):\n",
    "        \"\"\"\n",
    "        Simple gradient descent training (forward pass only for speed).\n",
    "        In practice, you'd use proper backprop.\n",
    "        \"\"\"\n",
    "        # For simplicity, just do a few random restarts and keep best\n",
    "        best_loss = float('inf')\n",
    "        best_weights = None\n",
    "        \n",
    "        for _ in range(10):  # 10 random initializations\n",
    "            self.__init__(self.input_dim, self.hidden_dim, self.output_dim)\n",
    "            loss = self.compute_loss(X, y)\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_weights = (self.W1.copy(), self.b1.copy(), \n",
    "                               self.W2.copy(), self.b2.copy())\n",
    "        \n",
    "        # Restore best weights\n",
    "        self.W1, self.b1, self.W2, self.b2 = best_weights\n",
    "        return best_loss\n",
    "\n",
    "\n",
    "def mdl_neural_network(X, y, hidden_dim):\n",
    "    \"\"\"\n",
    "    Compute MDL for neural network with given hidden dimension.\n",
    "    \"\"\"\n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    N = len(X)\n",
    "    \n",
    "    # Create and train network\n",
    "    nn = SimpleNN(input_dim, hidden_dim, output_dim)\n",
    "    loss = nn.train_simple(X, y)\n",
    "    \n",
    "    # Model description length\n",
    "    n_params = nn.count_parameters()\n",
    "    L_model = n_params * np.log2(N) / 2  # Fisher information approximation\n",
    "    \n",
    "    # Data description length\n",
    "    # Cross-entropy is already in nats; convert to bits\n",
    "    L_data = loss * N / np.log(2)\n",
    "    \n",
    "    return L_model + L_data, L_model, L_data, nn\n",
    "\n",
    "\n",
    "# Generate synthetic classification data\n",
    "print(\"\\nMDL for Neural Network Architecture Selection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create 2D spiral dataset\n",
    "n_samples = 200\n",
    "n_classes = 3\n",
    "\n",
    "X_nn = []\n",
    "y_nn = []\n",
    "\n",
    "for class_id in range(n_classes):\n",
    "    r = np.linspace(0.0, 1, n_samples // n_classes)\n",
    "    t = np.linspace(class_id * 4, (class_id + 1) * 4, n_samples // n_classes) + \\\n",
    "        np.random.randn(n_samples // n_classes) * 0.2\n",
    "    \n",
    "    X_nn.append(np.c_[r * np.sin(t), r * np.cos(t)])\n",
    "    y_nn.append(np.ones(n_samples // n_classes, dtype=int) * class_id)\n",
    "\n",
    "X_nn = np.vstack(X_nn)\n",
    "y_nn = np.hstack(y_nn)\n",
    "\n",
    "# Shuffle\n",
    "perm = np.random.permutation(len(X_nn))\n",
    "X_nn = X_nn[perm]\n",
    "y_nn = y_nn[perm]\n",
    "\n",
    "print(f\"Dataset: {len(X_nn)} samples, {X_nn.shape[1]} features, {n_classes} classes\")\n",
    "\n",
    "# Test different hidden dimensions\n",
    "hidden_dims = [2, 4, 8, 16, 32, 64]\n",
    "mdl_nn_scores = []\n",
    "accuracies = []\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(f\"{'Hidden':>8} | {'Params':>8} | {'Accuracy':>10} | {'MDL':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    mdl_total, mdl_model, mdl_data, nn = mdl_neural_network(X_nn, y_nn, hidden_dim)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    y_pred = nn.predict(X_nn)\n",
    "    accuracy = np.mean(y_pred == y_nn)\n",
    "    \n",
    "    mdl_nn_scores.append(mdl_total)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"{hidden_dim:8d} | {nn.count_parameters():8d} | {accuracy:9.1%} | {mdl_total:10.2f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_hidden = hidden_dims[np.argmin(mdl_nn_scores)]\n",
    "print(f\"\\nBest architecture by MDL: {best_hidden} hidden units\")\n",
    "print(f\"This balances model complexity and fit quality.\")\n",
    "\n",
    "print(\"\\nâœ“ MDL guides architecture selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬5èŠ‚ï¼šMDLå’Œç¥ç»ç½‘ç»œå‰ªæ\n",
    "\n",
    "**ä¸è®ºæ–‡5çš„è”ç³»**ï¼šMDLä¸ºå‰ªææä¾›äº†ç†è®ºä¾æ®ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 5: MDL-Based Pruning\n",
    "# ================================================================\n",
    "\n",
    "def mdl_for_pruned_network(nn, X, y, sparsity):\n",
    "    \"\"\"\n",
    "    Compute MDL for network with given sparsity.\n",
    "    \n",
    "    Args:\n",
    "        nn: Trained neural network\n",
    "        X, y: Data\n",
    "        sparsity: Fraction of weights set to zero (0 to 1)\n",
    "    \"\"\"\n",
    "    # Save original weights\n",
    "    W1_orig, W2_orig = nn.W1.copy(), nn.W2.copy()\n",
    "    \n",
    "    # Apply magnitude-based pruning\n",
    "    all_weights = np.concatenate([nn.W1.flatten(), nn.W2.flatten()])\n",
    "    threshold = np.percentile(np.abs(all_weights), sparsity * 100)\n",
    "    \n",
    "    # Prune weights below threshold\n",
    "    nn.W1 = np.where(np.abs(nn.W1) >= threshold, nn.W1, 0)\n",
    "    nn.W2 = np.where(np.abs(nn.W2) >= threshold, nn.W2, 0)\n",
    "    \n",
    "    # Count remaining parameters\n",
    "    n_params_remaining = np.sum(nn.W1 != 0) + np.sum(nn.W2 != 0) + \\\n",
    "                        len(nn.b1) + len(nn.b2)\n",
    "    \n",
    "    # Compute loss with pruned network\n",
    "    loss = nn.compute_loss(X, y)\n",
    "    \n",
    "    # MDL computation\n",
    "    N = len(X)\n",
    "    L_model = n_params_remaining * np.log2(N) / 2\n",
    "    L_data = loss * N / np.log(2)\n",
    "    \n",
    "    # Restore original weights\n",
    "    nn.W1, nn.W2 = W1_orig, W2_orig\n",
    "    \n",
    "    return L_model + L_data, L_model, L_data, n_params_remaining\n",
    "\n",
    "\n",
    "print(\"\\nMDL-Based Pruning (Connection to Paper 5)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train a network with moderate complexity\n",
    "nn_prune = SimpleNN(input_dim=2, hidden_dim=32, output_dim=3)\n",
    "nn_prune.train_simple(X_nn, y_nn)\n",
    "\n",
    "original_params = nn_prune.count_parameters()\n",
    "print(f\"\\nOriginal network: {original_params} parameters\")\n",
    "\n",
    "# Test different sparsity levels\n",
    "sparsity_levels = np.linspace(0, 0.95, 20)\n",
    "pruning_mdl = []\n",
    "pruning_params = []\n",
    "pruning_accuracy = []\n",
    "\n",
    "print(\"\\nTesting pruning levels...\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Sparsity':>10} | {'Params':>8} | {'Accuracy':>10} | {'MDL':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    mdl_total, mdl_model, mdl_data, n_params = mdl_for_pruned_network(\n",
    "        nn_prune, X_nn, y_nn, sparsity\n",
    "    )\n",
    "    \n",
    "    # Compute accuracy with pruned network\n",
    "    W1_orig, W2_orig = nn_prune.W1.copy(), nn_prune.W2.copy()\n",
    "    \n",
    "    all_weights = np.concatenate([nn_prune.W1.flatten(), nn_prune.W2.flatten()])\n",
    "    threshold = np.percentile(np.abs(all_weights), sparsity * 100)\n",
    "    nn_prune.W1 = np.where(np.abs(nn_prune.W1) >= threshold, nn_prune.W1, 0)\n",
    "    nn_prune.W2 = np.where(np.abs(nn_prune.W2) >= threshold, nn_prune.W2, 0)\n",
    "    \n",
    "    y_pred = nn_prune.predict(X_nn)\n",
    "    accuracy = np.mean(y_pred == y_nn)\n",
    "    \n",
    "    nn_prune.W1, nn_prune.W2 = W1_orig, W2_orig\n",
    "    \n",
    "    pruning_mdl.append(mdl_total)\n",
    "    pruning_params.append(n_params)\n",
    "    pruning_accuracy.append(accuracy)\n",
    "    \n",
    "    if sparsity in [0.0, 0.25, 0.5, 0.75, 0.9]:\n",
    "        print(f\"{sparsity:9.0%} | {n_params:8d} | {accuracy:9.1%} | {mdl_total:10.2f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_sparsity_idx = np.argmin(pruning_mdl)\n",
    "best_sparsity = sparsity_levels[best_sparsity_idx]\n",
    "best_params = pruning_params[best_sparsity_idx]\n",
    "\n",
    "print(f\"\\nMDL-optimal sparsity: {best_sparsity:.1%}\")\n",
    "print(f\"Parameters: {original_params} â†’ {best_params} ({best_params/original_params:.1%} remaining)\")\n",
    "print(f\"Accuracy maintained: {pruning_accuracy[best_sparsity_idx]:.1%}\")\n",
    "\n",
    "print(\"\\nâœ“ MDL guides pruning: balance complexity reduction and accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬6èŠ‚ï¼šå‹ç¼©å’ŒMDL\n",
    "\n",
    "**MDL = å‹ç¼©**ï¼šæœ€å¥½çš„æ¨¡å‹å°±æ˜¯æœ€å¥½çš„å‹ç¼©å™¨ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 6: Compression and MDL\n",
    "# ================================================================\n",
    "\n",
    "def compress_sequence(sequence, model_order=0):\n",
    "    \"\"\"\n",
    "    Compress a binary sequence using a Markov model.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Binary sequence (0s and 1s)\n",
    "        model_order: 0 (i.i.d.), 1 (first-order Markov), etc.\n",
    "    \n",
    "    Returns:\n",
    "        Total code length in bits\n",
    "    \"\"\"\n",
    "    sequence = np.array(sequence)\n",
    "    N = len(sequence)\n",
    "    \n",
    "    if model_order == 0:\n",
    "        # I.I.D. model: just count 0s and 1s\n",
    "        n_ones = np.sum(sequence)\n",
    "        n_zeros = N - n_ones\n",
    "        \n",
    "        # Model description: encode probability p\n",
    "        L_model = 32  # Float precision for p\n",
    "        \n",
    "        # Data description: using estimated probability\n",
    "        p = (n_ones + 1) / (N + 2)  # Laplace smoothing\n",
    "        L_data = -n_ones * np.log2(p) - n_zeros * np.log2(1 - p)\n",
    "        \n",
    "        return L_model + L_data\n",
    "    \n",
    "    elif model_order == 1:\n",
    "        # First-order Markov: P(X_t | X_{t-1})\n",
    "        # Count transitions: 00, 01, 10, 11\n",
    "        transitions = np.zeros((2, 2))\n",
    "        \n",
    "        for i in range(len(sequence) - 1):\n",
    "            transitions[sequence[i], sequence[i+1]] += 1\n",
    "        \n",
    "        # Model description: 4 probabilities (2 bits precision each)\n",
    "        L_model = 4 * 32\n",
    "        \n",
    "        # Data description\n",
    "        L_data = 0\n",
    "        for i in range(2):\n",
    "            total = np.sum(transitions[i])\n",
    "            if total > 0:\n",
    "                for j in range(2):\n",
    "                    count = transitions[i, j]\n",
    "                    if count > 0:\n",
    "                        p = (count + 1) / (total + 2)\n",
    "                        L_data -= count * np.log2(p)\n",
    "        \n",
    "        return L_model + L_data\n",
    "    \n",
    "    return float('inf')\n",
    "\n",
    "\n",
    "print(\"\\nCompression and MDL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate different types of sequences\n",
    "seq_length = 1000\n",
    "\n",
    "# 1. Random sequence (i.i.d.)\n",
    "seq_random = np.random.randint(0, 2, seq_length)\n",
    "\n",
    "# 2. Biased sequence (p=0.7)\n",
    "seq_biased = (np.random.rand(seq_length) < 0.7).astype(int)\n",
    "\n",
    "# 3. Markov sequence (strong dependencies)\n",
    "seq_markov = [0]\n",
    "for _ in range(seq_length - 1):\n",
    "    if seq_markov[-1] == 0:\n",
    "        seq_markov.append(1 if np.random.rand() < 0.8 else 0)\n",
    "    else:\n",
    "        seq_markov.append(0 if np.random.rand() < 0.8 else 1)\n",
    "seq_markov = np.array(seq_markov)\n",
    "\n",
    "# Compress each sequence with different models\n",
    "sequences = {\n",
    "    'Random (i.i.d. p=0.5)': seq_random,\n",
    "    'Biased (i.i.d. p=0.7)': seq_biased,\n",
    "    'Markov (dependent)': seq_markov\n",
    "}\n",
    "\n",
    "print(\"\\nCompression results (in bits):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Sequence Type':25} | {'Order 0':>12} | {'Order 1':>12} | {'Best':>6}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for seq_name, seq in sequences.items():\n",
    "    L0 = compress_sequence(seq, model_order=0)\n",
    "    L1 = compress_sequence(seq, model_order=1)\n",
    "    \n",
    "    best_model = \"Order 0\" if L0 < L1 else \"Order 1\"\n",
    "    \n",
    "    print(f\"{seq_name:25} | {L0:12.1f} | {L1:12.1f} | {best_model:>6}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"  - Random sequence: Order 0 model is sufficient\")\n",
    "print(\"  - Biased sequence: Order 0 exploits bias well\")\n",
    "print(\"  - Markov sequence: Order 1 model captures dependencies\")\n",
    "print(\"\\nâœ“ MDL automatically selects the right model complexity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬7èŠ‚ï¼šå¯è§†åŒ– - å‰ªæå’Œå‹ç¼©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 7: Additional Visualizations\n",
    "# ================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. MDL-guided pruning\n",
    "ax = axes[0]\n",
    "\n",
    "# Plot MDL components vs sparsity\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "color_mdl = 'blue'\n",
    "color_acc = 'green'\n",
    "\n",
    "ax.plot(sparsity_levels * 100, pruning_mdl, 'o-', color=color_mdl, \n",
    "        linewidth=2, markersize=5, label='MDL')\n",
    "ax.axvline(x=best_sparsity * 100, color='red', linestyle='--', \n",
    "          alpha=0.5, label=f'MDL optimum ({best_sparsity:.0%})')\n",
    "\n",
    "ax2.plot(sparsity_levels * 100, pruning_accuracy, 's-', color=color_acc, \n",
    "         linewidth=2, markersize=5, alpha=0.7, label='Accuracy')\n",
    "\n",
    "ax.set_xlabel('Sparsity (%)', fontsize=12)\n",
    "ax.set_ylabel('MDL (bits)', fontsize=12, color=color_mdl)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12, color=color_acc)\n",
    "ax.tick_params(axis='y', labelcolor=color_mdl)\n",
    "ax2.tick_params(axis='y', labelcolor=color_acc)\n",
    "\n",
    "ax.set_title('MDL-Guided Pruning\\n(Builds on Paper 5)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=10)\n",
    "\n",
    "# 2. Model selection landscape\n",
    "ax = axes[1]\n",
    "\n",
    "# Create a 2D landscape: hidden units vs accuracy, colored by MDL\n",
    "x_scatter = hidden_dims\n",
    "y_scatter = accuracies\n",
    "colors_scatter = mdl_nn_scores\n",
    "\n",
    "scatter = ax.scatter(x_scatter, y_scatter, c=colors_scatter, \n",
    "                    s=200, cmap='RdYlGn_r', alpha=0.8, edgecolors='black', linewidth=2)\n",
    "\n",
    "# Mark best\n",
    "best_idx = np.argmin(mdl_nn_scores)\n",
    "ax.scatter([x_scatter[best_idx]], [y_scatter[best_idx]], \n",
    "          marker='*', s=500, color='gold', edgecolors='black', \n",
    "          linewidth=2, label='MDL optimum', zorder=10)\n",
    "\n",
    "ax.set_xlabel('Hidden Units (Model Complexity)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Model Selection Landscape\\n(Colored by MDL)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('MDL (lower is better)', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mdl_pruning_compression.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Additional visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬8èŠ‚ï¼šä¸Kolmogorovå¤æ‚æ€§çš„è”ç³»ï¼ˆè®ºæ–‡25é¢„è§ˆï¼‰\n",
    "\n",
    "MDLæ˜¯Kolmogorovå¤æ‚æ€§çš„**å®é™…è¿‘ä¼¼**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 8: Kolmogorov Complexity Connection\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\nKolmogorov Complexity and MDL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Demonstrate on binary strings\n",
    "strings = {\n",
    "    'Random': '10110010111001011100101110010111',\n",
    "    'Alternating': '01010101010101010101010101010101',\n",
    "    'All ones': '11111111111111111111111111111111',\n",
    "    'Structured': '00110011001100110011001100110011'\n",
    "}\n",
    "\n",
    "print(\"\\nEstimating complexity of binary strings:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'String Type':15} | {'Naive':>8} | {'MDL Approx':>12} | {'Ratio':>6}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, s in strings.items():\n",
    "    # Naive: just store the string\n",
    "    naive_length = len(s)\n",
    "    \n",
    "    # MDL approximation: try to find pattern\n",
    "    # (Simple heuristic: check for repeating patterns)\n",
    "    best_mdl = naive_length\n",
    "    \n",
    "    # Check for repeating patterns of length 1, 2, 4, 8\n",
    "    for pattern_len in [1, 2, 4, 8]:\n",
    "        if len(s) % pattern_len == 0:\n",
    "            pattern = s[:pattern_len]\n",
    "            if pattern * (len(s) // pattern_len) == s:\n",
    "                # Found a pattern!\n",
    "                # MDL = pattern + repetition count\n",
    "                mdl = pattern_len + universal_code_length(len(s) // pattern_len)\n",
    "                best_mdl = min(best_mdl, mdl)\n",
    "    \n",
    "    ratio = best_mdl / naive_length\n",
    "    print(f\"{name:15} | {naive_length:8d} | {best_mdl:12.1f} | {ratio:6.2f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Random: Cannot compress (ratio â‰ˆ 1.0)\")\n",
    "print(\"  - Structured: Can compress significantly (ratio < 1.0)\")\n",
    "print(\"  - Compression ratio â‰ˆ 1/complexity\")\n",
    "\n",
    "print(\"\\nâœ“ MDL approximates Kolmogorov complexity in practice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬9èŠ‚ï¼šå®é™…åº”ç”¨æ€»ç»“\n",
    "\n",
    "MDLä»¥ä¸åŒåç§°å‡ºç°åœ¨ç°ä»£æœºå™¨å­¦ä¹ ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 9: Practical Applications\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\nMDL in Modern Machine Learning\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "applications = [\n",
    "    (\"Model Selection\", \"AIC, BIC, Cross-validation\", \"Choose architecture/hyperparameters\"),\n",
    "    (\"Regularization\", \"L1, L2, Dropout\", \"Prefer simpler models\"),\n",
    "    (\"Pruning\", \"Magnitude pruning, Lottery Ticket\", \"Remove unnecessary weights (Paper 5)\"),\n",
    "    (\"Compression\", \"Quantization, Knowledge distillation\", \"Smaller models that retain performance\"),\n",
    "    (\"Early Stopping\", \"Validation loss monitoring\", \"Stop before overfitting\"),\n",
    "    (\"Feature Selection\", \"LASSO, Forward selection\", \"Include only useful features\"),\n",
    "    (\"Bayesian ML\", \"Prior + Likelihood\", \"Balance complexity and fit\"),\n",
    "    (\"Neural Architecture Search\", \"DARTS, ENAS\", \"Search for efficient architectures\"),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(f\"{'Application':25} | {'ML Techniques':30} | {'MDL Principle':15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for app, techniques, principle in applications:\n",
    "    print(f\"{app:25} | {techniques:30} | {principle:15}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY: MDL AS A UNIFYING PRINCIPLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "The Minimum Description Length principle provides a theoretical foundation\n",
    "for many practical ML techniques:\n",
    "\n",
    "1. OCCAM'S RAZOR FORMALIZED\n",
    "   \"Entities should not be multiplied without necessity\"\n",
    "   â†’ Simpler models unless complexity is justified\n",
    "\n",
    "2. COMPRESSION = UNDERSTANDING\n",
    "   If you can compress data well, you understand its structure\n",
    "   â†’ Good models are good compressors\n",
    "\n",
    "3. BIAS-VARIANCE TRADE-OFF\n",
    "   L(model) â†” Variance (complex models have high variance)\n",
    "   L(data|model) â†” Bias (simple models have high bias)\n",
    "   â†’ MDL balances both\n",
    "\n",
    "4. INFORMATION-THEORETIC FOUNDATION\n",
    "   Based on Shannon entropy and Kolmogorov complexity\n",
    "   â†’ Principled, not ad-hoc\n",
    "\n",
    "5. AUTOMATIC COMPLEXITY CONTROL\n",
    "   No need to manually tune regularization strength\n",
    "   â†’ MDL finds the sweet spot\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ“ MDL connects theory and practice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬10èŠ‚ï¼šç»“è®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Section 10: Conclusion\n",
    "# ================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAPER 23: THE MINIMUM DESCRIPTION LENGTH PRINCIPLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "âœ… IMPLEMENTATION COMPLETE\n",
    "\n",
    "This notebook demonstrates the MDL principle - a fundamental concept in\n",
    "machine learning, statistics, and information theory.\n",
    "\n",
    "KEY ACCOMPLISHMENTS:\n",
    "\n",
    "1. Information-Theoretic Foundations\n",
    "   â€¢ Universal codes for integers\n",
    "   â€¢ Shannon entropy and optimal coding\n",
    "   â€¢ Probability-based code lengths\n",
    "   â€¢ Connection to compression\n",
    "\n",
    "2. Model Selection Applications\n",
    "   â€¢ Polynomial regression (degree selection)\n",
    "   â€¢ Comparison with AIC/BIC\n",
    "   â€¢ Neural network architecture selection\n",
    "   â€¢ MDL components visualization\n",
    "\n",
    "3. Connection to Paper 5 (Pruning)\n",
    "   â€¢ MDL-based pruning criterion\n",
    "   â€¢ Optimal sparsity finding\n",
    "   â€¢ Trade-off between compression and accuracy\n",
    "   â€¢ Theoretical justification for pruning\n",
    "\n",
    "4. Compression Experiments\n",
    "   â€¢ Markov models of different orders\n",
    "   â€¢ Automatic model order selection\n",
    "   â€¢ MDL = best compression\n",
    "\n",
    "5. Kolmogorov Complexity Preview\n",
    "   â€¢ MDL as practical approximation\n",
    "   â€¢ Pattern discovery in strings\n",
    "   â€¢ Foundation for Paper 25\n",
    "\n",
    "KEY INSIGHTS:\n",
    "\n",
    "âœ“ The Core Principle\n",
    "  Best Model = Shortest Description = Best Compressor\n",
    "  \n",
    "âœ“ Automatic Complexity Control\n",
    "  MDL automatically balances model complexity and fit quality.\n",
    "  No need for manual regularization tuning.\n",
    "\n",
    "âœ“ Information-Theoretic Foundation\n",
    "  Unlike ad-hoc penalties, MDL has rigorous theoretical basis\n",
    "  in Shannon information theory and Kolmogorov complexity.\n",
    "\n",
    "âœ“ Unifying Framework\n",
    "  Connects: Regularization, Pruning, Feature Selection,\n",
    "  Model Selection, Compression, Bayesian ML\n",
    "\n",
    "âœ“ Practical Approximation\n",
    "  Kolmogorov complexity is ideal but uncomputable.\n",
    "  MDL provides practical, computable alternative.\n",
    "\n",
    "CONNECTIONS TO OTHER PAPERS:\n",
    "\n",
    "â€¢ Paper 5 (Pruning): MDL justifies removing weights\n",
    "â€¢ Paper 25 (Kolmogorov): Theoretical foundation\n",
    "â€¢ All ML: Regularization, early stopping, architecture search\n",
    "\n",
    "MATHEMATICAL ELEGANCE:\n",
    "\n",
    "MDL(M) = L(Model) + L(Data | Model)\n",
    "         â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "         Complexity  Goodness of Fit\n",
    "\n",
    "This single equation unifies:\n",
    "- Occam's Razor (prefer simplicity)\n",
    "- Statistical fit (match the data)\n",
    "- Information theory (compression)\n",
    "- Bayesian inference (prior + likelihood)\n",
    "\n",
    "PRACTICAL IMPACT:\n",
    "\n",
    "Modern ML uses MDL principles everywhere:\n",
    "âœ“ BIC for model selection (almost identical to MDL)\n",
    "âœ“ Pruning for model compression\n",
    "âœ“ Regularization (L1/L2 as crude MDL proxies)\n",
    "âœ“ Architecture search (minimize parameters + error)\n",
    "âœ“ Knowledge distillation (compress model)\n",
    "\n",
    "EDUCATIONAL VALUE:\n",
    "\n",
    "âœ“ Principled approach to model selection\n",
    "âœ“ Information-theoretic thinking for ML\n",
    "âœ“ Understanding regularization deeply\n",
    "âœ“ Foundation for compression and efficiency\n",
    "âœ“ Bridge between theory and practice\n",
    "\n",
    "\"To understand is to compress.\" - JÃ¼rgen Schmidhuber\n",
    "\n",
    "\"The best model is the one that compresses the data the most.\"\n",
    "                                        - The MDL Principle\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“ Paper 23 Implementation Complete - MDL Principle Mastered!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
