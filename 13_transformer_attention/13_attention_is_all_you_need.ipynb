{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文13：Attention Is All You Need\n",
    "## Vaswani et al. (2017)\n",
    "\n",
    "### Transformer：纯注意力架构\n",
    "\n",
    "用自注意力机制取代 RNN 的革命性架构，为现代 LLM 奠定了基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缩放点积注意力\n",
    "\n",
    "基础构建块：\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"数值稳定的 softmax\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    缩放点积注意力\n",
    "    \n",
    "    Q: Queries (seq_len_q, d_k)\n",
    "    K: Keys (seq_len_k, d_k)\n",
    "    V: Values (seq_len_v, d_v)\n",
    "    mask: 可选的掩码 (seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # 计算注意力分数\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # 如果提供了掩码则应用（用于因果性或填充）\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    \n",
    "    # Softmax 得到注意力权重\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # 值的加权和\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 测试缩放点积注意力\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Attention weights sum (should be 1): {attn_weights.sum(axis=1)}\")\n",
    "\n",
    "# 可视化注意力模式\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attn_weights, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Attention Weights Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力\n",
    "\n",
    "多个注意力\"头\"关注输入的不同方面：\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(head_1, ..., head_h)W^O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # 所有头的 Q, K, V 线性投影（并行化）\n",
    "        self.W_q = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.1\n",
    "        \n",
    "        # 输出投影\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.1\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"分割为多个头: (seq_len, d_model) -> (num_heads, seq_len, d_k)\"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 0, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"合并头: (num_heads, seq_len, d_k) -> (seq_len, d_model)\"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        x = x.transpose(1, 0, 2)\n",
    "        return x.reshape(seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        多头注意力前向传播\n",
    "        \n",
    "        Q, K, V: (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 线性投影\n",
    "        Q = np.dot(Q, self.W_q.T)\n",
    "        K = np.dot(K, self.W_k.T)\n",
    "        V = np.dot(V, self.W_v.T)\n",
    "        \n",
    "        # 分割为多个头\n",
    "        Q = self.split_heads(Q)  # (num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # 对每个头应用注意力\n",
    "        head_outputs = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            head_out, head_attn = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            head_outputs.append(head_out)\n",
    "            self.attention_weights.append(head_attn)\n",
    "        \n",
    "        # 堆叠头\n",
    "        heads = np.stack(head_outputs, axis=0)  # (num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 合并头\n",
    "        combined = self.combine_heads(heads)  # (seq_len, d_model)\n",
    "        \n",
    "        # 最终线性投影\n",
    "        output = np.dot(combined, self.W_o.T)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 测试多头注意力\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "output = mha.forward(X, X, X)  # Self-attention\n",
    "\n",
    "print(f\"\\nMulti-Head Attention:\")\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Dimension per head: {mha.d_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "\n",
    "由于 Transformer 没有循环结构，我们需要添加位置信息：\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    创建正弦位置编码\n",
    "    \"\"\"\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    position = np.arange(0, seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    # 对偶数索引应用 sin\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    \n",
    "    # 对奇数索引应用 cos\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# 生成位置编码\n",
    "seq_len = 50\n",
    "d_model = 64\n",
    "pe = positional_encoding(seq_len, d_model)\n",
    "\n",
    "# 可视化位置编码\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(pe.T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Positional Encoding (All Dimensions)')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "# 绘制前几个维度\n",
    "for i in [0, 1, 2, 3, 10, 20]:\n",
    "    plt.plot(pe[:, i], label=f'Dim {i}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Value')\n",
    "plt.title('Positional Encoding (Selected Dimensions)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Positional encoding shape: {pe.shape}\")\n",
    "print(f\"不同频率在不同尺度上编码位置信息\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前馈网络\n",
    "\n",
    "独立应用于每个位置：\n",
    "$$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        self.W1 = np.random.randn(d_model, d_ff) * 0.1\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) * 0.1\n",
    "        self.b2 = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 第一层使用 ReLU\n",
    "        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        \n",
    "        # 第二层\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 测试前馈网络\n",
    "d_model = 64\n",
    "d_ff = 256  # 通常是 4 倍大小\n",
    "\n",
    "ff = FeedForward(d_model, d_ff)\n",
    "x = np.random.randn(10, d_model)\n",
    "output = ff.forward(x)\n",
    "\n",
    "print(f\"\\nFeed-Forward Network:\")\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Hidden: ({x.shape[0]}, {d_ff})\")\n",
    "print(f\"Output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 层归一化\n",
    "\n",
    "在特征维度上进行归一化（与 BatchNorm 不同，不是按批次）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        \n",
    "        normalized = (x - mean) / (std + self.eps)\n",
    "        output = self.gamma * normalized + self.beta\n",
    "        \n",
    "        return output\n",
    "\n",
    "ln = LayerNorm(d_model)\n",
    "x = np.random.randn(10, d_model) * 3 + 5  # 未归一化\n",
    "normalized = ln.forward(x)\n",
    "\n",
    "print(f\"\\nLayer Normalization:\")\n",
    "print(f\"Input mean: {x.mean():.4f}, std: {x.std():.4f}\")\n",
    "print(f\"Output mean: {normalized.mean():.4f}, std: {normalized.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的 Transformer 块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 带残差连接的多头注意力\n",
    "        attn_output = self.attention.forward(x, x, x, mask)\n",
    "        x = self.norm1.forward(x + attn_output)\n",
    "        \n",
    "        # 带残差连接的前馈网络\n",
    "        ff_output = self.ff.forward(x)\n",
    "        x = self.norm2.forward(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 测试 transformer 块\n",
    "block = TransformerBlock(d_model=64, num_heads=8, d_ff=256)\n",
    "x = np.random.randn(10, 64)\n",
    "output = block.forward(x)\n",
    "\n",
    "print(f\"\\nTransformer Block:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nBlock contains:\")\n",
    "print(f\"  1. Multi-Head Self-Attention\")\n",
    "print(f\"  2. Layer Normalization\")\n",
    "print(f\"  3. Feed-Forward Network\")\n",
    "print(f\"  4. Residual Connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化多头注意力模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建可解释输入的注意力\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "output = mha.forward(X, X, X)\n",
    "\n",
    "# 绘制每个头的注意力模式\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    attn = mha.attention_weights[i]\n",
    "    im = ax.imshow(attn, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {i+1}')\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "    \n",
    "plt.colorbar(im, ax=axes, label='Attention Weight', fraction=0.046, pad=0.04)\n",
    "plt.suptitle('Multi-Head Attention Patterns', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n每个头学习关注不同的模式！\")\n",
    "print(\"不同的头捕获数据中的不同关系。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自回归模型的因果（掩码）自注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"创建掩码以防止关注未来位置\"\"\"\n",
    "    mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "    return mask\n",
    "\n",
    "# 测试因果注意力\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# 无掩码（双向）\n",
    "output_bi, attn_bi = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# 带因果掩码（单向）\n",
    "output_causal, attn_causal = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "# 可视化差异\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 因果掩码\n",
    "ax1.imshow(causal_mask, cmap='Reds', aspect='auto')\n",
    "ax1.set_title('Causal Mask\\n(1 = masked/not allowed)')\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "\n",
    "# 双向注意力\n",
    "im2 = ax2.imshow(attn_bi, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_title('Bidirectional Attention\\n(can see future)')\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "\n",
    "# 因果注意力\n",
    "im3 = ax3.imshow(attn_causal, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "ax3.set_title('Causal Attention\\n(cannot see future)')\n",
    "ax3.set_xlabel('Key Position')\n",
    "ax3.set_ylabel('Query Position')\n",
    "\n",
    "plt.colorbar(im3, ax=[ax2, ax3], label='Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n因果掩码对于以下情况至关重要：\")\n",
    "print(\"  - 自回归生成（GPT、语言模型）\")\n",
    "print(\"  - 防止来自未来 token 的信息泄露\")\n",
    "print(\"  - 每个位置只能关注自身和之前的位置\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键要点\n",
    "\n",
    "### 为什么\"Attention Is All You Need\"？\n",
    "- **无循环结构**：并行处理整个序列\n",
    "- **无卷积**：纯注意力机制\n",
    "- **更好的扩展性**：O(n²d) vs RNN 的 O(n) 顺序操作\n",
    "- **长距离依赖**：任意位置之间的直接连接\n",
    "\n",
    "### 核心组件：\n",
    "1. **缩放点积注意力**：高效的注意力计算\n",
    "2. **多头注意力**：多个表示子空间\n",
    "3. **位置编码**：注入位置信息\n",
    "4. **前馈网络**：逐位置变换\n",
    "5. **层归一化**：稳定训练\n",
    "6. **残差连接**：使深度网络成为可能\n",
    "\n",
    "### 架构变体：\n",
    "- **Encoder-Decoder**：原始 Transformer（翻译）\n",
    "- **仅 Encoder**：BERT（双向理解）\n",
    "- **仅 Decoder**：GPT（自回归生成）\n",
    "\n",
    "### 优势：\n",
    "- 可并行化训练（与 RNN 不同）\n",
    "- 更好的长距离依赖\n",
    "- 可解释的注意力模式\n",
    "- 在多项任务上达到最先进水平\n",
    "\n",
    "### 影响：\n",
    "- 现代 NLP 的基础：GPT、BERT、T5 等\n",
    "- 扩展到视觉：Vision Transformer (ViT)\n",
    "- 多模态模型：CLIP、Flamingo\n",
    "- 使具有数十亿参数的 LLM 成为可能"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
