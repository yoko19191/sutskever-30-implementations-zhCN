{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文30：迷失在中间：语言模型如何使用长上下文\n",
    "## Nelson F. Liu, Kevin Lin, John Hewitt, et al., Stanford & UW (2023)\n",
    "\n",
    "### \"迷失在中间\"现象\n",
    "\n",
    "语言模型难以使用长上下文中间部分的信息。性能呈现 U 型曲线！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模拟多文档 QA 任务\n",
    "\n",
    "**设置**： \n",
    "- 查询需要来自一个文档的信息\n",
    "- 提供多个文档（1 个相关，其余为干扰项）\n",
    "- **问题**：相关文档的位置是否重要？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content, is_relevant=False):\n",
    "        self.content = content\n",
    "        self.is_relevant = is_relevant\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Doc(relevant={self.is_relevant}): {self.content[:50]}...\"\n",
    "\n",
    "# Create synthetic documents\n",
    "relevant_doc = Document(\n",
    "    \"The Eiffel Tower was completed in 1889 and stands 330 meters tall. \"\n",
    "    \"It was designed by Gustave Eiffel for the 1889 World's Fair in Paris.\",\n",
    "    is_relevant=True\n",
    ")\n",
    "\n",
    "distractor_docs = [\n",
    "    Document(\"The Great Wall of China is over 13,000 miles long and was built over many centuries.\"),\n",
    "    Document(\"The Statue of Liberty was gifted by France to the United States in 1886.\"),\n",
    "    Document(\"Mount Everest is the tallest mountain on Earth at 8,849 meters above sea level.\"),\n",
    "    Document(\"The Amazon River is the largest river by discharge volume in the world.\"),\n",
    "    Document(\"The Sahara Desert is the largest hot desert, covering much of North Africa.\"),\n",
    "    Document(\"The Colosseum in Rome was completed in 80 AD and could hold 50,000 spectators.\"),\n",
    "    Document(\"The Taj Mahal in India was built between 1632 and 1653 as a mausoleum.\"),\n",
    "    Document(\"The Grand Canyon in Arizona is 277 miles long and up to 18 miles wide.\"),\n",
    "    Document(\"The Great Barrier Reef is the world's largest coral reef system.\"),\n",
    "]\n",
    "\n",
    "query = \"When was the Eiffel Tower completed?\"\n",
    "correct_answer = \"1889\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Correct answer: {correct_answer}\")\n",
    "print(f\"\\nRelevant document: {relevant_doc.content}\")\n",
    "print(f\"\\nNumber of distractor documents: {len(distractor_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简化的语言模型\n",
    "\n",
    "模拟带位置偏差的基于注意力的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLM:\n",
    "    \"\"\"带位置偏差的简化 LM\"\"\"\n",
    "    def __init__(self, position_bias_type='u_shaped'):\n",
    "        \"\"\"\n",
    "        position_bias_type:\n",
    "        - 'uniform': 对所有位置给予相等的注意力\n",
    "        - 'u_shaped': 在开始/结束处高，中间低\n",
    "        - 'recency': 偏好最近（结束）位置\n",
    "        - 'primacy': 偏好早期（开始）位置\n",
    "        \"\"\"\n",
    "        self.position_bias_type = position_bias_type\n",
    "    \n",
    "    def get_position_weights(self, num_positions):\n",
    "        \"\"\"计算基于位置的注意力权重\"\"\"\n",
    "        positions = np.arange(num_positions)\n",
    "        \n",
    "        if self.position_bias_type == 'uniform':\n",
    "            weights = np.ones(num_positions)\n",
    "        \n",
    "        elif self.position_bias_type == 'u_shaped':\n",
    "            # U 型：边缘高，中间低\n",
    "            normalized_pos = positions / (num_positions - 1)  # 0 到 1\n",
    "            # 在 0.5 处有最小值的二次函数\n",
    "            weights = 4 * (normalized_pos - 0.5) ** 2 + 0.3\n",
    "        \n",
    "        elif self.position_bias_type == 'recency':\n",
    "            # 向开始处指数衰减\n",
    "            weights = np.exp(positions * 0.2)\n",
    "        \n",
    "        elif self.position_bias_type == 'primacy':\n",
    "            # 向结束处指数衰减\n",
    "            weights = np.exp(-positions * 0.2)\n",
    "        \n",
    "        # 归一化\n",
    "        weights = weights / np.sum(weights)\n",
    "        return weights\n",
    "    \n",
    "    def answer_query(self, query, documents):\n",
    "        \"\"\"\n",
    "        模拟使用文档回答查询\n",
    "        返回：找到正确答案的概率\n",
    "        \"\"\"\n",
    "        num_docs = len(documents)\n",
    "        \n",
    "        # 获取位置权重\n",
    "        position_weights = self.get_position_weights(num_docs)\n",
    "        \n",
    "        # 找到相关文档位置\n",
    "        relevant_position = None\n",
    "        for i, doc in enumerate(documents):\n",
    "            if doc.is_relevant:\n",
    "                relevant_position = i\n",
    "                break\n",
    "        \n",
    "        if relevant_position is None:\n",
    "            return 0.0  # 没有相关文档\n",
    "        \n",
    "        # 使用相关文档的概率\n",
    "        # 权重越高 → 越可能使用该文档\n",
    "        prob_correct = position_weights[relevant_position]\n",
    "        \n",
    "        return prob_correct\n",
    "\n",
    "# 测试不同的偏差类型\n",
    "num_docs = 10\n",
    "test_positions = np.arange(num_docs)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "bias_types = ['uniform', 'u_shaped', 'recency', 'primacy']\n",
    "for ax, bias_type in zip(axes, bias_types):\n",
    "    model = SimpleLM(position_bias_type=bias_type)\n",
    "    weights = model.get_position_weights(num_docs)\n",
    "    \n",
    "    ax.bar(test_positions, weights, color='steelblue', edgecolor='black')\n",
    "    ax.set_xlabel('Document Position', fontsize=11)\n",
    "    ax.set_ylabel('Attention Weight', fontsize=11)\n",
    "    ax.set_title(f'{bias_type.replace(\"_\", \" \").title()} Bias', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0, max(weights) * 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n真实的 LLM 显示 U 型偏差（开始/结束处高，中间低）！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试位置敏感性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_positions(model, query, relevant_doc, distractor_docs):\n",
    "    \"\"\"\n",
    "    Test performance with relevant document at each position\n",
    "    \"\"\"\n",
    "    num_positions = len(distractor_docs) + 1\n",
    "    accuracies = []\n",
    "    \n",
    "    for pos in range(num_positions):\n",
    "        # Create document list with relevant doc at position 'pos'\n",
    "        docs = distractor_docs[:pos] + [relevant_doc] + distractor_docs[pos:]\n",
    "        docs = docs[:num_positions]  # Keep fixed length\n",
    "        \n",
    "        # Get model's probability of answering correctly\n",
    "        prob_correct = model.answer_query(query, docs)\n",
    "        accuracies.append(prob_correct)\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Test U-shaped bias (realistic)\n",
    "model_realistic = SimpleLM(position_bias_type='u_shaped')\n",
    "accuracies_realistic = test_all_positions(model_realistic, query, relevant_doc, distractor_docs)\n",
    "\n",
    "# Test uniform (ideal)\n",
    "model_ideal = SimpleLM(position_bias_type='uniform')\n",
    "accuracies_ideal = test_all_positions(model_ideal, query, relevant_doc, distractor_docs)\n",
    "\n",
    "# Plot\n",
    "positions = np.arange(len(accuracies_realistic))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(positions, accuracies_realistic, 'o-', linewidth=3, markersize=10, \n",
    "        label='Realistic (U-shaped bias)', color='crimson')\n",
    "plt.plot(positions, accuracies_ideal, 's--', linewidth=2, markersize=8, \n",
    "        label='Ideal (No bias)', color='green', alpha=0.6)\n",
    "\n",
    "# Mark beginning and end\n",
    "plt.axvline(x=0, color='blue', linestyle=':', alpha=0.5, linewidth=2, label='Beginning')\n",
    "plt.axvline(x=len(positions)-1, color='purple', linestyle=':', alpha=0.5, linewidth=2, label='End')\n",
    "\n",
    "# Mark middle region\n",
    "middle_start = len(positions) // 4\n",
    "middle_end = 3 * len(positions) // 4\n",
    "plt.axvspan(middle_start, middle_end, alpha=0.2, color='red', label='Middle (worst)')\n",
    "\n",
    "plt.xlabel('Position of Relevant Document', fontsize=13)\n",
    "plt.ylabel('Accuracy', fontsize=13)\n",
    "plt.title('Lost in the Middle: Performance vs Position', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "beginning_acc = accuracies_realistic[0]\n",
    "middle_acc = np.mean(accuracies_realistic[middle_start:middle_end])\n",
    "end_acc = accuracies_realistic[-1]\n",
    "\n",
    "print(f\"\\nPerformance Analysis:\")\n",
    "print(f\"Beginning (pos 0): {beginning_acc:.1%}\")\n",
    "print(f\"Middle (pos {middle_start}-{middle_end}): {middle_acc:.1%}\")\n",
    "print(f\"End (pos {len(positions)-1}): {end_acc:.1%}\")\n",
    "print(f\"\\nMiddle penalty: -{(beginning_acc - middle_acc)/beginning_acc:.1%} relative to beginning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上下文长度的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_varying_lengths(model, query, relevant_doc, distractor_docs, lengths):\n",
    "    \"\"\"\n",
    "    测试性能如何随上下文长度变化\n",
    "    \"\"\"\n",
    "    results = {'beginning': [], 'middle': [], 'end': []}\n",
    "    \n",
    "    for length in lengths:\n",
    "        # 使用干扰项的子集\n",
    "        current_distractors = distractor_docs[:length-1]\n",
    "        \n",
    "        # 测试三个位置：开始、中间、结束\n",
    "        positions = {\n",
    "            'beginning': 0,\n",
    "            'middle': length // 2,\n",
    "            'end': length - 1\n",
    "        }\n",
    "        \n",
    "        for pos_name, pos in positions.items():\n",
    "            docs = current_distractors[:pos] + [relevant_doc] + current_distractors[pos:]\n",
    "            docs = docs[:length]\n",
    "            \n",
    "            acc = model.answer_query(query, docs)\n",
    "            results[pos_name].append(acc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 测试不同的上下文长度\n",
    "lengths = [3, 5, 7, 9, 10]\n",
    "results = test_varying_lengths(model_realistic, query, relevant_doc, distractor_docs, lengths)\n",
    "\n",
    "# 绘制\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(lengths, results['beginning'], 'o-', linewidth=3, markersize=10, \n",
    "        label='Beginning', color='blue')\n",
    "plt.plot(lengths, results['middle'], 's-', linewidth=3, markersize=10, \n",
    "        label='Middle', color='red')\n",
    "plt.plot(lengths, results['end'], '^-', linewidth=3, markersize=10, \n",
    "        label='End', color='purple')\n",
    "\n",
    "plt.xlabel('Number of Documents', fontsize=13)\n",
    "plt.ylabel('Accuracy', fontsize=13)\n",
    "plt.title('Performance Degradation with Context Length', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n更长的上下文 → 更差的性能（尤其是在中间！）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG 的排序策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_documents(documents, relevance_scores, strategy='default'):\n",
    "    \"\"\"\n",
    "    根据策略对文档进行排序\n",
    "    \n",
    "    策略：\n",
    "    - 'default': 保持检索顺序\n",
    "    - 'most_relevant_first': 将最佳文档放在开始\n",
    "    - 'most_relevant_edges': 将最佳文档放在开始和结束\n",
    "    - 'reverse': 反转检索顺序\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(documents))\n",
    "    \n",
    "    if strategy == 'default':\n",
    "        return documents\n",
    "    \n",
    "    elif strategy == 'most_relevant_first':\n",
    "        # 按相关性排序（降序）\n",
    "        sorted_indices = np.argsort(relevance_scores)[::-1]\n",
    "        return [documents[i] for i in sorted_indices]\n",
    "    \n",
    "    elif strategy == 'most_relevant_edges':\n",
    "        # 将最相关的放在开始和结束\n",
    "        sorted_indices = np.argsort(relevance_scores)[::-1]\n",
    "        \n",
    "        # 交错：最佳在边缘，最差在中间\n",
    "        ordered = []\n",
    "        for i in range(len(documents) // 2):\n",
    "            ordered.append(documents[sorted_indices[i]])  # 高相关性\n",
    "        for i in range(len(documents) // 2, len(documents)):\n",
    "            ordered.append(documents[sorted_indices[i]])  # 低相关性\n",
    "        \n",
    "        # 反转后半部分以将高相关性放在末尾\n",
    "        mid = len(ordered) // 2\n",
    "        return ordered[:mid] + ordered[mid:][::-1]\n",
    "    \n",
    "    elif strategy == 'reverse':\n",
    "        return documents[::-1]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Simulate retrieval scores\n",
    "num_test_docs = 10\n",
    "test_docs = [relevant_doc] + distractor_docs[:num_test_docs-1]\n",
    "\n",
    "# Relevance scores (relevant doc gets high score)\n",
    "relevance_scores = np.random.rand(num_test_docs) * 0.5\n",
    "relevance_scores[0] = 0.95  # Relevant doc has high score\n",
    "\n",
    "# Shuffle to simulate retrieval\n",
    "shuffle_idx = np.random.permutation(num_test_docs)\n",
    "test_docs = [test_docs[i] for i in shuffle_idx]\n",
    "relevance_scores = relevance_scores[shuffle_idx]\n",
    "\n",
    "# Test different strategies\n",
    "strategies = ['default', 'most_relevant_first', 'most_relevant_edges']\n",
    "strategy_accuracies = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    ordered = order_documents(test_docs, relevance_scores, strategy)\n",
    "    acc = model_realistic.answer_query(query, ordered)\n",
    "    strategy_accuracies[strategy] = acc\n",
    "    \n",
    "    # Find position of relevant doc\n",
    "    rel_pos = next(i for i, doc in enumerate(ordered) if doc.is_relevant)\n",
    "    print(f\"\\n{strategy:25s}: Relevant doc at position {rel_pos:2d}, Accuracy: {acc:.1%}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(len(strategies)), \n",
    "              [strategy_accuracies[s] for s in strategies],\n",
    "              color=['lightcoral', 'lightblue', 'lightgreen'],\n",
    "              edgecolor='black', linewidth=2)\n",
    "\n",
    "plt.xticks(range(len(strategies)), \n",
    "          [s.replace('_', '\\n').title() for s in strategies],\n",
    "          fontsize=11)\n",
    "plt.ylabel('Accuracy', fontsize=13)\n",
    "plt.title('Document Ordering Strategies', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, strategy in zip(bars, strategies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{strategy_accuracies[strategy]:.1%}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION: Put most important documents at edges!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力模式分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟不同上下文长度的注意力模式\n",
    "context_lengths = [10, 20, 30]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for ax, length in zip(axes, context_lengths):\n",
    "    # 生成注意力权重（U 型）\n",
    "    positions = np.arange(length)\n",
    "    normalized = positions / (length - 1)\n",
    "    attention = 4 * (normalized - 0.5) ** 2 + 0.3\n",
    "    attention = attention / np.sum(attention)\n",
    "    \n",
    "    # 绘制\n",
    "    ax.bar(positions, attention, color='steelblue', edgecolor='black', linewidth=1)\n",
    "    ax.set_xlabel('Position', fontsize=11)\n",
    "    ax.set_ylabel('Attention Weight', fontsize=11)\n",
    "    ax.set_title(f'Context Length = {length}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 高亮中间区域\n",
    "    middle_start = length // 4\n",
    "    middle_end = 3 * length // 4\n",
    "    ax.axvspan(middle_start, middle_end, alpha=0.2, color='red')\n",
    "\n",
    "plt.suptitle('Attention Patterns: Lost in the Middle', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n随着上下文增长，中间位置获得的注意力更少！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键要点\n",
    "\n",
    "### 迷失在中间现象：\n",
    "\n",
    "**观察**：语言模型显示 **U 型性能曲线**\n",
    "- ✅ 相关信息在**开始**时准确率高\n",
    "- ✅ 相关信息在**结束**时准确率高  \n",
    "- ❌ 相关信息在**中间**时准确率**低**\n",
    "\n",
    "### 为什么会发生这种情况？\n",
    "\n",
    "**假设**：\n",
    "\n",
    "1. **注意力模式**：\n",
    "   - 自注意力自然关注最近的 token（近因偏差）\n",
    "   - 也关注早期的 token（首因偏差）\n",
    "   - 中间的 token 获得较少的注意力\n",
    "\n",
    "2. **训练分布**：\n",
    "   - 大多数训练文档都很短\n",
    "   - 长上下文在预训练中很少见\n",
    "   - 模型没有学会很好地使用中间部分\n",
    "\n",
    "3. **因果掩码**：\n",
    "   - Decoder 模型无法\"向前看\"\n",
    "   - 中间的信息可能被后续 token\"覆盖\"\n",
    "\n",
    "### 实验结果：\n",
    "\n",
    "**来自论文**：\n",
    "\n",
    "**多文档 QA**：\n",
    "- 相关文档在位置 1（开始）：~90% 准确率\n",
    "- 相关文档在位置 5（中间）：~60% 准确率  \n",
    "- 相关文档在位置 10（结束）：~85% 准确率\n",
    "\n",
    "**上下文长度的影响**：\n",
    "- 10 个文档：中间惩罚 ~30%\n",
    "- 20 个文档：中间惩罚 ~40%\n",
    "- 30 个文档：中间惩罚 ~50%\n",
    "\n",
    "**测试的模型**：\n",
    "- GPT-3.5-turbo：强 U 型偏差\n",
    "- Claude：强 U 型偏差\n",
    "- GPT-4：有所缓解但仍存在\n",
    "- 开源 LLM：偏差更强\n",
    "\n",
    "### 位置偏差公式：\n",
    "\n",
    "位置 $p$（归一化 0-1）的性能：\n",
    "$$\n",
    "\\text{Accuracy}(p) \\propto 4(p - 0.5)^2 + c\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- 最小值在 $p = 0.5$（中间）\n",
    "- 最大值在 $p = 0$ 和 $p = 1$（边缘）\n",
    "- $c$ 是基线性能\n",
    "\n",
    "### 对 RAG 系统的影响：\n",
    "\n",
    "**问题**：\n",
    "```\n",
    "检索器返回: [Doc1, Doc2, ..., Doc20]\n",
    "            (按相关性分数排序)\n",
    "\n",
    "如果最相关的文档在中间 → 性能差！\n",
    "```\n",
    "\n",
    "**解决方案**：\n",
    "\n",
    "1. **重新排序检索到的文档**：\n",
    "   - 将最相关的放在开始\n",
    "   - 或交错：最佳在边缘，最差在中间\n",
    "\n",
    "2. **限制上下文长度**：\n",
    "   - 使用更少、更相关的文档\n",
    "   - Top-3 或 top-5 而不是 top-20\n",
    "\n",
    "3. **分块**：\n",
    "   - 将长上下文分成较小的块处理\n",
    "   - 聚合结果\n",
    "\n",
    "4. **显式注意力**：\n",
    "   - 微调模型以关注中间\n",
    "   - 添加抵消偏差的位置嵌入\n",
    "\n",
    "### 文档排序策略：\n",
    "\n",
    "| 策略 | 描述 | 性能 |\n",
    "|----------|-------------|-------------|\n",
    "| 检索顺序 | 保持检索顺序 | 基线 |\n",
    "| 最相关优先 | 最佳在开始 | 良好 |\n",
    "| 最相关边缘 | 最佳在开始和结束 | **最佳** |\n",
    "| 反转 | 反转检索顺序 | 变化 |\n",
    "\n",
    "### 最佳实践：\n",
    "\n",
    "1. **尽可能使用短上下文**\n",
    "2. **重要信息放在边缘**（开始或结束）\n",
    "3. **在传递给 LLM 之前重排序**文档\n",
    "4. **分块**处理非常长的上下文\n",
    "5. **测试**模型的位置敏感性\n",
    "\n",
    "### 代码示例（重新排序）：\n",
    "\n",
    "```python\n",
    "def reorder_for_llm(docs, scores):\n",
    "    \"\"\"将最相关的放在边缘\"\"\"\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    \n",
    "    # 交错高相关性和低相关性\n",
    "    reordered = []\n",
    "    for i in range(len(docs) // 2):\n",
    "        reordered.append(docs[sorted_idx[i]])  # 高\n",
    "    for i in range(len(docs) // 2, len(docs)):\n",
    "        reordered.append(docs[sorted_idx[i]])  # 低\n",
    "    \n",
    "    # 也将最佳移到末尾\n",
    "    mid = len(reordered) // 2\n",
    "    return reordered[:mid] + reordered[mid:][::-1]\n",
    "```\n",
    "\n",
    "### 缓解策略：\n",
    "\n",
    "**训练期间**：\n",
    "- 包含长上下文示例\n",
    "- 显式监督中间位置\n",
    "- 使用位置感知目标\n",
    "\n",
    "**推理期间**：\n",
    "- 策略性地重新排序文档\n",
    "- 使用多次传递（处理子集）\n",
    "- 显式提示：\"同等关注所有文档\"\n",
    "\n",
    "**架构变更**：\n",
    "- 稀疏注意力模式\n",
    "- 分层处理\n",
    "- 检索增强注意力\n",
    "\n",
    "### 未来方向：\n",
    "\n",
    "- **位置不变模型**：训练以忽略位置偏差\n",
    "- **自适应注意力**：学习关注相关部分\n",
    "- **分块处理**：在重叠窗口中处理\n",
    "- **多遍推理**：多次读取上下文\n",
    "\n",
    "### 要点信息：\n",
    "\n",
    "```\n",
    "⚠️  警告：不要假设 LLM 同等使用所有上下文！\n",
    "\n",
    "✅  做：测试位置敏感性\n",
    "✅  做：将重要信息放在边缘  \n",
    "✅  做：尽可能保持上下文简短\n",
    "❌  不做：假设中间位置工作良好\n",
    "❌  不做：盲目连接许多文档\n",
    "```\n",
    "\n",
    "### 影响：\n",
    "\n",
    "这篇论文揭示了当前 LLM 的关键局限性，并改变了我们对以下方面的思考：\n",
    "- RAG 系统设计\n",
    "- 长上下文评估\n",
    "- QA 的文档排序\n",
    "- 多源提示工程\n",
    "\n",
    "**记住**：即使有 100k+ 的上下文窗口，位置仍然重要！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
