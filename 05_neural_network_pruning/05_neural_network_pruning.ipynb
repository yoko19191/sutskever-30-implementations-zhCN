{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文5：通过最小化描述长度保持神经网络简单\n",
    "## Hinton & Van Camp (1993) + 现代剪枝技术\n",
    "\n",
    "### 网络剪枝与压缩\n",
    "\n",
    "关键洞察：移除不必要的权重以获得更简单、更泛化的网络。越小越好！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于分类的简单神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "class SimpleNN:\n",
    "    \"\"\"Simple 2-layer neural network\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "        \n",
    "        # Keep track of masks for pruning\n",
    "        self.mask1 = np.ones_like(self.W1)\n",
    "        self.mask2 = np.ones_like(self.W2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Apply masks (for pruned weights)\n",
    "        W1_masked = self.W1 * self.mask1\n",
    "        W2_masked = self.W2 * self.mask2\n",
    "        \n",
    "        # Hidden layer\n",
    "        self.h = relu(np.dot(X, W1_masked) + self.b1)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = np.dot(self.h, W2_masked) + self.b2\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"Compute accuracy\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total and active (non-pruned) parameters\"\"\"\n",
    "        total = self.W1.size + self.b1.size + self.W2.size + self.b2.size\n",
    "        active = int(np.sum(self.mask1) + self.b1.size + np.sum(self.mask2) + self.b2.size)\n",
    "        return total, active\n",
    "\n",
    "# Test network\n",
    "nn = SimpleNN(input_dim=10, hidden_dim=20, output_dim=3)\n",
    "X_test = np.random.randn(5, 10)\n",
    "y_test = nn.forward(X_test)\n",
    "print(f\"Network output shape: {y_test.shape}\")\n",
    "total, active = nn.count_parameters()\n",
    "print(f\"Parameters: {total} total, {active} active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成合成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n_samples=1000, n_features=20, n_classes=3):\n",
    "    \"\"\"\n",
    "    Generate synthetic classification dataset\n",
    "    Each class is a Gaussian blob\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    samples_per_class = n_samples // n_classes\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        # Random center for this class\n",
    "        center = np.random.randn(n_features) * 3\n",
    "        \n",
    "        # Generate samples around center\n",
    "        X_class = np.random.randn(samples_per_class, n_features) + center\n",
    "        y_class = np.full(samples_per_class, c)\n",
    "        \n",
    "        X.append(X_class)\n",
    "        y.append(y_class)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.concatenate(y)\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X_train, y_train = generate_classification_data(n_samples=1000, n_features=20, n_classes=3)\n",
    "X_test, y_test = generate_classification_data(n_samples=300, n_features=20, n_classes=3)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练基线网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, X_train, y_train, X_test, y_test, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Simple training loop\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        probs = model.forward(X_train)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        y_one_hot = np.zeros((len(y_train), model.output_dim))\n",
    "        y_one_hot[np.arange(len(y_train)), y_train] = 1\n",
    "        loss = -np.mean(np.sum(y_one_hot * np.log(probs + 1e-8), axis=1))\n",
    "        \n",
    "        # Backward pass (simplified)\n",
    "        batch_size = len(X_train)\n",
    "        dL_dlogits = (probs - y_one_hot) / batch_size\n",
    "        \n",
    "        # Gradients for W2, b2\n",
    "        dL_dW2 = np.dot(model.h.T, dL_dlogits)\n",
    "        dL_db2 = np.sum(dL_dlogits, axis=0)\n",
    "        \n",
    "        # Gradients for W1, b1\n",
    "        dL_dh = np.dot(dL_dlogits, (model.W2 * model.mask2).T)\n",
    "        dL_dh[model.h <= 0] = 0  # ReLU derivative\n",
    "        dL_dW1 = np.dot(X_train.T, dL_dh)\n",
    "        dL_db1 = np.sum(dL_dh, axis=0)\n",
    "        \n",
    "        # Update weights (only where mask is active)\n",
    "        model.W1 -= lr * dL_dW1 * model.mask1\n",
    "        model.b1 -= lr * dL_db1\n",
    "        model.W2 -= lr * dL_dW2 * model.mask2\n",
    "        model.b2 -= lr * dL_db2\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(loss)\n",
    "        test_acc = model.accuracy(X_test, y_test)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Test Acc: {test_acc:.2%}\")\n",
    "    \n",
    "    return train_losses, test_accuracies\n",
    "\n",
    "# Train baseline model\n",
    "print(\"Training baseline network...\\n\")\n",
    "baseline_model = SimpleNN(input_dim=20, hidden_dim=50, output_dim=3)\n",
    "train_losses, test_accs = train_network(baseline_model, X_train, y_train, X_test, y_test, epochs=100)\n",
    "\n",
    "baseline_acc = baseline_model.accuracy(X_test, y_test)\n",
    "total_params, active_params = baseline_model.count_parameters()\n",
    "print(f\"\\nBaseline: {baseline_acc:.2%} accuracy, {active_params} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于幅度的剪枝\n",
    "\n",
    "移除具有最小绝对值的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_by_magnitude(model, pruning_rate):\n",
    "    \"\"\"\n",
    "    Prune weights with smallest magnitudes\n",
    "    \n",
    "    pruning_rate: fraction of weights to remove (0-1)\n",
    "    \"\"\"\n",
    "    # Collect all weights\n",
    "    all_weights = np.concatenate([model.W1.flatten(), model.W2.flatten()])\n",
    "    all_magnitudes = np.abs(all_weights)\n",
    "    \n",
    "    # Find threshold\n",
    "    threshold = np.percentile(all_magnitudes, pruning_rate * 100)\n",
    "    \n",
    "    # Create new masks\n",
    "    model.mask1 = (np.abs(model.W1) > threshold).astype(float)\n",
    "    model.mask2 = (np.abs(model.W2) > threshold).astype(float)\n",
    "    \n",
    "    print(f\"Pruning threshold: {threshold:.6f}\")\n",
    "    print(f\"Pruned {pruning_rate:.1%} of weights\")\n",
    "    \n",
    "    total, active = model.count_parameters()\n",
    "    print(f\"Remaining parameters: {active}/{total} ({active/total:.1%})\")\n",
    "\n",
    "# Test pruning\n",
    "import copy\n",
    "pruned_model = copy.deepcopy(baseline_model)\n",
    "\n",
    "print(\"Before pruning:\")\n",
    "acc_before = pruned_model.accuracy(X_test, y_test)\n",
    "print(f\"Accuracy: {acc_before:.2%}\\n\")\n",
    "\n",
    "print(\"Pruning 50% of weights...\")\n",
    "prune_by_magnitude(pruned_model, pruning_rate=0.5)\n",
    "\n",
    "print(\"\\nAfter pruning (before retraining):\")\n",
    "acc_after = pruned_model.accuracy(X_test, y_test)\n",
    "print(f\"Accuracy: {acc_after:.2%}\")\n",
    "print(f\"Accuracy drop: {(acc_before - acc_after):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 剪枝后微调\n",
    "\n",
    "重新训练剩余权重以恢复准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fine-tuning pruned network...\\n\")\n",
    "finetune_losses, finetune_accs = train_network(\n",
    "    pruned_model, X_train, y_train, X_test, y_test, epochs=50, lr=0.005\n",
    ")\n",
    "\n",
    "acc_finetuned = pruned_model.accuracy(X_test, y_test)\n",
    "total, active = pruned_model.count_parameters()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Baseline:     {baseline_acc:.2%} accuracy, {total_params} params\")\n",
    "print(f\"Pruned 50%:   {acc_finetuned:.2%} accuracy, {active} params\")\n",
    "print(f\"Compression:  {total_params/active:.1f}x smaller\")\n",
    "print(f\"Acc. change:  {(acc_finetuned - baseline_acc):+.2%}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代剪枝\n",
    "\n",
    "逐渐增加剪枝率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_pruning(model, X_train, y_train, X_test, y_test, \n",
    "                     target_sparsity=0.9, num_iterations=5):\n",
    "    \"\"\"\n",
    "    Iteratively prune and finetune\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Initial state\n",
    "    total, active = model.count_parameters()\n",
    "    acc = model.accuracy(X_test, y_test)\n",
    "    results.append({\n",
    "        'iteration': 0,\n",
    "        'sparsity': 0.0,\n",
    "        'active_params': active,\n",
    "        'accuracy': acc\n",
    "    })\n",
    "    \n",
    "    # Gradually increase sparsity\n",
    "    for i in range(num_iterations):\n",
    "        # Sparsity for this iteration\n",
    "        current_sparsity = target_sparsity * (i + 1) / num_iterations\n",
    "        \n",
    "        print(f\"\\nIteration {i+1}/{num_iterations}: Target sparsity {current_sparsity:.1%}\")\n",
    "        \n",
    "        # Prune\n",
    "        prune_by_magnitude(model, pruning_rate=current_sparsity)\n",
    "        \n",
    "        # Finetune\n",
    "        train_network(model, X_train, y_train, X_test, y_test, epochs=30, lr=0.005)\n",
    "        \n",
    "        # Record results\n",
    "        total, active = model.count_parameters()\n",
    "        acc = model.accuracy(X_test, y_test)\n",
    "        results.append({\n",
    "            'iteration': i + 1,\n",
    "            'sparsity': current_sparsity,\n",
    "            'active_params': active,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run iterative pruning\n",
    "iterative_model = copy.deepcopy(baseline_model)\n",
    "results = iterative_pruning(iterative_model, X_train, y_train, X_test, y_test, \n",
    "                           target_sparsity=0.95, num_iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化剪枝结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "sparsities = [r['sparsity'] for r in results]\n",
    "accuracies = [r['accuracy'] for r in results]\n",
    "active_params = [r['active_params'] for r in results]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy vs Sparsity\n",
    "ax1.plot(sparsities, accuracies, 'o-', linewidth=2, markersize=10, color='steelblue')\n",
    "ax1.axhline(y=baseline_acc, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax1.set_xlabel('Sparsity (Fraction Pruned)', fontsize=12)\n",
    "ax1.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax1.set_title('Accuracy vs Sparsity', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Parameters vs Accuracy\n",
    "ax2.plot(active_params, accuracies, 's-', linewidth=2, markersize=10, color='darkgreen')\n",
    "ax2.axhline(y=baseline_acc, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax2.set_xlabel('Active Parameters', fontsize=12)\n",
    "ax2.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy vs Model Size', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.invert_xaxis()  # Fewer params on right\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation: Can remove 90%+ of weights with minimal accuracy loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化权重分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Baseline weights\n",
    "axes[0, 0].hist(baseline_model.W1.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Baseline W1 Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Weight Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(baseline_model.W2.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Baseline W2 Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Weight Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Pruned weights (only active)\n",
    "pruned_W1 = iterative_model.W1[iterative_model.mask1 > 0]\n",
    "pruned_W2 = iterative_model.W2[iterative_model.mask2 > 0]\n",
    "\n",
    "axes[1, 0].hist(pruned_W1.flatten(), bins=50, color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Pruned W1 Distribution (Active Weights Only)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Weight Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(pruned_W2.flatten(), bins=50, color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Pruned W2 Distribution (Active Weights Only)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Weight Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Pruned weights have larger magnitudes (small weights removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化稀疏性模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# W1 sparsity pattern\n",
    "im1 = ax1.imshow(iterative_model.mask1.T, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax1.set_xlabel('Input Dimension', fontsize=12)\n",
    "ax1.set_ylabel('Hidden Dimension', fontsize=12)\n",
    "ax1.set_title('W1 Sparsity Pattern (Green=Active, Red=Pruned)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# W2 sparsity pattern\n",
    "im2 = ax2.imshow(iterative_model.mask2.T, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax2.set_xlabel('Hidden Dimension', fontsize=12)\n",
    "ax2.set_ylabel('Output Dimension', fontsize=12)\n",
    "ax2.set_title('W2 Sparsity Pattern (Green=Active, Red=Pruned)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total, active = iterative_model.count_parameters()\n",
    "print(f\"\\nFinal sparsity: {(total - active) / total:.1%}\")\n",
    "print(f\"Compression ratio: {total / active:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDL原则\n",
    "\n",
    "最小描述长度：更简单的模型泛化更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mdl(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Simplified MDL computation\n",
    "    \n",
    "    MDL = Model Cost + Data Cost\n",
    "    - Model Cost: Bits to encode weights\n",
    "    - Data Cost: Bits to encode errors\n",
    "    \"\"\"\n",
    "    # Model cost: number of parameters (simplified)\n",
    "    total, active = model.count_parameters()\n",
    "    model_cost = active  # Each param = 1 \"bit\" (simplified)\n",
    "    \n",
    "    # Data cost: cross-entropy loss\n",
    "    probs = model.forward(X_train)\n",
    "    y_one_hot = np.zeros((len(y_train), model.output_dim))\n",
    "    y_one_hot[np.arange(len(y_train)), y_train] = 1\n",
    "    data_cost = -np.sum(y_one_hot * np.log(probs + 1e-8))\n",
    "    \n",
    "    total_cost = model_cost + data_cost\n",
    "    \n",
    "    return {\n",
    "        'model_cost': model_cost,\n",
    "        'data_cost': data_cost,\n",
    "        'total_cost': total_cost\n",
    "    }\n",
    "\n",
    "# Compare MDL for different models\n",
    "baseline_mdl = compute_mdl(baseline_model, X_train, y_train)\n",
    "pruned_mdl = compute_mdl(iterative_model, X_train, y_train)\n",
    "\n",
    "print(\"MDL Comparison:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Model':<20} {'Model Cost':<15} {'Data Cost':<15} {'Total'}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Baseline':<20} {baseline_mdl['model_cost']:<15.0f} {baseline_mdl['data_cost']:<15.2f} {baseline_mdl['total_cost']:.2f}\")\n",
    "print(f\"{'Pruned (95%)':<20} {pruned_mdl['model_cost']:<15.0f} {pruned_mdl['data_cost']:<15.2f} {pruned_mdl['total_cost']:.2f}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nPruned model has LOWER total cost → Better generalization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键要点\n",
    "\n",
    "### 神经网络剪枝：\n",
    "\n",
    "**核心思想**：移除不必要的权重以创建更简单、更小的网络\n",
    "\n",
    "### 基于幅度的剪枝：\n",
    "\n",
    "1. **正常训练**网络\n",
    "2. **识别**低幅度权重：$|w| < \\text{threshold}$\n",
    "3. **移除**这些权重（设为0，掩码掉）\n",
    "4. **微调**剩余权重\n",
    "\n",
    "### 迭代剪枝：\n",
    "\n",
    "比一次性剪枝更好：\n",
    "```\n",
    "for iteration in 1..N:\n",
    "    剪枝小部分（例如，20%）\n",
    "    微调\n",
    "```\n",
    "\n",
    "允许网络逐渐适应。\n",
    "\n",
    "### 结果（典型）：\n",
    "\n",
    "- **50%稀疏性**：通常没有准确率损失\n",
    "- **90%稀疏性**：轻微准确率损失（<2%）\n",
    "- **95%+稀疏性**：明显退化\n",
    "\n",
    "现代网络（ResNets，Transformers）通常可以在几乎没有影响的情况下剪枝到**90-95%稀疏性**！\n",
    "\n",
    "### MDL原则：\n",
    "\n",
    "$$\n",
    "\\text{MDL} = \\underbrace{L(\\text{Model})}_\\text{complexity} + \\underbrace{L(\\text{Data | Model})}_\\text{errors}\n",
    "$$\n",
    "\n",
    "**奥卡姆剃刀**：拟合数据的最简单解释（最小网络）是最好的。\n",
    "\n",
    "### 剪枝的好处：\n",
    "\n",
    "1. **更小的模型**：更少内存，更快推理\n",
    "2. **更好的泛化**：移除过拟合参数\n",
    "3. **能效**：更少的操作\n",
    "4. **可解释性**：更简单的结构\n",
    "\n",
    "### 剪枝类型：\n",
    "\n",
    "| 类型 | 移除什么 | 加速 |\n",
    "|------|----------------|----------|\n",
    "| **非结构化** | 单个权重 | 低（稀疏操作） |\n",
    "| **结构化** | 整个神经元/滤波器 | 高（密集操作） |\n",
    "| **通道** | 整个通道 | 高 |\n",
    "| **层** | 整个层 | 非常高 |\n",
    "\n",
    "### 现代技术：\n",
    "\n",
    "1. **彩票假设**： \n",
    "   - 剪枝的网络可以从初始化重新训练\n",
    "   - \"中奖彩票\"存在于随机初始化中\n",
    "\n",
    "2. **动态稀疏训练**：\n",
    "   - 训练期间剪枝（而非之后）\n",
    "   - 重新生长连接\n",
    "\n",
    "3. **幅度 + 梯度**：\n",
    "   - 使用梯度信息，不仅仅是幅度\n",
    "   - 移除小幅度和小梯度的权重\n",
    "\n",
    "4. **可学习稀疏性**：\n",
    "   - L0/L1正则化\n",
    "   - 自动稀疏性发现\n",
    "\n",
    "### 实用技巧：\n",
    "\n",
    "1. **从高开始，逐渐剪枝**：不要立即剪枝90%\n",
    "2. **剪枝后微调**：对恢复至关重要\n",
    "3. **逐层剪枝率**：不同层有不同的冗余\n",
    "4. **结构化剪枝以加速**：非结构化需要特殊硬件\n",
    "\n",
    "### 何时剪枝：\n",
    "\n",
    "✅ **适用于**：\n",
    "- 部署（边缘设备，移动设备）\n",
    "- 减少推理成本\n",
    "- 模型压缩\n",
    "\n",
    "❌ **不理想用于**：\n",
    "- 非常小的模型（已经高效）\n",
    "- 训练加速（仅结构化剪枝）\n",
    "\n",
    "### 实践中的压缩率：\n",
    "\n",
    "- **AlexNet**：9倍压缩（无准确率损失）\n",
    "- **VGG-16**：13倍压缩\n",
    "- **ResNet-50**：5-7倍压缩\n",
    "- **BERT**：10-40倍压缩（带量化）\n",
    "\n",
    "### 关键洞察：\n",
    "\n",
    "**神经网络是严重过度参数化的！**\n",
    "\n",
    "大多数权重对最终性能贡献很小。剪枝揭示了做实际工作的\"核心\"网络。\n",
    "\n",
    "**\"最好的模型是拟合数据的最简单模型\"** - MDL原则"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
